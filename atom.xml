<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zhisheng的博客</title>
  
  <subtitle>坑要一个个填，路要一步步走！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.54tianzhisheng.cn/"/>
  <updated>2019-06-02T10:02:42.000Z</updated>
  <id>http://www.54tianzhisheng.cn/</id>
  
  <author>
    <name>zhisheng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新）</title>
    <link href="http://www.54tianzhisheng.cn/2019/12/31/Flink-resources/"/>
    <id>http://www.54tianzhisheng.cn/2019/12/31/Flink-resources/</id>
    <published>2019-12-30T16:00:00.000Z</published>
    <updated>2019-06-02T10:02:42.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-02-flink.png" alt=""><br><a id="more"></a></p><h2 id="Flink-学习项目代码"><a href="#Flink-学习项目代码" class="headerlink" title="Flink 学习项目代码"></a>Flink 学习项目代码</h2><p><a href="https://github.com/zhisheng17/flink-learning">https://github.com/zhisheng17/flink-learning</a></p><p>麻烦路过的各位亲给这个项目点个 star，太不易了，写了这么多，算是对我坚持下来的一种鼓励吧！</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-124027.jpg" alt=""></p><h2 id="本项目结构"><a href="#本项目结构" class="headerlink" title="本项目结构"></a>本项目结构</h2><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-02-095222.jpg" alt=""></p><h2 id="博客"><a href="#博客" class="headerlink" title="博客"></a>博客</h2><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">Flink 从0到1学习 —— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">Flink 从0到1学习 —— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">Flink 从0到1学习 —— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">Flink 从0到1学习 —— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">Flink 从0到1学习 —— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">Flink 从0到1学习 —— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">Flink 从0到1学习 —— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">Flink 从0到1学习 —— 介绍 Flink 中的 Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">Flink 从0到1学习 —— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">Flink 从0到1学习 —— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">Flink 从0到1学习 —— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="https://t.zsxq.com/uVbi2nq">Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="https://t.zsxq.com/zV7MnuJ">Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HBase</a></p><p>19、<a href="https://t.zsxq.com/zV7MnuJ">Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 HDFS</a></p><p>20、<a href="https://t.zsxq.com/zV7MnuJ">Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Redis</a></p><p>21、<a href="https://t.zsxq.com/uVbi2nq">Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Cassandra</a></p><p>22、<a href="https://t.zsxq.com/zV7MnuJ">Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 Flume</a></p><p>23、<a href="https://t.zsxq.com/zV7MnuJ">Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 InfluxDB</a></p><p>24、<a href="https://t.zsxq.com/zV7MnuJ">Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RocketMQ</a></p><p>25、<a href="https://t.zsxq.com/uniY7mm">Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了</a></p><p>26、<a href="https://t.zsxq.com/zV7MnuJ">Flink 从0到1学习 —— 你的 Flink job 日志跑到哪里去了</a></p><h2 id="Flink-源码项目结构"><a href="#Flink-源码项目结构" class="headerlink" title="Flink 源码项目结构"></a>Flink 源码项目结构</h2><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-02-Flink%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90.png" alt=""></p><h2 id="学习资料"><a href="#学习资料" class="headerlink" title="学习资料"></a>学习资料</h2><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。<br>你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到，转载请联系本人获取授权，违者必究。</p><p><img src="https://ws2.sinaimg.cn/large/006tNbRwly1fyh07imy15j30bq0bwq43.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0c5a3l1cyj30u00gewg6.jpg" alt=""></p><p>有人要问知识星球里面更新什么内容？值得加入吗？</p><p>目前知识星球内已更新的系列文章：</p><p>1、<a href="https://t.zsxq.com/UZfaYfE">Flink 源码解析 —— 源码编译运行</a></p><p>2、<a href="https://t.zsxq.com/zZZjaYf">Flink 源码解析 —— 项目结构一览</a></p><p>3、<a href="https://t.zsxq.com/zV7MnuJ">Flink 源码解析—— local 模式启动流程</a></p><p>4、<a href="https://t.zsxq.com/QZVRZJA">Flink 源码解析 —— standalonesession 模式启动流程</a></p><p>5、<a href="https://t.zsxq.com/u3fayvf">Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动</a></p><p>6、<a href="https://t.zsxq.com/MnQRByb">Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动</a></p><p>7、<a href="https://t.zsxq.com/YJ2Zrfi">Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程</a></p><p>8、<a href="https://t.zsxq.com/qnMFEUJ">Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程</a></p><p>9、<a href="https://t.zsxq.com/naaMf6y">Flink 源码解析 —— 如何获取 JobGraph？</a></p><p>10、<a href="https://t.zsxq.com/qRFIm6I">Flink 源码解析 —— 如何获取 StreamGraph？</a></p><p>11、<a href="https://t.zsxq.com/2VRrbuf">Flink 源码解析 —— Flink JobManager 有什么作用？</a></p><p>12、<a href="https://t.zsxq.com/RZbu7yN">Flink 源码解析 —— Flink TaskManager 有什么作用？</a></p><p>13、<a href="https://t.zsxq.com/zV7MnuJ">Flink 源码解析 —— JobManager 处理 SubmitJob 的过程</a></p><p>14、<a href="https://t.zsxq.com/zV7MnuJ">Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程</a></p><p>15、<a href="https://t.zsxq.com/ynQNbeM">Flink 源码解析 —— 深度解析 Flink Checkpoint 机制</a></p><p>16、<a href="https://t.zsxq.com/JaQfeMf">Flink 源码解析 —— 深度解析 Flink 序列化机制</a></p><p>17、<a href="https://t.zsxq.com/zjQvjeM">Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？</a></p><p>除了《从1到100深入学习Flink》源码学习这个系列文章，《从0到1学习Flink》的案例文章也会优先在知识星球更新，让大家先通过一些 demo 学习 Flink，再去深入源码学习！</p><p>如果学习 Flink 的过程中，遇到什么问题，可以在里面提问，我会优先解答，这里做个抱歉，自己平时工作也挺忙，微信的问题不能做全部做一些解答，<br>但肯定会优先回复给知识星球的付费用户的，庆幸的是现在星球里的活跃氛围还是可以的，有不少问题通过提问和解答的方式沉淀了下来。</p><p>1、<a href="https://t.zsxq.com/62rZV7q">为何我使用 ValueState 保存状态 Job 恢复是状态没恢复？</a></p><p>2、<a href="https://t.zsxq.com/yF2rjmY">flink中watermark究竟是如何生成的，生成的规则是什么，怎么用来处理乱序数据</a></p><p>3、<a href="https://t.zsxq.com/uzFIeiq">消费kafka数据的时候，如果遇到了脏数据，或者是不符合规则的数据等等怎么处理呢？</a></p><p>4、<a href="https://t.zsxq.com/Nz7QZBY">在Kafka 集群中怎么指定读取/写入数据到指定broker或从指定broker的offset开始消费？</a></p><p>5、<a href="https://t.zsxq.com/7UVBeMj">Flink能通过oozie或者azkaban提交吗？</a></p><p>6、<a href="https://t.zsxq.com/mUzRbY7">jobmanager挂掉后，提交的job怎么不经过手动重新提交执行？</a></p><p>7、<a href="https://t.zsxq.com/Nju7EuV">使用flink-web-ui提交作业并执行 但是/opt/flink/log目录下没有日志文件 请问关于flink的日志（包括jobmanager、taskmanager、每个job自己的日志默认分别存在哪个目录 ）需要怎么配置？</a></p><p>8、<a href="https://t.zsxq.com/6muRz3j">通过flink 仪表盘提交的jar 是存储在哪个目录下？</a></p><p>9、<a href="https://t.zsxq.com/uvFQvFu">从Kafka消费数据进行etl清洗，把结果写入hdfs映射成hive表，压缩格式、hive直接能够读取flink写出的文件、按照文件大小或者时间滚动生成文件</a></p><p>10、<a href="https://t.zsxq.com/ubIY33f">flink jar包上传至集群上运行，挂掉后，挂掉期间kafka中未被消费的数据，在重新启动程序后，是自动从checkpoint获取挂掉之前的kafka offset位置，自动消费之前的数据进行处理，还是需要某些手动的操作呢？</a></p><p>11、<a href="https://t.zsxq.com/UfA2rBy">flink 启动时不自动创建 上传jar的路径，能指定一个创建好的目录吗</a></p><p>12、<a href="https://t.zsxq.com/zBMnIA6">Flink sink to es 集群上报 slot 不够，单机跑是好的，为什么？</a></p><p>13、<a href="https://t.zsxq.com/qrZBAQJ">Fllink to elasticsearch如何创建索引文档期时间戳？</a></p><p>14、<a href="https://t.zsxq.com/J2JiIMv">blink有没有api文档或者demo，是否建议blink用于生产环境。</a></p><p>15、<a href="https://t.zsxq.com/ZVVrjuv">flink的Python api怎样？bug多吗？</a></p><p>16、<a href="https://t.zsxq.com/zbybQNf">Flink VS Spark Streaming VS Storm VS Kafka Stream </a></p><p>17、<a href="https://t.zsxq.com/Zf6meAm">你们做实时大屏的技术架构是什么样子的？flume→kafka→flink→redis，然后后端去redis里面捞数据，酱紫可行吗？</a></p><p>18、<a href="https://t.zsxq.com/YniI2JQ">做一个统计指标的时候，需要在Flink的计算过程中多次读写redis，感觉好怪，星主有没有好的方案？</a></p><p>19、<a href="https://t.zsxq.com/fYZZfYf">Flink 使用场景大分析，列举了很多的常用场景，可以好好参考一下</a></p><p>20、<a href="https://t.zsxq.com/I6eEqR7">将kafka中数据sink到mysql时，metadata的数据为空，导入mysql数据不成功？？？</a></p><p>21、<a href="https://t.zsxq.com/62rZV7q">使用了ValueState来保存中间状态，在运行时中间状态保存正常，但是在手动停止后，再重新运行，发现中间状态值没有了，之前出现的键值是从0开始计数的，这是为什么？是需要实现CheckpointedFunction吗？</a></p><p>22、<a href="https://t.zsxq.com/mQ7YbQJ">flink on yarn jobmanager的HA需要怎么配置。还是说yarn给管理了</a></p><p>23、<a href="https://t.zsxq.com/q3VvB6U">有两个数据流就行connect，其中一个是实时数据流（kafka 读取)，另一个是配置流。由于配置流是从关系型数据库中读取，速度较慢，导致实时数据流流入数据的时候，配置信息还未发送，这样会导致有些实时数据读取不到配置信息。目前采取的措施是在connect方法后的flatmap的实现的在open 方法中，提前加载一次配置信息，感觉这种实现方式不友好，请问还有其他的实现方式吗？</a></p><p>24、<a href="https://t.zsxq.com/7UVBeMj">Flink能通过oozie或者azkaban提交吗？</a></p><p>25、<a href="https://t.zsxq.com/mUzRbY7">不采用yarm部署flink，还有其他的方案吗？ 主要想解决服务器重启后，flink服务怎么自动拉起？ jobmanager挂掉后，提交的job怎么不经过手动重新提交执行？</a></p><p>26、<a href="https://t.zsxq.com/bYnimQv">在一个 Job 里将同份数据昨晚清洗操作后，sink 到后端多个地方（看业务需求），如何保持一致性？（一个sink出错，另外的也保证不能插入）</a></p><p>27、<a href="https://t.zsxq.com/YvBAyrV">flink sql任务在某个特定阶段会发生tm和jm丢失心跳，是不是由于gc时间过长呢，</a></p><p>28、<a href="https://t.zsxq.com/fayf2Vv">有这样一个需求，统计用户近两周进入产品详情页的来源（1首页大搜索，2产品频道搜索，3其他），为php后端提供数据支持，该信息在端上报事件中，php直接获取有点困难。 我现在的解决方案 通过flink滚动窗口（半小时），统计用户半小时内3个来源pv，然后按照日期序列化，直接写mysql。php从数据库中解析出来，再去统计近两周占比。 问题1，这个需求适合用flink去做吗？ 问题2，我的方案总感觉怪怪的，有没有好的方案？</a></p><p>29、<a href="https://t.zsxq.com/ZFiY3VZ">一个task slot  只能同时运行一个任务还是多个任务呢？如果task  slot运行的任务比较大，会出现OOM的情况吗？</a></p><p>30、<a href="https://t.zsxq.com/Yn2JqB6">你们怎么对线上flink做监控的，如果整个程序失败了怎么自动重启等等</a></p><p>31、<a href="https://t.zsxq.com/YFMFeaA">flink cep规则动态解析有接触吗？有没有成型的框架？</a></p><p>32、<a href="https://t.zsxq.com/VZvRrjm">每一个Window都有一个watermark吗？window是怎么根据watermark进行触发或者销毁的？</a></p><p>33、<a href="https://t.zsxq.com/R3ZZJUF"> CheckPoint与SavePoint的区别是什么？</a></p><p>34、<a href="https://t.zsxq.com/Aa62Bim">flink可以在算子中共享状态吗？或者大佬你有什么方法可以共享状态的呢？</a></p><p>35、<a href="https://t.zsxq.com/ayFmmMF">运行几分钟就报了，看taskmager日志，报的是 failed elasticsearch bulk request null，可是我代码里面已经做过空值判断了呀 而且也过滤掉了，flink版本1.7.2 es版本6.3.1</a></p><p>36、<a href="https://t.zsxq.com/Yzzzb2b">这种情况，我们调并行度 还是配置参数好</a></p><p>37、<a href="https://t.zsxq.com/AqBUR3f">大家都用jdbc写，各种数据库增删查改拼sql有没有觉得很累，ps.set代码一大堆，还要计算每个参数的位置</a></p><p>38、<a href="https://t.zsxq.com/AqBUR3f">关于datasource的配置，每个taskmanager对应一个datasource?还是每个slot? 实际运行下来，每个slot中datasorce线程池只要设置1就行了，多了也用不到?</a></p><p>39、<a href="https://t.zsxq.com/AqBUR3f">kafka现在每天出现数据丢失，现在小批量数据，一天200W左右, kafka版本为 1.0.0，集群总共7个节点，TOPIC有十六个分区，单条报文1.5k左右</a></p><p>40、<a href="https://t.zsxq.com/AqBUR3f">根据key.hash的绝对值 对并发度求模，进行分组，假设10各并发度，实际只有8个分区有处理数据，有2个始终不处理，还有一个分区处理的数据是其他的三倍，如截图</a></p><p>41、<a href="https://t.zsxq.com/AqBUR3f">flink每7小时不知道在处理什么， CPU 负载 每7小时，有一次高峰，5分钟内平均负载超过0.8，如截图</a></p><p>42、<a href="https://t.zsxq.com/M3fIMbu">有没有Flink写的项目推荐？我想看到用Flink写的整体项目是怎么组织的，不单单是一个单例子</a></p><p>43、<a href="https://t.zsxq.com/yv7EQFA">Flink 源码的结构图</a></p><p>44、<a href="https://t.zsxq.com/vBAYNJq">我想根据不同业务表（case when）进行不同的redis sink（hash ，set），我要如何操作？</a></p><p>45、<a href="https://t.zsxq.com/b2zbUJa">这个需要清理什么数据呀，我把hdfs里面的已经清理了 启动还是报这个</a></p><p>46、<a href="https://t.zsxq.com/QjQFmQr">  在流处理系统，在机器发生故障恢复之后，什么情况消息最多会被处理一次？什么情况消息最少会被处理一次呢？</a></p><p>47、<a href="https://t.zsxq.com/zbQNfuJ">我检查点都调到5分钟了，这是什么问题</a></p><p>48、<a href="https://t.zsxq.com/ZrjEauN">reduce方法后 那个交易时间 怎么不是最新的，是第一次进入的那个时间，</a></p><p>49、<a href="https://t.zsxq.com/VJyr3bM">Flink  on Yarn 模式，用yarn session脚本启动的时候，我在后台没有看到到Jobmanager，TaskManager，ApplicationMaster这几个进程，想请问一下这是什么原因呢？因为之前看官网的时候，说Jobmanager就是一个jvm进程，Taskmanage也是一个JVM进程</a></p><p>50、<a href="https://t.zsxq.com/VJyr3bM">Flink  on Yarn的时候得指定 多少个TaskManager和每个TaskManager slot去运行任务，这样做感觉不太合理，因为用户也不知道需要多少个TaskManager适合，Flink 有动态启动TaskManager的机制吗。</a></p><p>51、<a href="https://t.zsxq.com/UBmUJMv">参考这个例子，Flink 零基础实战教程：如何计算实时热门商品 | Jark’s Blog， 窗口聚合的时候，用keywindow，用的是timeWindowAll，然后在aggregate的时候用aggregate(new CustomAggregateFunction(), new CustomWindowFunction())，打印结果后，发现窗口中一直使用的重复的数据，统计的结果也不变，去掉CustomWindowFunction()就正常了 ？ 非常奇怪</a></p><p>52、<a href="https://t.zsxq.com/naQb6aI">用户进入产品预定页面（端埋点上报），并填写了一些信息（端埋点上报），但半小时内并没有产生任何订单，然后给该类用户发送一个push。 1. 这种需求适合用flink去做吗？2. 如果适合，说下大概的思路</a></p><p>53、<a href="https://t.zsxq.com/AUf2VNz">业务场景是实时获取数据存redis，请问我要如何按天、按周、按月分别存入redis里？（比方说过了一天自动换一个位置存redis）</a></p><p>54、<a href="https://t.zsxq.com/UJ6Y7m2">有人 AggregatingState 的例子吗, 感觉官方的例子和 官网的不太一样?</a></p><p>55、<a href="https://t.zsxq.com/r3BaAY3">flink-jdbc这个jar有吗？怎么没找到啊？1.8.0的没找到，1.6.2的有</a></p><p>56、<a href="https://t.zsxq.com/jiybIee">现有个关于savepoint的问题，操作流程为，取消任务时设置保存点，更新任务，从保存点启动任务；现在遇到个问题，假设我中间某个算子重写，原先通过state编写，有用定时器，现在更改后，采用窗口，反正就是实现方式完全不一样；从保存点启动就会一直报错，重启，原先的保存点不能还原，此时就会有很多数据重复等各种问题，如何才能保证数据不丢失，不重复等，恢复到停止的时候，现在想到的是记下kafka的偏移量，再做处理，貌似也不是很好弄，有什么解决办法吗</a></p><p>57、<a href="https://t.zsxq.com/eMJmiQz">需要在flink计算app页面访问时长，消费Kafka计算后输出到Kafka。第一条log需要等待第二条log的时间戳计算访问时长。我想问的是，flink是分布式的，那么它能否保证执行的顺序性？后来的数据有没有可能先被执行？</a></p><p>58、<a href="https://t.zsxq.com/Y7e6aIu">我公司想做实时大屏，现有技术是将业务所需指标实时用spark拉到redis里存着，然后再用一条spark streaming流计算简单乘除运算，指标包含了各月份的比较。请问我该如何用flink简化上述流程？</a></p><p>59、<a href="https://t.zsxq.com/QbIayJ6">flink on yarn 方式，这样理解不知道对不对，yarn-session这个脚本其实就是准备yarn环境的，执行run任务的时候，根据yarn-session初始化的yarnDescription 把 flink 任务的jobGraph提交到yarn上去执行</a></p><p>60、<a href="https://t.zsxq.com/VFMRbYN">同样的代码逻辑写在单独的main函数中就可以成功的消费kafka ，写在一个spring boot的程序中，接受外部请求，然后执行相同的逻辑就不能消费kafka。你遇到过吗？能给一些查问题的建议，或者在哪里打个断点，能看到为什么消费不到kafka的消息呢？</a></p><p>61、<a href="https://t.zsxq.com/QNvjI6Q">请问下flink可以实现一个流中同时存在订单表和订单商品表的数据 两者是一对多的关系  能实现得到 以订单表为主 一个订单多个商品 这种需求嘛</a></p><p>62、<a href="https://t.zsxq.com/6ie66EE">在用中间状态的时候，如果中间一些信息保存在state中，有没有必要在redis中再保存一份，来做第三方的存储。</a></p><p>63、<a href="https://t.zsxq.com/bm6mYjI">能否出一期flink state的文章。什么场景下用什么样的state？如，最简单的，实时累加update到state。</a></p><p>64、<a href="https://t.zsxq.com/II6AEe2">flink的双流join博主有使用的经验吗？会有什么常见的问题吗</a></p><p>65、<a href="https://t.zsxq.com/V7EmUZR">窗口触发的条件问题</a></p><p>66、<a href="https://t.zsxq.com/JY3NJam">flink 定时任务怎么做？有相关的demo么？</a></p><p>67、<a href="https://t.zsxq.com/7YZ3Fuz">流式处理过程中数据的一致性如何保证或者如何检测</a></p><p>68、<a href="https://t.zsxq.com/nEEQvzR">重启flink单机集群，还报job not found 异常。</a></p><p>69、<a href="https://t.zsxq.com/qJyvzNj">kafka的数据是用 org.apache.kafka.common.serialization.ByteArraySerialize序列化的，flink这边消费的时候怎么通过FlinkKafkaConsumer创建DataStream<String>？</a></p><p>70、<a href="https://t.zsxq.com/byvnaEi">现在公司有一个需求，一些用户的支付日志，通过sls收集，要把这些日志处理后，结果写入到MySQL，关键这些日志可能连着来好几条才是一个用户的，因为发起请求，响应等每个环节都有相应的日志，这几条日志综合处理才能得到最终的结果，请问博主有什么好的方法没有？</a></p><p>71、<a href="https://t.zsxq.com/qfie6qR">flink 支持hadoop 主备么？ hadoop主节点挂了 flink 会切换到hadoop 备用节点？</a></p><p>72、<a href="https://t.zsxq.com/ZVZzZv7">请教大家: 实际 flink 开发中用 scala 多还是 java多些？ 刚入手 flink 大数据 scala 需要深入学习么？</a></p><p>73、<a href="https://t.zsxq.com/Qzbi6yn">我使用的是flink是1.7.2最近用了split的方式分流，但是底层的SplitStream上却标注为Deprecated，请问是官方不推荐使用分流的方式吗？</a></p><p>74、<a href="https://t.zsxq.com/Auf2NVR">KeyBy 的正确理解，和数据倾斜问题的解释</a></p><p>75、<a href="https://t.zsxq.com/3vnIm62">用flink时，遇到个问题 checkpoint大概有2G左右， 有背压时，flink会重启有遇到过这个问题吗</a></p><p>76、<a href="https://t.zsxq.com/URzVBIm">flink使用yarn-session方式部署，如何保证yarn-session的稳定性，如果yarn-session挂了，需要重新部署一个yarn-session，如何恢复之前yarn-session上的job呢，之前的checkpoint还能使用吗？</a></p><p>77、<a href="https://t.zsxq.com/MjyN7Uf">我想请教一下关于sink的问题。我现在的需求是从Kafka消费Json数据，这个Json数据字段可能会增加，然后将拿到的json数据以parquet的格式存入hdfs。现在我可以拿到json数据的schema，但是在保存parquet文件的时候不知道怎么处理。一是flink没有专门的format parquet，二是对于可变字段的Json怎么处理成parquet比较合适？</a></p><p>78、<a href="https://t.zsxq.com/6qBqVvZ">flink如何在较大的数据量中做去重计算。</a></p><p>79、<a href="https://t.zsxq.com/Eqjyju7">flink能在没有数据的时候也定时执行算子吗？</a></p><p>80、<a href="https://t.zsxq.com/i2zVfIi">使用rocksdb状态后端，自定义pojo怎么实现序列化和反序列化的，有相关demo么？</a></p><p>81、<a href="https://t.zsxq.com/vRJujAi">check point 老是失败，是不是自定义的pojo问题？到本地可以，到hdfs就不行，网上也有很多类似的问题 都没有一个很好的解释和解决方案</a></p><p>82、<a href="https://t.zsxq.com/MVFmuB6">cep规则如图，当start事件进入时，时间00:00:15，而后进入end事件，时间00:00:40。我发现规则无法命中。请问within 是从start事件开始计时？还是跟window一样根据系统时间划分的？如果是后者，请问怎么配置才能从start开始计时？</a></p><p>83、<a href="https://t.zsxq.com/EybM3vR">Flink聚合结果直接写Mysql的幂等性设计问题</a></p><p>84、<a href="https://t.zsxq.com/62VzNRF">Flink job打开了checkpoint，用的rocksdb，通过观察hdfs上checkpoint目录，为啥算副本总量会暴增爆减</a></p><p>85、<a href="">Flink 提交任务的 jar包可以指定路径为 HDFS 上的吗</a></p><p>86、<a href="https://t.zsxq.com/VfimieI">在flink web Ui上提交的任务，设置的并行度为2，flink是stand alone部署的。两个任务都正常的运行了几天了，今天有个地方逻辑需要修改，于是将任务cancel掉(在命令行cancel也试了)，结果taskmanger挂掉了一个节点。后来用其他任务试了，也同样会导致节点挂掉</a></p><p>87、<a href="https://t.zsxq.com/nee6qRv">一个配置动态更新的问题折腾好久（配置用个静态的map变量存着，有个线程定时去数据库捞数据然后存在这个map里面更新一把），本地 idea 调试没问题，集群部署就一直报 空指针异常。下游的算子使用这个静态变量map去get key在集群模式下会出现这个空指针异常，估计就是拿不到 map</a></p><p>88、<a href="https://t.zsxq.com/3bEUZfQ">批量写入MySQL，完成HBase批量写入</a></p><p>89、<a href="https://t.zsxq.com/Zb6AM3V">用flink清洗数据，其中要访问redis，根据redis的结果来决定是否把数据传递到下流，这有可能实现吗？</a></p><p>90、<a href="https://t.zsxq.com/RbeYZvb">监控页面流处理的时候这个发送和接收字节为0。</a></p><p>91、<a href="https://t.zsxq.com/MN7iuZf">sink到MySQL，如果直接用idea的话可以运行，并且成功，大大的代码上面用的FlinkKafkaConsumer010，而我的Flink版本为1.7，kafka版本为2.12，所以当我用FlinkKafkaConsumer010就有问题，于是改为<br>    FlinkKafkaConsumer就可以直接在idea完成sink到MySQL，但是为何当我把该程序打成Jar包，去运行的时候，就是报FlinkKafkaConsumer找不到呢</a></p><p>92、<a href="https://t.zsxq.com/e2VNN7Y">SocketTextStreamWordCount中输入中文统计不出来，请问这个怎么解决，我猜测应该是需要修改一下代码，应该是这个例子默认统计英文</a></p><p>93、<a href="https://t.zsxq.com/RVRn6AE"> Flink 应用程序本地 ide 里面运行的时候并行度是怎么算的？</a></p><p>等等等，还有很多，复制粘贴的我手累啊 😂</p><p>另外里面还会及时分享 Flink 的一些最新的资料（包括数据、视频、PPT、优秀博客，持续更新，保证全网最全，因为我知道 Flink 目前的资料还不多）</p><p><a href="https://t.zsxq.com/AybAimM">关于自己对 Flink 学习的一些想法和建议</a></p><p><a href="https://t.zsxq.com/iaEiyB2">Flink 全网最全资料获取，持续更新，点击可以获取</a></p><p>再就是星球用户给我提的一点要求：不定期分享一些自己遇到的 Flink 项目的实战，生产项目遇到的问题，是如何解决的等经验之谈！</p><p>1、<a href="https://t.zsxq.com/Zz3ny3V">如何查看自己的 Job 执行计划并获取执行计划图</a></p><p>2、<a href="https://t.zsxq.com/AIAQrnq">当实时告警遇到 Kafka 千万数据量堆积该咋办？</a></p><p>3、<a href="https://t.zsxq.com/QnYjy7M">如何在流数据中比两个数据的大小？多种解决方法</a></p><p>4、<a href="https://t.zsxq.com/6Q3vN3b">kafka 系列文章</a></p><p>5、<a href="https://t.zsxq.com/iiYfMBe">Flink环境部署、应用配置及运行应用程序</a></p><p>6、<a href="https://t.zsxq.com/yfYrvFA">监控平台该有架构是长这样子的</a></p><p>7、<a href="https://t.zsxq.com/beu7Mvj">《大数据“重磅炸弹”——实时计算框架 Flink》专栏系列文章目录大纲</a></p><p>8、<a href="https://t.zsxq.com/UvrRNJM">《大数据“重磅炸弹”——实时计算框架 Flink》Chat 付费文章</a></p><p>9、<a href="https://t.zsxq.com/zjQvjeM">Apache Flink 是如何管理好内存的？</a></p><p>10、<a href="https://t.zsxq.com/eYNBaAa">Flink On K8s</a></p><p>当然，除了更新 Flink 相关的东西外，我还会更新一些大数据相关的东西，因为我个人之前不是大数据开发，所以现在也要狂补些知识！总之，希望进来的童鞋们一起共同进步！</p><p>1、<a href="https://t.zsxq.com/7I6Iyrf">Java 核心知识点整理.pdf</a></p><p>2、<a href="https://t.zsxq.com/myJYZRF">假如我是面试官，我会问你这些问题</a></p><p>3、<a href="https://t.zsxq.com/iUZnamE">Kafka 系列文章和学习视频</a></p><p>4、<a href="https://t.zsxq.com/r7eIeyJ">重新定义 Flink 第二期 pdf</a></p><p>5、<a href="https://t.zsxq.com/ZjiYrVr">GitChat Flink 文章答疑记录</a></p><p>6、<a href="https://t.zsxq.com/QZVJyz7">Java 并发课程要掌握的知识点</a></p><p>7、<a href="https://t.zsxq.com/VVN7YB2">Lightweight Asynchronous Snapshots for Distributed Dataflows</a></p><p>8、<a href="https://t.zsxq.com/VVN7YB2">Apache Flink™- Stream and Batch Processing in a Single Engine</a></p><p>9、<a href="https://t.zsxq.com/NjAQFi2">Flink状态管理与容错机制</a></p><p>10、<a href="https://t.zsxq.com/MvfUvzN">Flink 流批一体的技术架构以及在阿里 的实践</a></p><p>11、<a href="https://t.zsxq.com/QVFqjea">Flink Checkpoint-轻量级分布式快照</a></p><p>12、<a href="https://t.zsxq.com/MvfUvzN">Flink 流批一体的技术架构以及在阿里 的实践</a></p><p>13、<a href="https://t.zsxq.com/N37mUzB">Stream Processing with Apache Flink pdf</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-02-flink.png&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 从0到1学习—— 分享四本 Flink 国外的书和二十多篇 Paper 论文</title>
    <link href="http://www.54tianzhisheng.cn/2019/06/13/flink-book-paper/"/>
    <id>http://www.54tianzhisheng.cn/2019/06/13/flink-book-paper/</id>
    <published>2019-06-12T16:00:00.000Z</published>
    <updated>2019-06-13T13:10:35.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-13-124033.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前也分享了不少自己的文章，但是对于 Flink 来说，还是有不少新入门的朋友，这里给大家分享点 Flink 相关的资料（国外数据 pdf 和流处理相关的 Paper），期望可以帮你更好的理解 Flink。</p><h3 id="书籍"><a href="#书籍" class="headerlink" title="书籍"></a>书籍</h3><p>1、《Introduction to Apache Flink book》</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-13-124410.jpg" alt=""></p><p>这本书比较薄，简单介绍了 Flink，也有中文版，读完可以对 Flink 有个大概的了解。 </p><p>2、《Learning Apache Flink》</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-13-124646.jpg" alt=""></p><p>这本书还是讲的比较多的 API 使用，不仅有 Java 版本还有 Scala 版本，入门看这本我觉得还是 OK 的。</p><p>3、《Stream Processing with Apache Flink》</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-13-124858.jpg" alt=""></p><p>这本书是 Flink PMC 写的，质量还是很好的，对 Flink 中的概念讲的很清楚，还有不少图片帮忙理解，美中不足的是没有 Table 和 SQL API 相关的介绍。</p><p>4、《Streaming System》</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-13-125217.jpg" alt=""></p><p>这本书是讲流处理引擎的，对流处理引擎的发展带来不少的推动，书本的质量非常高，配了大量的图，目的就是让你很容易的懂流处理引擎中的概念（比如时间、窗口、水印等），我强烈的推荐大家都看一下，这本书的内容被很多博客和书籍都引用了。</p><h3 id="Paper"><a href="#Paper" class="headerlink" title="Paper"></a>Paper</h3><p>这是一份 streaming systems 领域相关的论文列表 20+ 篇，涉及 streaming systems 的设计，实现，故障恢复，弹性扩展等各方面。也包含自 2014 年以来 streaming system 和 batch system 的统一模型的论文。</p><h4 id="2016-年"><a href="#2016-年" class="headerlink" title="2016 年"></a>2016 年</h4><ul><li>Drizzle: Fast and Adaptable Stream Processing at Scale (Draft): Record-at-a-time 的系统，如 Naiad, Flink，处理延迟较低、但恢复延迟较高；micro-batch 系统，如 Spark Streaming，恢复延迟低但处理延迟略高。Drizzle 则采用 group scheduling + pre-scheduling shuffles 的方式对 Spark Streaming 做了改进，保留低恢复延迟的同时，降低了处理延迟至 100ms 量级。</li></ul><ul><li>Realtime Data Processing at Facebook (SIGMOD): Facebook 明确自己实时的使用场景是 seconds of latency, not milliseconds，并基于自己的需求构建了 3 个实时处理组件：Puma, Swift, 以及 Stylus。Puma, Swift 和 Stylus 都从 Scribe 读数据，并可向 Scribe 写回数据（Scribe 是 Facebook 内部的分布式消息系统，类似 Kafka）。</li></ul><h4 id="2015-年"><a href="#2015-年" class="headerlink" title="2015 年"></a>2015 年</h4><ul><li>The Dataflow Model: A Practical Approach to Balancing Correctness, Latency, and Cost in Massive-Scale, Unbounded, Out-of-Order Data Processing (VLDB): 来自 Google 的将 stream processing 模型和 batch processing 模型统一的尝试。在 Dataflow model 下，底层依赖 FlumeJava 支持 batch processing，依赖 MillWheel 支持 stream processing。Dataflow model 的开源实现是 Apache Beam 项目。</li></ul><ul><li>Apache Flink: Stream and Batch Processing in a Single Engine Apache Flink 是一个处理 streaming data 和 batch data 的开源系统。Flink 的设计哲学是，包括实时分析 (real-time analytics)、持续数据处理 (continuous data pipelines)、历史数据处理 (historic data processing / batch)、迭代式算法 (iterative algorithms - machine learning, graph analysis) 等的很多类数据处理应用，都能用 pipelined fault-tolerant 的 dataflows 执行模型来表达。</li></ul><ul><li><a href="https://arxiv.org/pdf/1506.08603.pdf">Lightweight asynchronous snapshots for distributed dataflows</a>: Apache Flink 所实现的一个轻量级的、异步做状态快照的方法。基于此，Flink 得以保证分布式状态的一致性，从而保证整个系统的 exactly-once 语义。具体的，Flink 会持续性的在 stream 里插入 barrier markers，制造一个分布式的顺序关系，使得不同的节点能够在同一批 barrier marker 上达成整个系统的一致性状态。</li></ul><ul><li><a href="https://pdfs.semanticscholar.org/e847/c3ec130da57328db79a7fea794b07dbccdd9.pdf">Twitter Heron: Stream Processing at Scale</a> (SIGMOD): Heron 是 Twitter 开发的用于代替 Storm 的实时处理系统，解决了 Storm 在扩展性、调试能力、性能、管理方式上的一些问题。Heron 实现了 Storm 的接口，因此对 Storm 有很好的兼容性，也成为了 Twitter 内部实时处理系统的事实上的标准。</li></ul><h4 id="2014-年"><a href="#2014-年" class="headerlink" title="2014 年"></a>2014 年</h4><ul><li>Trill: A High-Performance Incremental Query Processor for Diverse Analytics (VLDB): 此篇介绍了 Microsoft 的 Trill - 一个新的分析查询处理器。Trill 很好的结合以下 3 方面需求：(1) <em>Query Model</em>: Trill 是基于时间-关系 (tempo-relational) 模型，所以很好的支持从实时到离线计算的延迟需求；(2) <em>Fabric and Language Integration</em>: Trill 作为一个类库，可以很好的与高级语言、已有类库结合；以及 (3) <em>Performance</em>: 无论实时还是离线，Trill 的 throughput 都很高 —— 实时计算比流处理引擎高 2-4 个数量级，离线计算与商业的列式 DBMS 同等。从实现角度讲，包括 punctuation 的使用来分 batch 满足 latency 需求，batch 内使用列式存储、code-gen 等技术来提高 performance，都具有很好的借鉴意义 —— 尤其注意这是 2014 年发表的论文。</li></ul><ul><li>Summingbird: A Framework for Integrating Batch and Online MapReduce Computations (VLDB): Twitter 开发的目标是将 online Storm 计算和 batch MapReduce 计算逻辑统一描述的一套 domain-specific language。Summingbird 抽象了 sources, sinks, 以及 stores 等，基于此抽象，上层应用就不必为 streaming 和 batch 维护两套计算逻辑，而可以使用同一套计算逻辑，只在运行时分别编译后跑在 streaming 的 Storm 上和 batch 的 MapReduce 上。</li></ul><ul><li>Storm@Twitter (SIGMOD): 这是一篇来迟的论文。Apache Storm 最初在 Backtype 及 Twitter，而后在业界范围都有广泛的应用，甚至曾经一度也是事实上的流处理系统标准。此篇介绍了 Storm 的设计，及在 Twitter 内部的应用情况。当然后面我们知道 Apache Storm 也暴露出一些问题，业界也出现了一些更优秀的流处理系统。Twitter 虽没有在 2012 年 Storm 时代开启时发声，但在 2014 年 Storm 落幕时以此文发声向其致敬，也算是弥补了些许遗憾吧。</li></ul><h4 id="2013-年"><a href="#2013-年" class="headerlink" title="2013 年"></a>2013 年</h4><ul><li>Discretized Streams: Fault-Tolerant Streaming Computation at Scale (SOSP): Spark Streaming 是基于 Spark 执行引擎、micro-batch 模式的准实时处理系统。对比 RDD 是 Spark 引擎的数据抽象，DStream (Discretized Stream) 则是 Spark Streaming 引擎的数据抽象。DStream 像 RDD 一样，具有分布式、可故障恢复的特点，并且能够充分利用 Spark 引擎的推测执行，应对 straggler 的出现。</li></ul><ul><li>MillWheel: Fault-Tolerant Stream Processing at Internet Scale (VLDB): MillWheel 是 Google 内部研发的实时流数据处理系统，具有分布式、低延迟、高可用、支持 exactly-once 语义的特点。不出意外，MillWheel 是 Google 强大 infra structure 和强大 engeering 能力的综合体现 —— 利用 Bigtable/Spanner 作为后备状态存储、保证 exactly-once 特性等等。另外，MillWheel 将 watermark 机制发扬光大，对 event time 有着非常好的支持。推荐对 streaming system 感兴趣的朋友一定多读几遍此篇论文 —— 虽然此篇已经发表了几年，但工业界开源的系统尚未完全达到 MillWheel 的水平。</li></ul><ul><li>Integrating Scale Out and Fault Tolerance in Stream Processing using Operator State Management (SIGMOD): 针对有状态的算子的状态，此篇的基本洞察是，scale out 和 fault tolerance 其实很相通，应该结合到一起考虑和实现，而不是将其割裂开来。文章提出了算子的 3 类状态：(a) processing state, (b) buffer state, 和 (c) routing state，并提出了算子状态的 4 个操作原语：(1) checkpoint state, (2) backup state, (3) restore state, (4) partition state。</li></ul><h4 id="2010-年"><a href="#2010-年" class="headerlink" title="2010 年"></a>2010 年</h4><ul><li>S4: Distributed Stream Computing Platform (ICDMW): 2010 年算是 general stream processing engine 元年 —— Yahoo! 研发并发布了 S4, Backtype 开始研发了 Storm 并将在 1 年后（由 Twitter）将其开源。S4 和 Storm 都是 general-purpose 的 stream processing engine，允许用户通过代码自定义计算逻辑，而不是仅仅是使用声明式的语言或算子。</li></ul><h4 id="2008-年"><a href="#2008-年" class="headerlink" title="2008 年"></a>2008 年</h4><ul><li>Out-of-Order Processing: A New Architecture for HighPerformance Stream System (VLDB): 这篇文章提出了一种新的处理模型，即 out-of-order processing (OOP)，取消了以往 streaming system 里对事件有序的假设。重要的是，这篇文章提出了并实现了 low watermark: lwm(n, S, A) is the smallest value for A that occurs after prefix Sn of stream S。我们看到，在 2 年后 Google 开始研发的 MillWheel 里，watermark 将被发扬光大。</li></ul><ul><li>Fast and Highly-Available Stream Processing over Wide Area Networks (ICDE): 针对广域网 (wide area networks) 的 stream processing 设计的快速、高可用方案。主要思想是依靠 replication。</li></ul><h4 id="2007-年"><a href="#2007-年" class="headerlink" title="2007 年"></a>2007 年</h4><ul><li>A Cooperative, Self-Configuring High-Availability Solution for Stream Processing (ICDE): 与 2005 年 ICDE 的文章一样，此篇也讨论 stream processing 的高可用问题。与 2005 年文章做法不同的是，此篇的 checkpointing 方法更细粒度一些，所以一个节点上的不同状态能够备份到不同的节点上去，因而在恢复的时候能够并行恢复以提高速度。</li></ul><h4 id="2005-年"><a href="#2005-年" class="headerlink" title="2005 年"></a>2005 年</h4><ul><li>The 8 Requirements of Real-Time Stream Processing (SIGMOD): 图领奖得主 Michael Stonebraker 老爷子与他在 StreamBase 的小伙伴们勾画的 stream processing applications 应当满足的 8 条规则，如 Rule 1: Keep the Data Moving, Rule 2: Query using SQL on Streams (StreamSQL), Rule 3: Handle Stream Imperfections (Delayed, Missing and Out-of-Order Data) … 等等。虽然此篇有引导舆论的嫌疑 —— 不知是先有了这流 8 条、再有了 StreamBase，还是先有了 StreamBase、再有了这流 8 条 —— 但其内容还是有相当的借鉴意义。</li></ul><ul><li>The Design of the Borealis Stream Processing Engine (CIDR): Borealis 是 Aurora 的分布式、更优化版本的续作。Borealis 提出并解决了 3 个新一代系统的基础问题：(1) dynamic revision of query results, (2) dynamic query modification, 以及 (3) flexible and highly-scalable optimization. 此篇讲解了 Borealis 的设计与实现 —— p.s. 下，Aurora 及续作 Borealis 的命名还真是非常讲究，是学院派的风格 :-D</li></ul><ul><li>High-availability algorithms for distributed stream processing (ICDE): 此篇主要聚焦在 streaming system 的高可用性，即故障恢复。文章提出了 3 种 recovery types: (a) precise, (b) gap, 和 (c) rollback，并通过 (1) passive standby, (2) upstream backup, (3) active standby 的方式进行 recover。可与 2007 年 ICDE 的文章对比阅读。</li></ul><h4 id="2004-年"><a href="#2004-年" class="headerlink" title="2004 年"></a>2004 年</h4><ul><li>STREAM: The Stanford Data Stream Management System (Technique Report): 这篇 technique report 定义了一种 Continuous Query Language (CQL)，讲解了 Query Plans 和 Execution，讨论了一些 Performance Issues。系统也注意到并讨论了 Adaptivity 和 Approximation 的问题。从这篇 technique report 可以看出，这时的流式计算，更多是传统 RDBMS 的思路，扩展到了处理实时流式数据；这大约也是 2010 以前的 stream processing 相关研究的缩影。</li></ul><h4 id="2002-年"><a href="#2002-年" class="headerlink" title="2002 年"></a>2002 年</h4><ul><li>Monitoring Streams – A New Class of Data Management Applications (VLDB): 大约在 2002 年前后，从实时数据监控（如监控 sensors 数据等）应用出发，大家已经开始区分传统的查询主动、数据被动 (Human-Active, DBMS-Passive) 模式和新兴的数据主动、查询被动 (DBMS-Active, Human-Passive) 模式的区别 —— 此篇即是其中的典型代表。此篇提出了新式的 DBMS 的 Aurora，描述了其基本系统模型、面向流式数据的操作算子集、 优化策略、及实时应用。</li></ul><ul><li>Exploiting Punctuation Semantics in Continuous Data Streams (TKDE): 此篇很早的注意到了一些传统的操作算子不能用于无尽的数据流入的场景，因为将导致无尽的状态（考虑 outer join），或者无尽的阻塞（考虑 count 或 max）等。此篇提出，如果在 stream 里加入一些特殊的 punctuation，来标识一段一段的数据，那么我们就可以把无限的 stream 划分为多个有限的数据集的集合，从而使得之前提到的算子变得可用。此篇的价值更多体现在给了 2008 年 watermark 相关的文章以基础，乃至集大成在了 2010 年 Google MillWheel 中。</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文分享了四本 Flink 相关的书籍和一份 streaming systems 领域相关的论文列表 20+ 篇，涉及 streaming systems 的设计，实现，故障恢复，弹性扩展等各方面。</p><p>如何获取呢？你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6Elrml.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/uPBJ5b.jpg" alt=""></p><p>另外你如果感兴趣的话，也可以关注我的公众号。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-13-130351.jpg" alt=""></p><p>本篇文章连接是：<a href="http://www.54tianzhisheng.cn/2019/06/13/flink-book-paper/">http://www.54tianzhisheng.cn/2019/06/13/flink-book-paper/</a></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="Flink-实战"><a href="#Flink-实战" class="headerlink" title="Flink 实战"></a>Flink 实战</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">Flink 从0到1学习—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">Flink 从0到1学习—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">Flink 从0到1学习—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">Flink 从0到1学习—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">Flink 从0到1学习—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">Flink 从0到1学习—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">Flink 从0到1学习—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">Flink 从0到1学习—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">Flink 从0到1学习—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">Flink 从0到1学习—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">Flink 从0到1学习—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">Flink 从0到1学习—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">Flink 从0到1学习—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">Flink 从0到1学习—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">Flink 从0到1学习—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">Flink 从0到1学习—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">Flink 从0到1学习—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">Flink 从0到1学习》—— 你上传的 jar 包藏到哪里去了?</a></p><p>19、<a href="http://www.54tianzhisheng.cn/2019/03/28/flink-additional-data/">Flink 从0到1学习 —— Flink 中如何管理配置？</a></p><p>20、<a href="http://www.54tianzhisheng.cn/2019/06/12/flink-split/">Flink 从0到1学习—— Flink 不可以连续 Split(分流)？</a></p><p>21、<a href="http://www.54tianzhisheng.cn/2019/12/31/Flink-resources/">Flink 全网最全资源（视频、博客、PPT、入门、实战、源码解析、问答等持续更新）</a></p><h3 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h3><p>1、<a href="https://t.zsxq.com/UZfaYfE">Flink 源码解析 —— 源码编译运行</a></p><p>2、<a href="https://t.zsxq.com/zZZjaYf">Flink 源码解析 —— 项目结构一览</a></p><p>3、<a href="https://t.zsxq.com/zV7MnuJ">Flink 源码解析—— local 模式启动流程</a></p><p>4、<a href="https://t.zsxq.com/QZVRZJA">Flink 源码解析 —— standalonesession 模式启动流程</a></p><p>5、<a href="https://t.zsxq.com/u3fayvf">Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动</a></p><p>6、<a href="https://t.zsxq.com/MnQRByb">Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动</a></p><p>7、<a href="https://t.zsxq.com/YJ2Zrfi">Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程</a></p><p>8、<a href="https://t.zsxq.com/qnMFEUJ">Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程</a></p><p>9、<a href="https://t.zsxq.com/naaMf6y">Flink 源码解析 —— 如何获取 JobGraph？</a></p><p>10、<a href="https://t.zsxq.com/qRFIm6I">Flink 源码解析 —— 如何获取 StreamGraph？</a></p><p>11、<a href="https://t.zsxq.com/2VRrbuf">Flink 源码解析 —— Flink JobManager 有什么作用？</a></p><p>12、<a href="https://t.zsxq.com/RZbu7yN">Flink 源码解析 —— Flink TaskManager 有什么作用？</a></p><p>13、<a href="https://t.zsxq.com/zV7MnuJ">Flink 源码解析 —— JobManager 处理 SubmitJob 的过程</a></p><p>14、<a href="https://t.zsxq.com/zV7MnuJ">Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程</a></p><p>15、<a href="https://t.zsxq.com/ynQNbeM">Flink 源码解析 —— 深度解析 Flink Checkpoint 机制</a></p><p>16、<a href="https://t.zsxq.com/JaQfeMf">Flink 源码解析 —— 深度解析 Flink 序列化机制</a></p><p>17、<a href="https://t.zsxq.com/zjQvjeM">Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-13-124033.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 从0到1学习—— Flink 不可以连续 Split(分流)？</title>
    <link href="http://www.54tianzhisheng.cn/2019/06/12/flink-split/"/>
    <id>http://www.54tianzhisheng.cn/2019/06/12/flink-split/</id>
    <published>2019-06-11T16:00:00.000Z</published>
    <updated>2019-06-12T16:15:05.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-12-161415.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>今天上午被 Flink 的一个算子困惑了下，具体问题是什么呢？</p><p>我有这么个需求：有不同种类型的告警数据流(包含恢复数据)，然后我要将这些数据流做一个拆分，拆分后的话，每种告警里面的数据又想将告警数据和恢复数据拆分出来。</p><p>结果，这个需求用 Flink 的 Split 运算符出现了问题。</p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>需求如下图所示：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-12-141928.jpg" alt=""></p><p>我是期望如上这样将数据流进行拆分的，最后将每种告警和恢复用不同的消息模版做一个渲染，渲染后再通过各种其他的方式（钉钉群<br>邮件、短信）进行告警通知。</p><p>于是我的代码大概的结构如下代码所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//dataStream 是总的数据流</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//split 是拆分后的数据流</span></span><br><span class="line">SplitStream&lt;AlertEvent&gt; split = dataStream.split(<span class="keyword">new</span> OutputSelector&lt;AlertEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(AlertEvent value)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; tags = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">switch</span> (value.getType()) &#123;</span><br><span class="line">            <span class="keyword">case</span> MIDDLEWARE:</span><br><span class="line">                tags.add(MIDDLEWARE);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> HEALTH_CHECK:</span><br><span class="line">                tags.add(HEALTH_CHECK);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> DOCKER:</span><br><span class="line">                tags.add(DOCKER);</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="comment">//...</span></span><br><span class="line">            <span class="comment">//当然这里还可以很多种类型</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tags;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line"></span><br><span class="line"><span class="comment">//然后你想获取每种不同的数据类型，你可以使用 select</span></span><br><span class="line">DataStream&lt;AlertEvent&gt; middleware = split.select(MIDDLEWARE);   <span class="comment">//选出中间件的数据流</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//然后你又要将中间件的数据流分流成告警和恢复</span></span><br><span class="line">SplitStream&lt;AlertEvent&gt; middlewareSplit = middleware.split(<span class="keyword">new</span> OutputSelector&lt;AlertEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(AlertEvent value)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; tags = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(value.isRecover()) &#123;</span><br><span class="line">            tags.add(RECOVER)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            tags.add(ALERT)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tags;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">middlewareSplit.select(ALERT).print();    </span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">DataStream&lt;AlertEvent&gt; healthCheck = split.select(HEALTH_CHECK);   <span class="comment">//选出健康检查的数据流</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//然后你又要将健康检查的数据流分流成告警和恢复</span></span><br><span class="line">SplitStream&lt;AlertEvent&gt; healthCheckSplit = healthCheck.split(<span class="keyword">new</span> OutputSelector&lt;AlertEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(AlertEvent value)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; tags = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(value.isRecover()) &#123;</span><br><span class="line">            tags.add(RECOVER)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            tags.add(ALERT)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tags;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">healthCheckSplit.select(ALERT).print();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">DataStream&lt;AlertEvent&gt; docekr = split.select(DOCKER);   <span class="comment">//选出容器的数据流</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//然后你又要将容器的数据流分流成告警和恢复</span></span><br><span class="line">SplitStream&lt;AlertEvent&gt; dockerSplit = docekr.split(<span class="keyword">new</span> OutputSelector&lt;AlertEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Iterable&lt;String&gt; <span class="title">select</span><span class="params">(AlertEvent value)</span> </span>&#123;</span><br><span class="line">        List&lt;String&gt; tags = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">if</span>(value.isRecover()) &#123;</span><br><span class="line">            tags.add(RECOVER)</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            tags.add(ALERT)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> tags;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br><span class="line">dockerSplit.select(ALERT).print();</span><br></pre></td></tr></table></figure><p>结构我抽象后大概就长上面这样，然后我先本地测试的时候只把容器的数据那块代码打开了，其他种告警的分流代码注释掉了，一运行，发现竟然容器告警的数据怎么还掺杂着健康检查的数据也一起打印出来了，一开始我以为自己出了啥问题，就再起码运行了三遍 IDEA 才发现结果一直都是这样的。</p><p>于是，我只好在第二步分流前将 docekr 数据流打印出来，发现是没什么问题，打印出来的数据都是容器相关的，没有掺杂着其他种的数据啊。这会儿遍陷入了沉思，懵逼发呆了一会。</p><h3 id="解决问题"><a href="#解决问题" class="headerlink" title="解决问题"></a>解决问题</h3><p>于是还是开始面向 Google 编程：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-12-145537.jpg" alt=""></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-12-145652.jpg" alt=""></p><p>发现第一条就找到答案了，简直不要太快，点进去可以看到他也有这样的需求：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-12-150438.jpg" alt=""></p><p>然后这个小伙伴还挣扎了下用不同的方法（虽然结果更惨）：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-12-150638.jpg" alt=""></p><p>最后换了个姿势就好了（果然小伙子会的姿势挺多的）：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-12-150821.jpg" alt=""></p><p>但从这篇文章中，我找到了关联到的两个 Flink Issue，分别是：</p><p>1、<a href="https://issues.apache.org/jira/browse/FLINK-5031">https://issues.apache.org/jira/browse/FLINK-5031</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-12-151155.jpg" alt=""></p><p>2、<a href="https://issues.apache.org/jira/browse/FLINK-11084">https://issues.apache.org/jira/browse/FLINK-11084</a></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-12-151228.jpg" alt=""></p><p>然后呢，从第二个 Issue 的讨论中我发现了一些很有趣的讨论：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-12-152012.jpg" alt=""></p><p>对话很有趣，但是我突然想到之前我的知识星球里面一位很细心的小伙伴问的一个问题了：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-12-152715.jpg" alt=""></p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-12-152900.jpg" alt=""></p><p>可以发现代码上确实是标明了过期了，但是注释里面没写清楚推荐用啥，幸好我看到了这个 Issue，不然脑子里面估计这个问题一直会存着呢。</p><p>那么这个问题解决方法是不是意味着就可以利用 Side Outputs 来解决呢？当然可以啦，官方都推荐了，还不能都话，那么不是打脸啪啪啪的响吗？不过这里还是卖个关子将 Side Outputs 后面专门用一篇文章来讲，感兴趣的可以先看看官网介绍：<a href="https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/side_output.html">https://ci.apache.org/projects/flink/flink-docs-stable/dev/stream/side_output.html</a></p><p>另外其实也可以通过 split + filter 组合来解决这个问题，反正关键就是不要连续的用 split 来分流。</p><p>用 split + filter 的方案代码大概如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;AlertEvent&gt; docekr = split.select(DOCKER);   <span class="comment">//选出容器的数据流</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//容器告警的数据流</span></span><br><span class="line">docekr.filter(<span class="keyword">new</span> FilterFunction&lt;AlertEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(AlertEvent value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> !value.isRecover();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.print();</span><br><span class="line">        </span><br><span class="line"><span class="comment">//容器恢复的数据流        </span></span><br><span class="line">docekr.filter(<span class="keyword">new</span> FilterFunction&lt;AlertEvent&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(AlertEvent value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> value.isRecover();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.print();</span><br></pre></td></tr></table></figure><p>上面这种就是多次 filter 也可以满足需求，但是就是代码有点啰嗦。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Flink 中不支持连续的 Split/Select 分流操作，要实现连续分流也可以通过其他的方式（split + filter 或者 side output）来实现</p><p>本篇文章连接是：<a href="http://www.54tianzhisheng.cn/2019/06/12/flink-split/">http://www.54tianzhisheng.cn/2019/06/12/flink-split/</a></p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6Elrml.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/uPBJ5b.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="Flink-实战"><a href="#Flink-实战" class="headerlink" title="Flink 实战"></a>Flink 实战</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">Flink 从0到1学习—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">Flink 从0到1学习—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">Flink 从0到1学习—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">Flink 从0到1学习—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">Flink 从0到1学习—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">Flink 从0到1学习—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">Flink 从0到1学习—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">Flink 从0到1学习—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">Flink 从0到1学习—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">Flink 从0到1学习—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">Flink 从0到1学习—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">Flink 从0到1学习—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">Flink 从0到1学习—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">Flink 从0到1学习—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">Flink 从0到1学习—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">Flink 从0到1学习—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">Flink 从0到1学习—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">Flink 从0到1学习》—— 你上传的 jar 包藏到哪里去了?</a></p><p>19、<a href="http://www.54tianzhisheng.cn/2019/03/28/flink-additional-data/">Flink 从0到1学习 —— Flink 中如何管理配置？</a></p><h3 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h3><p>1、<a href="https://t.zsxq.com/UZfaYfE">Flink 源码解析 —— 源码编译运行</a></p><p>2、<a href="https://t.zsxq.com/zZZjaYf">Flink 源码解析 —— 项目结构一览</a></p><p>3、<a href="https://t.zsxq.com/zV7MnuJ">Flink 源码解析—— local 模式启动流程</a></p><p>4、<a href="https://t.zsxq.com/QZVRZJA">Flink 源码解析 —— standalonesession 模式启动流程</a></p><p>5、<a href="https://t.zsxq.com/u3fayvf">Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动</a></p><p>6、<a href="https://t.zsxq.com/MnQRByb">Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动</a></p><p>7、<a href="https://t.zsxq.com/YJ2Zrfi">Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程</a></p><p>8、<a href="https://t.zsxq.com/qnMFEUJ">Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程</a></p><p>9、<a href="https://t.zsxq.com/naaMf6y">Flink 源码解析 —— 如何获取 JobGraph？</a></p><p>10、<a href="https://t.zsxq.com/qRFIm6I">Flink 源码解析 —— 如何获取 StreamGraph？</a></p><p>11、<a href="https://t.zsxq.com/2VRrbuf">Flink 源码解析 —— Flink JobManager 有什么作用？</a></p><p>12、<a href="https://t.zsxq.com/RZbu7yN">Flink 源码解析 —— Flink TaskManager 有什么作用？</a></p><p>13、<a href="https://t.zsxq.com/zV7MnuJ">Flink 源码解析 —— JobManager 处理 SubmitJob 的过程</a></p><p>14、<a href="https://t.zsxq.com/zV7MnuJ">Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程</a></p><p>15、<a href="https://t.zsxq.com/ynQNbeM">Flink 源码解析 —— 深度解析 Flink Checkpoint 机制</a></p><p>16、<a href="https://t.zsxq.com/JaQfeMf">Flink 源码解析 —— 深度解析 Flink 序列化机制</a></p><p>17、<a href="https://t.zsxq.com/zjQvjeM">Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-12-161415.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 从0到1学习 —— Flink 中如何管理配置？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/28/flink-additional-data/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/28/flink-additional-data/</id>
    <published>2019-03-27T16:00:00.000Z</published>
    <updated>2019-06-10T11:42:14.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-10-113303.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>如果你了解 Apache Flink 的话，那么你应该熟悉该如何像 Flink 发送数据或者如何从 Flink 获取数据。但是在某些情况下，我们需要将配置数据发送到 Flink 集群并从中接收一些额外的数据。</p><p>在本文的第一部分中，我将描述如何将配置数据发送到 Flink 集群。我们需要配置很多东西:方法参数、配置文件、机器学习模型。Flink 提供了几种不同的方法，我们将介绍如何使用它们以及何时使用它们。在本文的第二部分中，我将描述如何从 Flink 集群中获取数据。</p><h3 id="如何发送数据给-TaskManager？"><a href="#如何发送数据给-TaskManager？" class="headerlink" title="如何发送数据给 TaskManager？"></a>如何发送数据给 TaskManager？</h3><p>在我们深入研究如何在 Apache Flink 中的不同组件之间发送数据之前，让我们先谈谈 Flink 集群中的组件，下图展示了 Flink 中的主要组件以及它们是如何相互作用的：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-07-055044.jpg" alt=""></p><p>当我们运行 Flink 应用程序时，它会与 Flink JobManager 进行交互，这个 Flink JobManager 存储了那些正在运行的 Job 的详细信息，例如执行图。<br>JobManager 它控制着 TaskManager，每个 TaskManager 中包含了一部分数据来执行我们定义的数据处理方法。</p><p>在许多的情况下，我们希望能够去配置 Flink Job 中某些运行的函数参数。根据用例，我们可能需要设置单个变量或者提交具有静态配置的文件，我们下面将讨论在 Flink 中该如何实现？</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-07-062513.jpg" alt=""></p><p>除了向 TaskManager 发送配置数据外，有时我们可能还希望从 Flink Job 的函数方法中返回数据。</p><h3 id="如何配置用户自定义函数？"><a href="#如何配置用户自定义函数？" class="headerlink" title="如何配置用户自定义函数？"></a>如何配置用户自定义函数？</h3><p>假设我们有一个从 CSV 文件中读取电影列表的应用程序（它要过滤特定类型的所有电影）:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//读取电影列表数据集合</span></span><br><span class="line">DataSet&lt;Tuple3&lt;Long, String, String&gt;&gt; lines = env.readCsvFile(<span class="string">"movies.csv"</span>)</span><br><span class="line">        .ignoreFirstLine()</span><br><span class="line">        .parseQuotedStrings(<span class="string">'"'</span>)</span><br><span class="line">        .ignoreInvalidLines()</span><br><span class="line">        .types(Long.class, String.class, String.class);</span><br><span class="line"></span><br><span class="line">lines.filter((FilterFunction&lt;Tuple3&lt;Long, String, String&gt;&gt;) movie -&gt; &#123;</span><br><span class="line">    <span class="comment">// 以“|”符号分隔电影类型</span></span><br><span class="line">    String[] genres = movie.f2.split(<span class="string">"\\|"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 查找所有 “动作” 类型的电影</span></span><br><span class="line">    <span class="keyword">return</span> Stream.of(genres).anyMatch(g -&gt; g.equals(<span class="string">"Action"</span>));</span><br><span class="line">&#125;).print();</span><br></pre></td></tr></table></figure><p>我们很可能想要提取不同类型的电影，为此我们需要能够配置我们的过滤功能。 当你要实现这样的函数时，最直接的配置方法是实现构造函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 传递类型名称</span></span><br><span class="line">lines.filter(<span class="keyword">new</span> FilterGenre(<span class="string">"Action"</span>))</span><br><span class="line">    .print();</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FilterGenre</span> <span class="keyword">implements</span> <span class="title">FilterFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">Long</span>, <span class="title">String</span>, <span class="title">String</span>&gt;&gt; </span>&#123;</span><br><span class="line">    <span class="comment">//类型</span></span><br><span class="line">    String genre;</span><br><span class="line">    <span class="comment">//初始化构造方法</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">FilterGenre</span><span class="params">(String genre)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.genre = genre;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Tuple3&lt;Long, String, String&gt; movie)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] genres = movie.f2.split(<span class="string">"\\|"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Stream.of(genres).anyMatch(g -&gt; g.equals(genre));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>或者，如果你使用 lambda 函数，你可以简单地使用它的闭包中的一个变量:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> String genre = <span class="string">"Action"</span>;</span><br><span class="line"></span><br><span class="line">lines.filter((FilterFunction&lt;Tuple3&lt;Long, String, String&gt;&gt;) movie -&gt; &#123;</span><br><span class="line">    String[] genres = movie.f2.split(<span class="string">"\\|"</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用变量</span></span><br><span class="line">    <span class="keyword">return</span> Stream.of(genres).anyMatch(g -&gt; g.equals(genre));</span><br><span class="line">&#125;).print();</span><br></pre></td></tr></table></figure><p>Flink 将序列化此变量并将其与函数一起发送到集群。</p><p>如果你需要将大量变量传递给函数，那么这些方法就会变得非常烦人了。 为了解决这个问题，Flink 提供了 withParameters 方法。 要使用它，你需要实现那些 Rich 函数，比如你不必实现 MapFunction 接口，而是实现 RichMapFunction。</p><p>Rich 函数允许你使用 withParameters 方法传递许多参数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Configuration 类来存储参数</span></span><br><span class="line">Configuration configuration = <span class="keyword">new</span> Configuration();</span><br><span class="line">configuration.setString(<span class="string">"genre"</span>, <span class="string">"Action"</span>);</span><br><span class="line"></span><br><span class="line">lines.filter(<span class="keyword">new</span> FilterGenreWithParameters())</span><br><span class="line">        <span class="comment">// 将参数传递给函数</span></span><br><span class="line">        .withParameters(configuration)</span><br><span class="line">        .print();</span><br></pre></td></tr></table></figure><p>要读取这些参数，我们需要实现 “open” 方法并读取其中的参数: </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FilterGenreWithParameters</span> <span class="keyword">extends</span> <span class="title">RichFilterFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">Long</span>, <span class="title">String</span>, <span class="title">String</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    String genre;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//读取配置</span></span><br><span class="line">        genre = parameters.getString(<span class="string">"genre"</span>, <span class="string">""</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Tuple3&lt;Long, String, String&gt; movie)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] genres = movie.f2.split(<span class="string">"\\|"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Stream.of(genres).anyMatch(g -&gt; g.equals(genre));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所有这些选项都可以使用，但如果需要为多个函数设置相同的参数，则可能会很繁琐。在 Flink 中要处理此种情况， 你可以设置所有 TaskManager 都可以访问的全局环境变量。</p><p>为此，首先需要使用 <code>ParameterTool.fromArgs</code> 从命令行读取参数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String... args)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//读取命令行参数</span></span><br><span class="line">    ParameterTool parameterTool = ParameterTool.fromArgs(args);</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后使用 <code>setGlobalJobParameters</code> 设置全局作业参数:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.getConfig().setGlobalJobParameters(parameterTool);</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">//该函数将能够读取这些全局参数</span></span><br><span class="line">lines.filter(<span class="keyword">new</span> FilterGenreWithGlobalEnv()) <span class="comment">//这个函数是自己定义的</span></span><br><span class="line">                .print();</span><br></pre></td></tr></table></figure><p>现在我们来看看这个读取这些参数的函数，和上面说的一样，它是一个 Rich 函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FilterGenreWithGlobalEnv</span> <span class="keyword">extends</span> <span class="title">RichFilterFunction</span>&lt;<span class="title">Tuple3</span>&lt;<span class="title">Long</span>, <span class="title">String</span>, <span class="title">String</span>&gt;&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">filter</span><span class="params">(Tuple3&lt;Long, String, String&gt; movie)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] genres = movie.f2.split(<span class="string">"\\|"</span>);</span><br><span class="line">        <span class="comment">//获取全局的配置</span></span><br><span class="line">        ParameterTool parameterTool = (ParameterTool) getRuntimeContext().getExecutionConfig().getGlobalJobParameters();</span><br><span class="line">        <span class="comment">//读取配置</span></span><br><span class="line">        String genre = parameterTool.get(<span class="string">"genre"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> Stream.of(genres).anyMatch(g -&gt; g.equals(genre));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>要读取配置，我们需要调用 getGlobalJobParameter 来获取所有全局参数，然后使用 get 方法获取我们要的参数。</p><h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><p>如果你想将数据从客户端发送到 TaskManager，上面文章中讨论的方法都适合你，但如果数据以数据集的形式存在于 TaskManager 中，该怎么办？ 在这种情况下，最好使用 Flink 中的另一个功能 —— 广播变量。 它只允许将数据集发送给那些执行你 Job 里面函数的任务管理器。</p><p>假设我们有一个数据集，其中包含我们在进行文本处理时应忽略的单词，并且我们希望将其设置为我们的函数。 要为单个函数设置广播变量，我们需要使用 withBroadcastSet 方法和数据集。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">DataSet&lt;Integer&gt; toBroadcast = env.fromElements(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>);</span><br><span class="line"><span class="comment">// 获取要忽略的单词集合</span></span><br><span class="line">DataSet&lt;String&gt; wordsToIgnore = ...</span><br><span class="line"></span><br><span class="line">data.map(<span class="keyword">new</span> RichFlatMapFunction&lt;String, String&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 存储要忽略的单词集合. 这将存储在 TaskManager 的内存中</span></span><br><span class="line">    Collection&lt;String&gt; wordsToIgnore;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//读取要忽略的单词的集合</span></span><br><span class="line">        wordsToIgnore = getRuntimeContext().getBroadcastVariable(<span class="string">"wordsToIgnore"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">map</span><span class="params">(String line, Collector&lt;String&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] words = line.split(<span class="string">"\\W+"</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words)</span><br><span class="line">            <span class="comment">//使用要忽略的单词集合</span></span><br><span class="line">            <span class="keyword">if</span> (wordsToIgnore.contains(word))</span><br><span class="line">                out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//通过广播变量传递数据集</span></span><br><span class="line">&#125;).withBroadcastSet(wordsToIgnore, <span class="string">"wordsToIgnore"</span>);</span><br></pre></td></tr></table></figure><p>你应该记住，如果要使用广播变量，那么数据集将会存储在 TaskManager 的内存中，如果数据集和越大，那么占用的内存就会越大，因此使用广播变量适用于较小的数据集。</p><p>如果要向每个 TaskManager 发送更多数据并且不希望将这些数据存储在内存中，可以使用 Flink 的分布式缓存向 TaskManager 发送静态文件。 要使用 Flink 的分布式缓存，你首先需要将文件存储在一个分布式文件系统（如 HDFS）中，然后在缓存中注册该文件：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line"><span class="comment">//从 HDFS 注册文件</span></span><br><span class="line">env.registerCachedFile(<span class="string">"hdfs:///path/to/file"</span>, <span class="string">"machineLearningModel"</span>)</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">env.execute()</span><br></pre></td></tr></table></figure><p>为了访问分布式缓存，我们需要实现一个 Rich 函数：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyClassifier</span> <span class="keyword">extends</span> <span class="title">RichMapFunction</span>&lt;<span class="title">String</span>, <span class="title">Integer</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration config)</span> </span>&#123;</span><br><span class="line">      File machineLearningModel = getRuntimeContext().getDistributedCache().getFile(<span class="string">"machineLearningModel"</span>);</span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Integer <span class="title">map</span><span class="params">(String value)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">      ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>请注意，要访问分布式缓存中的文件，我们需要使用我们用于注册文件的 key，比如上面代码中的 <code>machineLearningModel</code>。</p><h3 id="Accumulator-累加器"><a href="#Accumulator-累加器" class="headerlink" title="Accumulator(累加器)"></a>Accumulator(累加器)</h3><p>我们前面已经介绍了如何将数据发送给 TaskManager，但现在我们将讨论如何从 TaskManager 中返回数据。 你可能想知道为什么我们需要做这种事情。 毕竟，Apache Flink 就是建立数据处理流水线，读取输入数据，处理数据并返回结果。</p><p>为了表达清楚，让我们来看一个例子。假设我们需要计算每个单词在文本中出现的次数，同时我们要计算文本中有多少行：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//要处理的数据集合</span></span><br><span class="line">DataSet&lt;String&gt; lines = ...</span><br><span class="line"></span><br><span class="line"><span class="comment">// Word count 算法</span></span><br><span class="line">lines.flatMap(<span class="keyword">new</span> FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] words = line.split(<span class="string">"\\W+"</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.groupBy(<span class="number">0</span>)</span><br><span class="line">.sum(<span class="number">1</span>)</span><br><span class="line">.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">// 计算要处理的文本中的行数</span></span><br><span class="line"><span class="keyword">int</span> linesCount = lines.count()</span><br><span class="line">System.out.println(linesCount);</span><br></pre></td></tr></table></figure><p>问题是如果我们运行这个应用程序，它将运行两个 Flink 作业！首先得到单词统计数，然后计算行数。</p><p>这绝对是低效的，但我们怎样才能避免这种情况呢？一种方法是使用累加器。它们允许你从 TaskManager 发送数据，并使用预定义的功能聚合此数据。 Flink 有以下内置累加器：</p><ul><li><p>IntCounter，LongCounter，DoubleCounter：允许将 TaskManager 发送的 int，long，double 值汇总在一起</p></li><li><p>AverageAccumulator：计算双精度值的平均值</p></li><li><p>LongMaximum，LongMinimum，IntMaximum，IntMinimum，DoubleMaximum，DoubleMinimum：累加器，用于确定不同类型的最大值和最小值</p></li><li><p>直方图 - 用于计算 TaskManager 的值分布</p></li></ul><p>要使用累加器，我们需要创建并注册一个用户定义的函数，然后在客户端上读取结果。下面我们来看看该如何使用呢：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">lines.flatMap(<span class="keyword">new</span> RichFlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt;() &#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//创建一个累加器</span></span><br><span class="line">    <span class="keyword">private</span> IntCounter linesNum = <span class="keyword">new</span> IntCounter();</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//注册一个累加器</span></span><br><span class="line">        getRuntimeContext().addAccumulator(<span class="string">"linesNum"</span>, linesNum);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">flatMap</span><span class="params">(String line, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        String[] words = line.split(<span class="string">"\\W+"</span>);</span><br><span class="line">        <span class="keyword">for</span> (String word : words) &#123;</span><br><span class="line">            out.collect(<span class="keyword">new</span> Tuple2&lt;&gt;(word, <span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 处理每一行数据后 linesNum 递增</span></span><br><span class="line">        linesNum.add(<span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;)</span><br><span class="line">.groupBy(<span class="number">0</span>)</span><br><span class="line">.sum(<span class="number">1</span>)</span><br><span class="line">.print();</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取累加器结果</span></span><br><span class="line"><span class="keyword">int</span> linesNum = env.getLastJobExecutionResult().getAccumulatorResult(<span class="string">"linesNum"</span>);</span><br><span class="line">System.out.println(linesNum);</span><br></pre></td></tr></table></figure><p>这样计算就可以统计输入文本中每个单词出现的次数以及它有多少行。</p><p>如果需要自定义累加器，还可以使用 Accumulator 或 SimpleAccumulator 接口实现自己的累加器。</p><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>本篇文章由 zhisheng 翻译，禁止任何无授权的转载。</p><p>翻译后地址：<a href="http://www.54tianzhisheng.cn/2019/03/28/flink-additional-data/">http://www.54tianzhisheng.cn/2019/03/28/flink-additional-data/</a></p><p>原文地址：<a href="https://brewing.codes/2017/10/24/flink-additional-data/">https://brewing.codes/2017/10/24/flink-additional-data/</a></p><p>本文部分代码地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/batch/accumulator">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-examples/src/main/java/com/zhisheng/examples/batch/accumulator</a></p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Ri9glD.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/hqYbHa.jpg" alt=""></p><h3 id="Flink-实战"><a href="#Flink-实战" class="headerlink" title="Flink 实战"></a>Flink 实战</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">Flink 从0到1学习 —— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">Flink 从0到1学习 —— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">Flink 从0到1学习 —— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">Flink 从0到1学习 —— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">Flink 从0到1学习 —— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">Flink 从0到1学习 —— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">Flink 从0到1学习 —— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">Flink 从0到1学习 —— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">Flink 从0到1学习 —— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">Flink 从0到1学习 —— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">Flink 从0到1学习 —— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">Flink 从0到1学习 —— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">Flink 从0到1学习 —— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">Flink 从0到1学习 —— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">Flink 从0到1学习 —— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">Flink 从0到1学习 —— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">Flink 从0到1学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">Flink 从0到1学习 —— 你上传的 jar 包藏到哪里去了?</a></p><p>19、<a href="http://www.54tianzhisheng.cn/2019/03/28/flink-additional-data/">Flink 从0到1学习 —— Flink 中如何管理配置？</a></p><h3 id="Flink-源码解析"><a href="#Flink-源码解析" class="headerlink" title="Flink 源码解析"></a>Flink 源码解析</h3><p>1、<a href="http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/">Flink 源码解析 —— 源码编译运行</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2019/03/14/Flink-code-structure/">Flink 源码解析 —— 项目结构一览</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2019/12/31/Flink-resources/">Flink 源码解析—— local 模式启动流程</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2019/03/15/Flink-code-Standalone-start/">Flink 源码解析 —— standalone session 模式启动流程</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2019/03/16/Flink-code-Standalone-JobManager-start/">Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2019/03/17/Flink-code-Standalone-TaskManager-start/">Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2019/03/18/Flink-code-batch-wordcount-start/">Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2019/03/19/Flink-code-streaming-wordcount-start/">Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2019/03/21/Flink-code-JobGraph/">Flink 源码解析 —— 如何获取 JobGraph？</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2019/03/20/Flink-code-StreamGraph/">Flink 源码解析 —— 如何获取 StreamGraph？</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2019/03/25/Flink-code-jobmanager/">Flink 源码解析 —— Flink JobManager 有什么作用？</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/03/25/Flink-code-taskmanager/">Flink 源码解析 —— Flink TaskManager 有什么作用？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/12/31/Flink-resources/">Flink 源码解析 —— JobManager 处理 SubmitJob 的过程</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/12/31/Flink-resources/">Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/03/23/Flink-code-checkpoint/">Flink 源码解析 —— 深度解析 Flink Checkpoint 机制</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/03/22/Flink-code-serialize/">Flink 源码解析 —— 深度解析 Flink 序列化机制</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/">Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/26/Flink-code-ExecutionGraph/">Flink 源码解析 —— 如何获取 ExecutionGraph ？</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-06-10-113303.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 如何获取 ExecutionGraph ？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/26/Flink-code-ExecutionGraph/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/26/Flink-code-ExecutionGraph/</id>
    <published>2019-03-25T16:00:00.000Z</published>
    <updated>2019-05-30T15:07:08.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/3NN3bu3">https://t.zsxq.com/3NN3bu3</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/3NN3bu3&quot;&gt;https://t.zsxq.com/3NN3bu3&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— Flink JobManager 有什么作用？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/25/Flink-code-jobmanager/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/25/Flink-code-jobmanager/</id>
    <published>2019-03-24T16:00:00.000Z</published>
    <updated>2019-05-25T06:30:44.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/2VRrbuf">https://t.zsxq.com/2VRrbuf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/2VRrbuf&quot;&gt;https://t.zsxq.com/2VRrbuf&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— Flink TaskManager 有什么作用？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/25/Flink-code-taskmanager/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/25/Flink-code-taskmanager/</id>
    <published>2019-03-24T16:00:00.000Z</published>
    <updated>2019-05-25T06:31:02.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/RZbu7yN">https://t.zsxq.com/RZbu7yN</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/RZbu7yN&quot;&gt;https://t.zsxq.com/RZbu7yN&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/</id>
    <published>2019-03-23T16:00:00.000Z</published>
    <updated>2019-05-30T14:59:24.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-30-145307.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>如今，许多用于分析大型数据集的开源系统都是用 Java 或者是基于 JVM 的编程语言实现的。最着名的例子是 Apache Hadoop，还有较新的框架，如 Apache Spark、Apache Drill、Apache Flink。基于 JVM 的数据分析引擎面临的一个常见挑战就是如何在内存中存储大量的数据（包括缓存和高效处理）。合理的管理好 JVM 内存可以将 难以配置且不可预测的系统 与 少量配置且稳定运行的系统区分开来。</p><p>在这篇文章中，我们将讨论 Apache Flink 如何管理内存，讨论其自定义序列化与反序列化机制，以及它是如何操作二进制数据的。</p><h3 id="数据对象直接放在堆内存中"><a href="#数据对象直接放在堆内存中" class="headerlink" title="数据对象直接放在堆内存中"></a>数据对象直接放在堆内存中</h3><p>在 JVM 中处理大量数据最直接的方式就是将这些数据做为对象存储在堆内存中，然后直接在内存中操作这些数据，如果想进行排序则就是对对象列表进行排序。然而这种方法有一些明显的缺点，首先，在频繁的创建和销毁大量对象的时候，监视和控制堆内存的使用并不是一件很简单的事情。如果对象分配过多的话，那么会导致内存过度使用，从而触发 OutOfMemoryError，导致 JVM 进程直接被杀死。另一个方面就是因为这些对象大都是生存在新生代，当 JVM 进行垃圾回收时，垃圾收集的开销很容易达到 50% 甚至更多。最后就是 Java 对象具有一定的空间开销（具体取决于 JVM 和平台）。对于具有许多小对象的数据集，这可以显著减少有效可用的内存量。如果你精通系统设计和系统调优，你可以根据系统进行特定的参数调整，可以或多或少的控制出现 OutOfMemoryError 的次数和避免堆内存的过多使用，但是这种设置和调优的作用有限，尤其是在数据量较大和执行环境发生变化的情况下。</p><h3 id="Flink-是怎么做的"><a href="#Flink-是怎么做的" class="headerlink" title="Flink 是怎么做的?"></a>Flink 是怎么做的?</h3><p>Apache Flink 起源于一个研究项目，该项目旨在结合基于 MapReduce 的系统和并行数据库系统的最佳技术。在此背景下，Flink 一直有自己的内存数据处理方法。Flink 将对象序列化为固定数量的预先分配的内存段，而不是直接把对象放在堆内存上。它的 DBMS 风格的排序和连接算法尽可能多地对这个二进制数据进行操作，以此将序列化和反序列化开销降到最低。如果需要处理的数据多于可以保存在内存中的数据，Flink 的运算符会将部分数据溢出到磁盘。事实上，很多Flink 的内部实现看起来更像是 C / C ++，而不是普通的 Java。下图概述了 Flink 如何在内存段中存储序列化数据并在必要时溢出到磁盘：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-051601.jpg" alt=""></p><p>Flink 的主动内存管理和操作二进制数据有几个好处：</p><p>1、<strong>内存安全执行和高效的核外算法</strong> 由于分配的内存段的数量是固定的，因此监控剩余的内存资源是非常简单的。在内存不足的情况下，处理操作符可以有效地将更大批的内存段写入磁盘，后面再将它们读回到内存。因此，OutOfMemoryError 就有效的防止了。</p><p>2、<strong>减少垃圾收集压力</strong> 因为所有长生命周期的数据都是在 Flink 的管理内存中以二进制表示的，所以所有数据对象都是短暂的，甚至是可变的，并且可以重用。短生命周期的对象可以更有效地进行垃圾收集，这大大降低了垃圾收集的压力。现在，预先分配的内存段是 JVM 堆上的长期存在的对象，为了降低垃圾收集的压力，Flink 社区正在积极地将其分配到堆外内存。这种努力将使得 JVM 堆变得更小，垃圾收集所消耗的时间将更少。</p><p>3、<strong>节省空间的数据存储</strong> Java 对象具有存储开销，如果数据以二进制的形式存储，则可以避免这种开销。</p><p>4、<strong>高效的二进制操作和缓存敏感性</strong> 在给定合适的二进制表示的情况下，可以有效地比较和操作二进制数据。此外，二进制表示可以将相关值、哈希码、键和指针等相邻地存储在内存中。这使得数据结构通常具有更高效的缓存访问模式。</p><p>主动内存管理的这些特性在用于大规模数据分析的数据处理系统中是非常可取的，但是要实现这些功能的代价也是高昂的。要实现对二进制数据的自动内存管理和操作并非易事，使用 <code>java.util.HashMap</code> 比实现一个可溢出的 <code>hash-table</code> （由字节数组和自定义序列化支持）。当然，Apache Flink 并不是唯一一个基于 JVM 且对二进制数据进行操作的数据处理系统。例如 Apache Drill、Apache Ignite、Apache Geode 也有应用类似技术，最近 Apache Spark 也宣布将向这个方向演进。</p><p>下面我们将详细讨论 Flink 如何分配内存、如果对对象进行序列化和反序列化以及如果对二进制数据进行操作。我们还将通过一些性能表现数据来比较处理堆内存上的对象和对二进制数据的操作。</p><h3 id="Flink-如何分配内存"><a href="#Flink-如何分配内存" class="headerlink" title="Flink 如何分配内存?"></a>Flink 如何分配内存?</h3><p>Flink TaskManager 是由几个内部组件组成的：actor 系统（负责与 Flink master 协调）、IOManager（负责将数据溢出到磁盘并将其读取回来）、MemoryManager（负责协调内存使用）。在本篇文章中，我们主要讲解 MemoryManager。</p><p>MemoryManager 负责将 MemorySegments 分配、计算和分发给数据处理操作符，例如 sort 和 join 等操作符。MemorySegment 是 Flink 的内存分配单元，由常规 Java 字节数组支持(默认大小为 32 KB)。MemorySegment 通过使用 Java 的 unsafe 方法对其支持的字节数组提供非常有效的读写访问。你可以将 MemorySegment 看作是 Java 的 NIO ByteBuffer 的定制版本。为了在更大的连续内存块上操作多个 MemorySegment，Flink 使用了实现 Java 的 java.io.DataOutput 和 java.io.DataInput 接口的逻辑视图。</p><p>MemorySegments 在 TaskManager 启动时分配一次，并在 TaskManager 关闭时销毁。因此，在 TaskManager 的整个生命周期中，MemorySegment 是重用的，而不会被垃圾收集的。在初始化 TaskManager 的所有内部数据结构并且已启动所有核心服务之后，MemoryManager 开始创建 MemorySegments。默认情况下，服务初始化后，70％ 可用的 JVM 堆内存由 MemoryManager 分配（也可以配置全部）。剩余的 JVM 堆内存用于在任务处理期间实例化的对象，包括由用户定义的函数创建的对象。下图显示了启动后 TaskManager JVM 中的内存分布：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-051655.jpg" alt=""></p><h3 id="Flink-如何序列化对象？"><a href="#Flink-如何序列化对象？" class="headerlink" title="Flink 如何序列化对象？"></a>Flink 如何序列化对象？</h3><p>Java 生态系统提供了几个库，可以将对象转换为二进制表示形式并返回。常见的替代方案是标准 Java 序列化，Kryo，Apache Avro，Apache Thrift 或 Google 的 Protobuf。Flink 包含自己的自定义序列化框架，以便控制数据的二进制表示。这一点很重要，因为对二进制数据进行操作需要对序列化布局有准确的了解。此外，根据在二进制数据上执行的操作配置序列化布局可以显著提升性能。Flink 的序列化机制利用了这一特性，即在执行程序之前，要序列化和反序列化的对象的类型是完全已知的。</p><p>Flink 程序可以处理表示为任意 Java 或 Scala 对象的数据。在优化程序之前，需要识别程序数据流的每个处理步骤中的数据类型。对于 Java 程序，Flink 提供了一个基于反射的类型提取组件，用于分析用户定义函数的返回类型。Scala 程序可以在 Scala 编译器的帮助下进行分析。Flink 使用 TypeInformation 表示每种数据类型。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-23-163200.jpg" alt="Flink 类型分类"></p><blockquote><p>注：该图选自董伟柯的文章《Apache Flink 类型和序列化机制简介》，侵删</p></blockquote><p>Flink 有如下几种数据类型的 TypeInformations：</p><ul><li><p>BasicTypeInfo：所有 Java 的基础类型或 java.lang.String</p></li><li><p>BasicArrayTypeInfo：Java 基本类型构成的数组或 java.lang.String</p></li><li><p>WritableTypeInfo：Hadoop 的 Writable 接口的任何实现</p></li><li><p>TupleTypeInfo：任何 Flink tuple（Tuple1 到 Tuple25）。Flink tuples 是具有类型化字段的固定长度元组的 Java 表示</p></li><li><p>CaseClassTypeInfo：任何 Scala CaseClass（包括 Scala tuples）</p></li><li><p>PojoTypeInfo：任何 POJO（Java 或 Scala），即所有字段都是 public 的或通过 getter 和 setter 访问的对象，遵循通用命名约定</p></li><li><p>GenericTypeInfo：不能标识为其他类型的任何数据类型</p></li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-23-163454.jpg" alt="TypeInformation 类继承关系图"></p><blockquote><p>注：该图选自董伟柯的文章《Apache Flink 类型和序列化机制简介》，侵删</p></blockquote><p>每个 TypeInformation 都为它所代表的数据类型提供了一个序列化器。例如，BasicTypeInfo 返回一个序列化器，该序列化器写入相应的基本类型；WritableTypeInfo 的序列化器将序列化和反序列化委托给实现 Hadoop 的 Writable 接口的对象的 write() 和 readFields() 方法；GenericTypeInfo 返回一个序列化器，该序列化器将序列化委托给 Kryo。对象将自动通过 Java 中高效的 Unsafe 方法来序列化到 Flink MemorySegments 支持的 DataOutput。对于可用作键的数据类型，例如哈希值，TypeInformation 提供了 TypeComparators，TypeComparators 比较和哈希对象，并且可以根据具体的数据类型有效的比较二进制并提取固定长度的二进制 key 前缀。</p><p>Tuple，Pojo 和 CaseClass 类型是复合类型，它们可能嵌套一个或者多个数据类型。因此，它们的序列化和比较也都比较复杂，一般将其成员数据类型的序列化和比较都交给各自的 Serializers（序列化器） 和 Comparators（比较器）。下图说明了 <code>Tuple3&lt;Integer, Double, Person&gt;</code>对象的序列化，其中<code>Person</code> 是 POJO 并定义如下： </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> id;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-051739.jpg" alt=""></p><p>通过提供定制的 TypeInformations、Serializers（序列化器） 和 Comparators（比较器），可以方便地扩展 Flink 的类型系统，从而提高序列化和比较自定义数据类型的性能。</p><h3 id="Flink-如何对二进制数据进行操作？"><a href="#Flink-如何对二进制数据进行操作？" class="headerlink" title="Flink 如何对二进制数据进行操作？"></a>Flink 如何对二进制数据进行操作？</h3><p>与其他的数据处理框架的 API（包括 SQL）类似，Flink 的 API 也提供了对数据集进行分组、排序和连接等转换操作。这些转换操作的数据集可能非常大。关系数据库系统具有非常高效的算法，比如 merge-sort、merge-join 和 hash-join。Flink 建立在这种技术的基础上，但是主要分为使用自定义序列化和自定义比较器来处理任意对象。在下面文章中我们将通过 Flink 的内存排序算法示例演示 Flink 如何使用二进制数据进行操作。</p><p>Flink 为其数据处理操作符预先分配内存，初始化时，排序算法从 MemoryManager 请求内存预算，并接收一组相应的 MemorySegments。这些 MemorySegments 变成了缓冲区的内存池，缓冲区中收集要排序的数据。下图说明了如何将数据对象序列化到排序缓冲区中：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-051848.jpg" alt=""></p><p>排序缓冲区在内部分为两个内存区域：第一个区域保存所有对象的完整二进制数据，第二个区域包含指向完整二进制对象数据的指针（取决于 key 的数据类型）。将对象添加到排序缓冲区时，它的二进制数据会追加到第一个区域，指针(可能还有一个 key)被追加到第二个区域。分离实际数据和指针以及固定长度的 key 有两个目的：它可以有效的交换固定长度的 entries（key 和指针），还可以减少排序时需要移动的数据。如果排序的 key 是可变长度的数据类型（比如 String），则固定长度的排序 key 必须是前缀 key，比如字符串的前 n 个字符。请注意：并非所有数据类型都提供固定长度的前缀排序 key。将对象序列化到排序缓冲区时，两个内存区域都使用内存池中的 MemorySegments 进行扩展。一旦内存池为空且不能再添加对象时，则排序缓冲区将会被完全填充并可以进行排序。Flink 的排序缓冲区提供了比较和交换元素的方法，这使得实际的排序算法是可插拔的。默认情况下， Flink 使用了 Quicksort（快速排序）实现，可以使用 HeapSort（堆排序）。下图显示了如何比较两个对象：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-051920.jpg" alt=""></p><p>排序缓冲区通过比较它们的二进制固定长度排序 key 来比较两个元素。如果元素的完整 key（不是前缀 key） 或者二进制前缀 key 不相等，则代表比较成功。如果前缀 key 相等(或者排序 key 的数据类型不提供二进制前缀 key)，则排序缓冲区遵循指向实际对象数据的指针，对两个对象进行反序列化并比较对象。根据比较结果，排序算法决定是否交换比较的元素。排序缓冲区通过移动其固定长度 key 和指针来交换两个元素，实际数据不会移动，排序算法完成后，排序缓冲区中的指针被正确排序。下图演示了如何从排序缓冲区返回已排序的数据：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-051949.jpg" alt=""></p><p>通过顺序读取排序缓冲区的指针区域，跳过排序 key 并按照实际数据的排序指针返回排序数据。此数据要么反序列化并作为对象返回，要么在外部合并排序的情况下复制二进制数据并将其写入磁盘。</p><h3 id="基准测试数据"><a href="#基准测试数据" class="headerlink" title="基准测试数据"></a>基准测试数据</h3><p>那么，对二进制数据进行操作对性能意味着什么？我们将运行一个基准测试，对 1000 万个<code>Tuple2&lt;Integer, String&gt;</code>对象进行排序以找出答案。整数字段的值从均匀分布中采样。String 字段值的长度为 12 个字符，并从长尾分布中进行采样。输入数据由返回可变对象的迭代器提供，即返回具有不同字段值的相同 Tuple 对象实例。Flink 在从内存，网络或磁盘读取数据时使用此技术，以避免不必要的对象实例化。基准测试在具有 900 MB 堆大小的 JVM 中运行，在堆上存储和排序 1000 万个 Tuple 对象并且不会导致触发 OutOfMemoryError 大约需要这么大的内存。我们使用三种排序方法在Integer 字段和 String 字段上对 Tuple 对象进行排序：</p><p>1、<strong>对象存在堆中</strong>：Tuple 对象存储在常用的 <code>java.util.ArrayList</code> 中，初始容量设置为 1000 万，并使用 Java 中常用的集合排序进行排序。</p><ol><li><strong>Flink 序列化</strong>：使用 Flink 的自定义序列化程序将 Tuple 字段序列化为 600 MB 大小的排序缓冲区，如上所述排序，最后再次反序列化。在 Integer 字段上进行排序时，完整的 Integer 用作排序 key，以便排序完全发生在二进制数据上（不需要对象的反序列化）。对于 String 字段的排序，使用 8 字节前缀 key，如果前缀 key 相等，则对 Tuple 对象进行反序列化。</li></ol><p>3、<strong>Kryo 序列化</strong>：使用 Kryo 序列化将 Tuple 字段序列化为 600 MB 大小的排序缓冲区，并在没有二进制排序 key 的情况下进行排序。这意味着每次比较需要对两个对象进行反序列化。</p><p>所有排序方法都使用单线程实现。结果的时间是十次运行结果的平均值。在每次运行之后，我们调用<code>System.gc()</code>请求垃圾收集运行，该运行不会进入测量的执行时间。下图显示了将输入数据存储在内存中，对其进行排序并将其作为对象读回的时间。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-040201.jpg" alt=""></p><p>我们看到 Flink 使用自己的序列化器对二进制数据进行排序明显优于其他两种方法。与存储在堆内存上相比，我们看到将数据加载到内存中要快得多。因为我们实际上是在收集对象，没有机会重用对象实例，但必须重新创建每个 Tuple。这比 Flink 的序列化器（或Kryo序列化）效率低。另一方面，与反序列化相比，从堆中读取对象是无性能消耗的。在我们的基准测试中，对象克隆比序列化和反序列化组合更耗性能。查看排序时间，我们看到对二进制数据的排序也比 Java 的集合排序更快。使用没有二进制排序 key 的 Kryo 序列化的数据排序比其他方法慢得多。这是因为反序列化带来很大的开销。在String 字段上对 Tuple 进行排序比在 Integer 字段上排序更快，因为长尾值分布显着减少了成对比较的数量。为了更好地了解排序过程中发生的状况，我们使用 VisualVM 监控执行的 JVM。以下截图显示了执行 10次 运行时的堆内存使用情况、垃圾收集情况和 CPU 使用情况。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-044335.jpg" alt=""></p><p>测试是在 8 核机器上运行单线程，因此一个核心的完全利用仅对应 12.5％ 的总体利用率。截图显示，对二进制数据进行操作可显著减少垃圾回收活动。对于对象存在堆中，垃圾收集器在排序缓冲区被填满时以非常短的时间间隔运行，并且即使对于单个处理线程也会导致大量 CPU 使用（排序本身不会触发垃圾收集器）。JVM 垃圾收集多个并行线程，解释了高CPU 总体利用率。另一方面，对序列化数据进行操作的方法很少触发垃圾收集器并且 CPU 利用率低得多。实际上，如果使用 Flink 序列化的方式在 Integer 字段上对 Tuple 进行排序，则垃圾收集器根本不运行，因为对于成对比较，不需要反序列化任何对象。Kryo 序列化需要比较多的垃圾收集，因为它不使用二进制排序 key 并且每次排序都要反序列化两个对象。</p><p>内存使用情况上图显示 Flink 序列化和 Kryo 序列化不断的占用大量内存</p><p>存使用情况图表显示flink-serialized和kryo-serialized不断占用大量内存。这是由于 MemorySegments 的预分配。实际内存使用率要低得多，因为排序缓冲区并未完全填充。下表显示了每种方法的内存消耗。1000 万条数据产生大约 280 MB 的二进制数据（对象数据、指针和排序 key），具体取决于使用的序列化程序以及二进制排序 key 的存在和大小。将其与数据存储在堆上的方法进行比较，我们发现对二进制数据进行操作可以显著提高内存效率。在我们的基准测试中，如果序列化为排序缓冲区而不是将其作为堆上的对象保存，则可以在内存中对两倍以上的数据进行排序。</p><table><thead><tr><th>占用内存</th><th>对象存在堆中</th><th>Flink 序列化</th><th>Kryo 序列化</th></tr></thead><tbody><tr><td>对 Integer 排序</td><td>约 700 MB（堆内存）</td><td>277 MB（排序缓冲区）</td><td>266 MB（排序缓冲区）</td></tr><tr><td>对 String 排序</td><td>约 700 MB（堆内存）</td><td>315 MB（排序缓冲区）</td><td>266 MB（排序缓冲区）</td></tr></tbody></table><p>总而言之，测试验证了文章前面说的对二进制数据进行操作的好处。</p><h3 id="展望未来"><a href="#展望未来" class="headerlink" title="展望未来"></a>展望未来</h3><p>Apache Flink 具有相当多的高级技术，可以通过有限的内存资源安全有效地处理大量数据。但是有几点可以使 Flink 更有效率。Flink 社区正在努力将管理内存移动到堆外内存。这将允许更小的 JVM，更低的垃圾收集开销，以及更容易的系统配置。使用 Flink 的 Table API，所有操作（如 aggregation 和 projection）的语义都是已知的（与黑盒用户定义的函数相反）。因此，我们可以为直接对二进制数据进行操作的 Table API 操作生成代码。进一步的改进包括序列化设计，这些设计针对应用于二进制数据的操作和针对序列化器和比较器的代码生成而定制。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li><p>Flink 的主动内存管理减少了因触发 OutOfMemoryErrors 而杀死 JVM 进程和垃圾收集开销的问题。</p></li><li><p>Flink 具有高效的数据序列化和反序列化机制，有助于对二进制数据进行操作，并使更多数据适合内存。</p></li><li><p>Flink 的 DBMS 风格的运算符本身在二进制数据上运行，在必要时可以在内存中高性能地传输到磁盘。</p></li></ul><p>本文地址: <a href="http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/">http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/</a></p><blockquote><p>本文翻译自：<a href="https://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html">https://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html</a><br>翻译：zhisheng，二次转载请注明地址，否则保留追究法律责任。</p></blockquote><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6Elrml.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/uPBJ5b.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-30-145307.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 深度解析 Flink Checkpoint 机制</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/23/Flink-code-checkpoint/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/23/Flink-code-checkpoint/</id>
    <published>2019-03-22T16:00:00.000Z</published>
    <updated>2019-05-22T12:31:47.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/ynQNbeM">https://t.zsxq.com/ynQNbeM</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/ynQNbeM&quot;&gt;https://t.zsxq.com/ynQNbeM&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 深度解析 Flink 序列化机制</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/22/Flink-code-serialize/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/22/Flink-code-serialize/</id>
    <published>2019-03-21T16:00:00.000Z</published>
    <updated>2019-05-22T12:30:48.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/JaQfeMf">https://t.zsxq.com/JaQfeMf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/JaQfeMf&quot;&gt;https://t.zsxq.com/JaQfeMf&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 如何获取 JobGraph？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/21/Flink-code-JobGraph/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/21/Flink-code-JobGraph/</id>
    <published>2019-03-20T16:00:00.000Z</published>
    <updated>2019-04-21T14:18:06.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/naaMf6y">https://t.zsxq.com/naaMf6y</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/naaMf6y&quot;&gt;https://t.zsxq.com/naaMf6y&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 如何获取 StreamGraph？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/20/Flink-code-StreamGraph/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/20/Flink-code-StreamGraph/</id>
    <published>2019-03-19T16:00:00.000Z</published>
    <updated>2019-04-16T15:04:06.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/qRFIm6I">https://t.zsxq.com/qRFIm6I</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/qRFIm6I&quot;&gt;https://t.zsxq.com/qRFIm6I&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/19/Flink-code-streaming-wordcount-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/19/Flink-code-streaming-wordcount-start/</id>
    <published>2019-03-18T16:00:00.000Z</published>
    <updated>2019-04-16T15:02:46.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/qnMFEUJ">https://t.zsxq.com/qnMFEUJ</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/qnMFEUJ&quot;&gt;https://t.zsxq.com/qnMFEUJ&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/18/Flink-code-batch-wordcount-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/18/Flink-code-batch-wordcount-start/</id>
    <published>2019-03-17T16:00:00.000Z</published>
    <updated>2019-04-16T15:00:18.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/YJ2Zrfi">https://t.zsxq.com/YJ2Zrfi</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/YJ2Zrfi&quot;&gt;https://t.zsxq.com/YJ2Zrfi&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/17/Flink-code-Standalone-TaskManager-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/17/Flink-code-Standalone-TaskManager-start/</id>
    <published>2019-03-16T16:00:00.000Z</published>
    <updated>2019-04-16T14:59:00.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/qjEUFau">https://t.zsxq.com/qjEUFau</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/qjEUFau&quot;&gt;https://t.zsxq.com/qjEUFau&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/16/Flink-code-Standalone-JobManager-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/16/Flink-code-Standalone-JobManager-start/</id>
    <published>2019-03-15T16:00:00.000Z</published>
    <updated>2019-04-16T14:57:48.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/AurR3rN">https://t.zsxq.com/AurR3rN</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/AurR3rN&quot;&gt;https://t.zsxq.com/AurR3rN&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— standalonesession 模式启动流程</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/15/Flink-code-Standalone-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/15/Flink-code-Standalone-start/</id>
    <published>2019-03-14T16:00:00.000Z</published>
    <updated>2019-04-16T14:56:44.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/EemAEIi">https://t.zsxq.com/EemAEIi</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/EemAEIi&quot;&gt;https://t.zsxq.com/EemAEIi&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 项目结构一览</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/14/Flink-code-structure/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/14/Flink-code-structure/</id>
    <published>2019-03-13T16:00:00.000Z</published>
    <updated>2019-04-16T14:46:50.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/MNfAYne">https://t.zsxq.com/MNfAYne</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/MNfAYne&quot;&gt;https://t.zsxq.com/MNfAYne&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 从 0 到 1 学习 —— 你上传的 jar 包藏到哪里去了?</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/</id>
    <published>2019-03-12T16:00:00.000Z</published>
    <updated>2019-06-03T08:55:06.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ieWce6.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>写这篇文章其实也是知识星球里面的一个小伙伴问了这样一个问题：</p><blockquote><p>通过 flink UI 仪表盘提交的 jar 是存储在哪个目录下？</p></blockquote><p>这个问题其实我自己也有问过，但是自己因为自己的问题没有啥压力也就没深入去思考，现在可是知识星球的付费小伙伴问的，所以自然要逼着自己去深入然后才能给出正确的答案。</p><p>跟着我一起来看看我的探寻步骤吧！小小的 jar 竟然还敢和我捉迷藏？</p><h3 id="查看配置文件"><a href="#查看配置文件" class="headerlink" title="查看配置文件"></a>查看配置文件</h3><p>首先想到的是这个肯定可以在配置文件中有设置的地方的：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/dAbvto.jpg" alt=""></p><h3 id="谷歌大法好"><a href="#谷歌大法好" class="headerlink" title="谷歌大法好"></a>谷歌大法好</h3><p>虽然有个是 upload 的，但是并不是我们想要的目录！于是，只好动用我的“谷歌大法好”。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/PNdNmo.jpg" alt=""></p><p>找到了一条，点进去看 Issue 如下：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/N4vkGA.jpg" alt=""></p><p>发现这 tm 不就是想要的吗？都支持配置文件来填写上传的 jar 后存储的目录了！赶紧点进去看一波源码：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6uzvWx.jpg" alt=""></p><h3 id="源码确认"><a href="#源码确认" class="headerlink" title="源码确认"></a>源码确认</h3><p>这个 <code>jobmanager.web.upload.dir</code> 是不是？我去看下 1.8 的源码确认一下：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/APCWVr.jpg" alt=""></p><p>发现这个 <code>jobmanager.web.upload.dir</code> 还过期了，用 <code>WebOptions</code> 类中的 <code>UPLOAD_DIR</code> 替代了！</p><p>继续跟进去看看这个 <code>UPLOAD_DIR</code> 是啥玩意？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/i39RJI.jpg" alt=""></p><p>看这注释的意思是说，如果这个配置 <code>web.upload.dir</code> 没有配置具体的路径的话就会使用 <code>JOB_MANAGER_WEB_TMPDIR_KEY</code> 目录，那么我们来看看是否配置了这个目录呢？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/2rEtf8.jpg" alt=""></p><p>确实没有配置这个 jar 文件上传的目录，那么我们来看看这个临时目录 <code>JOB_MANAGER_WEB_TMPDIR_KEY</code> 是在哪里的？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/AhiogN.jpg" alt=""></p><p>又是一个过期的目录，mmp，继续跟下去看下这个目录 <code>TMP_DIR</code>。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/lxOQ3N.jpg" alt=""></p><p>我们查看下配置文件是否有配置这个 <code>web.tmpdir</code> 的值，又是没有：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/W6Y4h6.jpg" alt=""></p><p>so，它肯定使用的是 <code>System.getProperty(&quot;java.io.tmpdir&quot;)</code> 这个目录了，</p><p>我查看了下我本地电脑起的 job 它的配置中有这个配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.io.tmpdir/var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Krx3dH.jpg" alt=""></p><p>再观察了下 job，发现 jobManager 这里有个 <code>web.tmpdir</code> 的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.tmpdir/var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/flink-web-ea909e9e-4bac-452d-8450-b4ff082298c7</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/oGwx4D.jpg" alt=""></p><p>发现这个 <code>web.tmpdir</code> 的就是由 java.io.tmpdir + “flink-web-” + UUID 组成的！</p><h3 id="水落石出"><a href="#水落石出" class="headerlink" title="水落石出"></a>水落石出</h3><p>进入这个目录发现我们上传的 jar 终于被找到了：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/kvmMCC.jpg" alt=""></p><h3 id="配置上传-jar-目录确认"><a href="#配置上传-jar-目录确认" class="headerlink" title="配置上传 jar 目录确认"></a>配置上传 jar 目录确认</h3><p>上面我们虽然已经知道我们上传的 jar 是存储在这个临时目录里，那么我们现在要验证一下，我们在配置文件中配置一下上传 jar 的固定位置，我们先在目录下创建一个 jars 目录，然后在配置文件中加入这个配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.tmpdir: /usr/local/blink-1.5.1/jars</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/aEeHsZ.jpg" alt=""></p><p>更改之后再看 <code>web.tmpdir</code> 是这样的:</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/GnQfIQ.jpg" alt=""></p><p>从 Flink UI 上上传了三个 jar，查看 <code>/usr/local/blink-1.5.1/jars/flink-web-7a98165b-1d56-44be-be8c-d0cd9166b179</code> 目录下就出现了我们的 jar 了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/sN50JN.jpg" alt=""></p><p>我们重启 Flink，发现这三个 jar 又没有了，这也能解释之前我自己也遇到过的问题了，Flink 重启后之前所有上传的 jar 都被删除了！作为生产环境，这样玩，肯定不行的，所以我们还是得固定一个目录来存储所有的上传 jar 包，并且不能够被删除，要配置固定的目录（Flink 重启也不删除的话）需要配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.upload.dir: /usr/local/blink-1.5.1/jars</span><br></pre></td></tr></table></figure><p>这样的话，就可以保证你的 jar 不再会被删除了！</p><p>再来看看源码是咋写的哈：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//从配置文件中找 UPLOAD_DIR</span></span><br><span class="line"><span class="keyword">final</span> Path uploadDir = Paths.get(</span><br><span class="line">config.getString(WebOptions.UPLOAD_DIR,config.getString(WebOptions.TMP_DIR)),</span><br><span class="line"><span class="string">"flink-web-upload"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> RestServerEndpointConfiguration(</span><br><span class="line">restAddress,restBindAddress,port,sslEngineFactory,</span><br><span class="line">uploadDir,maxContentLength,responseHeaders);</span><br></pre></td></tr></table></figure><p>他就是从配置文件中找 <code>UPLOAD_DIR</code>，如果为 <code>null</code> 就找 <code>TMP_DIR</code> 目录来当作 jar 上传的路径！</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文从知识星球一个朋友的问题，从现象到本质再到解决方案的讲解了下如何找到 Flink UI 上上传的 jar 包藏身之处，并提出了如何解决 Flink 上传的 jar 包被删除的问题。</p><p>本篇文章连接是：<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/</a></p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6Elrml.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/uPBJ5b.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p><p>目前知识星球内已更新的系列文章：</p><p>1、<a href="https://t.zsxq.com/UZfaYfE">Flink 源码解析 —— 源码编译运行</a></p><p>2、<a href="https://t.zsxq.com/zZZjaYf">Flink 源码解析 —— 项目结构一览</a></p><p>3、<a href="https://t.zsxq.com/zV7MnuJ">Flink 源码解析—— local 模式启动流程</a></p><p>4、<a href="https://t.zsxq.com/QZVRZJA">Flink 源码解析 —— standalonesession 模式启动流程</a></p><p>5、<a href="https://t.zsxq.com/u3fayvf">Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动</a></p><p>6、<a href="https://t.zsxq.com/MnQRByb">Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动</a></p><p>7、<a href="https://t.zsxq.com/YJ2Zrfi">Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程</a></p><p>8、<a href="https://t.zsxq.com/qnMFEUJ">Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程</a></p><p>9、<a href="https://t.zsxq.com/naaMf6y">Flink 源码解析 —— 如何获取 JobGraph？</a></p><p>10、<a href="https://t.zsxq.com/qRFIm6I">Flink 源码解析 —— 如何获取 StreamGraph？</a></p><p>11、<a href="https://t.zsxq.com/2VRrbuf">Flink 源码解析 —— Flink JobManager 有什么作用？</a></p><p>12、<a href="https://t.zsxq.com/RZbu7yN">Flink 源码解析 —— Flink TaskManager 有什么作用？</a></p><p>13、<a href="https://t.zsxq.com/zV7MnuJ">Flink 源码解析 —— JobManager 处理 SubmitJob 的过程</a></p><p>14、<a href="https://t.zsxq.com/zV7MnuJ">Flink 源码解析 —— TaskManager 处理 SubmitJob 的过程</a></p><p>15、<a href="https://t.zsxq.com/ynQNbeM">Flink 源码解析 —— 深度解析 Flink Checkpoint 机制</a></p><p>16、<a href="https://t.zsxq.com/JaQfeMf">Flink 源码解析 —— 深度解析 Flink 序列化机制</a></p><p>17、<a href="https://t.zsxq.com/zjQvjeM">Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ieWce6.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>阿里巴巴开源的 Blink 实时计算框架真香</title>
    <link href="http://www.54tianzhisheng.cn/2019/02/28/blink/"/>
    <id>http://www.54tianzhisheng.cn/2019/02/28/blink/</id>
    <published>2019-02-27T16:00:00.000Z</published>
    <updated>2019-04-25T13:33:57.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-25-133346.jpg" alt=""></p><a id="more"></a><p>Blink 开源了有一段时间了，竟然没发现有人写相关的博客，其实我已经在我的知识星球里开始写了，今天来看看 Blink 为什么香？</p><p>我们先看看 Blink 黑色版本：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/DziJHz.jpg" alt=""></p><p>对比下 Flink 版本你就知道黑色版本多好看了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/sr3sje.jpg" alt=""></p><p>你上传 jar 包的时候是这样的：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/d1MBe7.jpg" alt=""></p><p>我们来看看 Blink 运行的 job 长啥样？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/U07enG.jpg" alt=""></p><p>再来对比一下 Flink 的样子：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/4triEY.jpg" alt=""></p><p>查看 Job Task 的详情，可以看到开始时间、接收记录、并行度、duration、Queue in/out、TPS</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/t644Ll.jpg" alt=""></p><p>查看 subTask，这里可以直接点击这个日志就可以查看 task 日志：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/N5r6pZ.jpg" alt=""></p><p>查看背压：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/cGA6bW.jpg" alt=""></p><p>查看 task metric，可以手动添加，支持的有很多，这点很重要，可以根据每个算子的监控以及时对每个算子进行调优：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Js2sxO.jpg" alt=""></p><p>查看 job 运行时间段的情况：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/pVkBXg.jpg" alt=""></p><p>查看 running 的 job：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/KKrNcP.jpg" alt=""></p><p>查看已经完成的 job：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/oSPEqu.jpg" alt=""></p><p>查看 Task Manager：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/33c6LH.jpg" alt=""></p><p>Task Manager 分配的资源详情：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/zwPTdI.jpg" alt=""></p><p>Task Manager metric 监控信息详情：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/voXrWm.jpg" alt=""></p><p>Task Manager log 文件详情，包含运行产生的日志和 GC 日志：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/nHgGci.jpg" alt=""></p><p>Task Manager 日志详情，支持高亮和分页，特别友好，妈妈再也不担心我看不见 “刷刷刷” 的日志了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/xMFiQq.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>介绍了 Flink 的 Blink 分支编译后运行的界面情况，总体来说很棒，期待后面 Blink 合并到 Flink！</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/02/28/blink/">http://www.54tianzhisheng.cn/2019/02/28/blink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/LM86BR.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/KRGtVb.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-25-133346.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Blink" scheme="http://www.54tianzhisheng.cn/tags/Blink/"/>
    
  </entry>
  
</feed>
