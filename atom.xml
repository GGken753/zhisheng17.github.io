<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zhisheng的博客</title>
  
  <subtitle>坑要一个个填，路要一步步走！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.54tianzhisheng.cn/"/>
  <updated>2019-05-30T15:07:08.000Z</updated>
  <id>http://www.54tianzhisheng.cn/</id>
  
  <author>
    <name>zhisheng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink 源码解析 —— 如何获取 ExecutionGraph ？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/26/Flink-code-ExecutionGraph/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/26/Flink-code-ExecutionGraph/</id>
    <published>2019-03-25T16:00:00.000Z</published>
    <updated>2019-05-30T15:07:08.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/3NN3bu3">https://t.zsxq.com/3NN3bu3</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/3NN3bu3&quot;&gt;https://t.zsxq.com/3NN3bu3&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— Flink JobManager 有什么作用？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/25/Flink-code-jobmanager/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/25/Flink-code-jobmanager/</id>
    <published>2019-03-24T16:00:00.000Z</published>
    <updated>2019-05-25T06:30:44.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/2VRrbuf">https://t.zsxq.com/2VRrbuf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/2VRrbuf&quot;&gt;https://t.zsxq.com/2VRrbuf&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— Flink TaskManager 有什么作用？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/25/Flink-code-taskmanager/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/25/Flink-code-taskmanager/</id>
    <published>2019-03-24T16:00:00.000Z</published>
    <updated>2019-05-25T06:31:02.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/RZbu7yN">https://t.zsxq.com/RZbu7yN</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/RZbu7yN&quot;&gt;https://t.zsxq.com/RZbu7yN&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/</id>
    <published>2019-03-23T16:00:00.000Z</published>
    <updated>2019-05-30T14:59:24.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-30-145307.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>如今，许多用于分析大型数据集的开源系统都是用 Java 或者是基于 JVM 的编程语言实现的。最着名的例子是 Apache Hadoop，还有较新的框架，如 Apache Spark、Apache Drill、Apache Flink。基于 JVM 的数据分析引擎面临的一个常见挑战就是如何在内存中存储大量的数据（包括缓存和高效处理）。合理的管理好 JVM 内存可以将 难以配置且不可预测的系统 与 少量配置且稳定运行的系统区分开来。</p><p>在这篇文章中，我们将讨论 Apache Flink 如何管理内存，讨论其自定义序列化与反序列化机制，以及它是如何操作二进制数据的。</p><h3 id="数据对象直接放在堆内存中"><a href="#数据对象直接放在堆内存中" class="headerlink" title="数据对象直接放在堆内存中"></a>数据对象直接放在堆内存中</h3><p>在 JVM 中处理大量数据最直接的方式就是将这些数据做为对象存储在堆内存中，然后直接在内存中操作这些数据，如果想进行排序则就是对对象列表进行排序。然而这种方法有一些明显的缺点，首先，在频繁的创建和销毁大量对象的时候，监视和控制堆内存的使用并不是一件很简单的事情。如果对象分配过多的话，那么会导致内存过度使用，从而触发 OutOfMemoryError，导致 JVM 进程直接被杀死。另一个方面就是因为这些对象大都是生存在新生代，当 JVM 进行垃圾回收时，垃圾收集的开销很容易达到 50% 甚至更多。最后就是 Java 对象具有一定的空间开销（具体取决于 JVM 和平台）。对于具有许多小对象的数据集，这可以显著减少有效可用的内存量。如果你精通系统设计和系统调优，你可以根据系统进行特定的参数调整，可以或多或少的控制出现 OutOfMemoryError 的次数和避免堆内存的过多使用，但是这种设置和调优的作用有限，尤其是在数据量较大和执行环境发生变化的情况下。</p><h3 id="Flink-是怎么做的"><a href="#Flink-是怎么做的" class="headerlink" title="Flink 是怎么做的?"></a>Flink 是怎么做的?</h3><p>Apache Flink 起源于一个研究项目，该项目旨在结合基于 MapReduce 的系统和并行数据库系统的最佳技术。在此背景下，Flink 一直有自己的内存数据处理方法。Flink 将对象序列化为固定数量的预先分配的内存段，而不是直接把对象放在堆内存上。它的 DBMS 风格的排序和连接算法尽可能多地对这个二进制数据进行操作，以此将序列化和反序列化开销降到最低。如果需要处理的数据多于可以保存在内存中的数据，Flink 的运算符会将部分数据溢出到磁盘。事实上，很多Flink 的内部实现看起来更像是 C / C ++，而不是普通的 Java。下图概述了 Flink 如何在内存段中存储序列化数据并在必要时溢出到磁盘：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-051601.jpg" alt=""></p><p>Flink 的主动内存管理和操作二进制数据有几个好处：</p><p>1、<strong>内存安全执行和高效的核外算法</strong> 由于分配的内存段的数量是固定的，因此监控剩余的内存资源是非常简单的。在内存不足的情况下，处理操作符可以有效地将更大批的内存段写入磁盘，后面再将它们读回到内存。因此，OutOfMemoryError 就有效的防止了。</p><p>2、<strong>减少垃圾收集压力</strong> 因为所有长生命周期的数据都是在 Flink 的管理内存中以二进制表示的，所以所有数据对象都是短暂的，甚至是可变的，并且可以重用。短生命周期的对象可以更有效地进行垃圾收集，这大大降低了垃圾收集的压力。现在，预先分配的内存段是 JVM 堆上的长期存在的对象，为了降低垃圾收集的压力，Flink 社区正在积极地将其分配到堆外内存。这种努力将使得 JVM 堆变得更小，垃圾收集所消耗的时间将更少。</p><p>3、<strong>节省空间的数据存储</strong> Java 对象具有存储开销，如果数据以二进制的形式存储，则可以避免这种开销。</p><p>4、<strong>高效的二进制操作和缓存敏感性</strong> 在给定合适的二进制表示的情况下，可以有效地比较和操作二进制数据。此外，二进制表示可以将相关值、哈希码、键和指针等相邻地存储在内存中。这使得数据结构通常具有更高效的缓存访问模式。</p><p>主动内存管理的这些特性在用于大规模数据分析的数据处理系统中是非常可取的，但是要实现这些功能的代价也是高昂的。要实现对二进制数据的自动内存管理和操作并非易事，使用 <code>java.util.HashMap</code> 比实现一个可溢出的 <code>hash-table</code> （由字节数组和自定义序列化支持）。当然，Apache Flink 并不是唯一一个基于 JVM 且对二进制数据进行操作的数据处理系统。例如 Apache Drill、Apache Ignite、Apache Geode 也有应用类似技术，最近 Apache Spark 也宣布将向这个方向演进。</p><p>下面我们将详细讨论 Flink 如何分配内存、如果对对象进行序列化和反序列化以及如果对二进制数据进行操作。我们还将通过一些性能表现数据来比较处理堆内存上的对象和对二进制数据的操作。</p><h3 id="Flink-如何分配内存"><a href="#Flink-如何分配内存" class="headerlink" title="Flink 如何分配内存?"></a>Flink 如何分配内存?</h3><p>Flink TaskManager 是由几个内部组件组成的：actor 系统（负责与 Flink master 协调）、IOManager（负责将数据溢出到磁盘并将其读取回来）、MemoryManager（负责协调内存使用）。在本篇文章中，我们主要讲解 MemoryManager。</p><p>MemoryManager 负责将 MemorySegments 分配、计算和分发给数据处理操作符，例如 sort 和 join 等操作符。MemorySegment 是 Flink 的内存分配单元，由常规 Java 字节数组支持(默认大小为 32 KB)。MemorySegment 通过使用 Java 的 unsafe 方法对其支持的字节数组提供非常有效的读写访问。你可以将 MemorySegment 看作是 Java 的 NIO ByteBuffer 的定制版本。为了在更大的连续内存块上操作多个 MemorySegment，Flink 使用了实现 Java 的 java.io.DataOutput 和 java.io.DataInput 接口的逻辑视图。</p><p>MemorySegments 在 TaskManager 启动时分配一次，并在 TaskManager 关闭时销毁。因此，在 TaskManager 的整个生命周期中，MemorySegment 是重用的，而不会被垃圾收集的。在初始化 TaskManager 的所有内部数据结构并且已启动所有核心服务之后，MemoryManager 开始创建 MemorySegments。默认情况下，服务初始化后，70％ 可用的 JVM 堆内存由 MemoryManager 分配（也可以配置全部）。剩余的 JVM 堆内存用于在任务处理期间实例化的对象，包括由用户定义的函数创建的对象。下图显示了启动后 TaskManager JVM 中的内存分布：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-051655.jpg" alt=""></p><h3 id="Flink-如何序列化对象？"><a href="#Flink-如何序列化对象？" class="headerlink" title="Flink 如何序列化对象？"></a>Flink 如何序列化对象？</h3><p>Java 生态系统提供了几个库，可以将对象转换为二进制表示形式并返回。常见的替代方案是标准 Java 序列化，Kryo，Apache Avro，Apache Thrift 或 Google 的 Protobuf。Flink 包含自己的自定义序列化框架，以便控制数据的二进制表示。这一点很重要，因为对二进制数据进行操作需要对序列化布局有准确的了解。此外，根据在二进制数据上执行的操作配置序列化布局可以显著提升性能。Flink 的序列化机制利用了这一特性，即在执行程序之前，要序列化和反序列化的对象的类型是完全已知的。</p><p>Flink 程序可以处理表示为任意 Java 或 Scala 对象的数据。在优化程序之前，需要识别程序数据流的每个处理步骤中的数据类型。对于 Java 程序，Flink 提供了一个基于反射的类型提取组件，用于分析用户定义函数的返回类型。Scala 程序可以在 Scala 编译器的帮助下进行分析。Flink 使用 TypeInformation 表示每种数据类型。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-23-163200.jpg" alt="Flink 类型分类"></p><blockquote><p>注：该图选自董伟柯的文章《Apache Flink 类型和序列化机制简介》，侵删</p></blockquote><p>Flink 有如下几种数据类型的 TypeInformations：</p><ul><li><p>BasicTypeInfo：所有 Java 的基础类型或 java.lang.String</p></li><li><p>BasicArrayTypeInfo：Java 基本类型构成的数组或 java.lang.String</p></li><li><p>WritableTypeInfo：Hadoop 的 Writable 接口的任何实现</p></li><li><p>TupleTypeInfo：任何 Flink tuple（Tuple1 到 Tuple25）。Flink tuples 是具有类型化字段的固定长度元组的 Java 表示</p></li><li><p>CaseClassTypeInfo：任何 Scala CaseClass（包括 Scala tuples）</p></li><li><p>PojoTypeInfo：任何 POJO（Java 或 Scala），即所有字段都是 public 的或通过 getter 和 setter 访问的对象，遵循通用命名约定</p></li><li><p>GenericTypeInfo：不能标识为其他类型的任何数据类型</p></li></ul><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-23-163454.jpg" alt="TypeInformation 类继承关系图"></p><blockquote><p>注：该图选自董伟柯的文章《Apache Flink 类型和序列化机制简介》，侵删</p></blockquote><p>每个 TypeInformation 都为它所代表的数据类型提供了一个序列化器。例如，BasicTypeInfo 返回一个序列化器，该序列化器写入相应的基本类型；WritableTypeInfo 的序列化器将序列化和反序列化委托给实现 Hadoop 的 Writable 接口的对象的 write() 和 readFields() 方法；GenericTypeInfo 返回一个序列化器，该序列化器将序列化委托给 Kryo。对象将自动通过 Java 中高效的 Unsafe 方法来序列化到 Flink MemorySegments 支持的 DataOutput。对于可用作键的数据类型，例如哈希值，TypeInformation 提供了 TypeComparators，TypeComparators 比较和哈希对象，并且可以根据具体的数据类型有效的比较二进制并提取固定长度的二进制 key 前缀。</p><p>Tuple，Pojo 和 CaseClass 类型是复合类型，它们可能嵌套一个或者多个数据类型。因此，它们的序列化和比较也都比较复杂，一般将其成员数据类型的序列化和比较都交给各自的 Serializers（序列化器） 和 Comparators（比较器）。下图说明了 <code>Tuple3&lt;Integer, Double, Person&gt;</code>对象的序列化，其中<code>Person</code> 是 POJO 并定义如下： </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">int</span> id;</span><br><span class="line">    <span class="keyword">public</span> String name;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-051739.jpg" alt=""></p><p>通过提供定制的 TypeInformations、Serializers（序列化器） 和 Comparators（比较器），可以方便地扩展 Flink 的类型系统，从而提高序列化和比较自定义数据类型的性能。</p><h3 id="Flink-如何对二进制数据进行操作？"><a href="#Flink-如何对二进制数据进行操作？" class="headerlink" title="Flink 如何对二进制数据进行操作？"></a>Flink 如何对二进制数据进行操作？</h3><p>与其他的数据处理框架的 API（包括 SQL）类似，Flink 的 API 也提供了对数据集进行分组、排序和连接等转换操作。这些转换操作的数据集可能非常大。关系数据库系统具有非常高效的算法，比如 merge-sort、merge-join 和 hash-join。Flink 建立在这种技术的基础上，但是主要分为使用自定义序列化和自定义比较器来处理任意对象。在下面文章中我们将通过 Flink 的内存排序算法示例演示 Flink 如何使用二进制数据进行操作。</p><p>Flink 为其数据处理操作符预先分配内存，初始化时，排序算法从 MemoryManager 请求内存预算，并接收一组相应的 MemorySegments。这些 MemorySegments 变成了缓冲区的内存池，缓冲区中收集要排序的数据。下图说明了如何将数据对象序列化到排序缓冲区中：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-051848.jpg" alt=""></p><p>排序缓冲区在内部分为两个内存区域：第一个区域保存所有对象的完整二进制数据，第二个区域包含指向完整二进制对象数据的指针（取决于 key 的数据类型）。将对象添加到排序缓冲区时，它的二进制数据会追加到第一个区域，指针(可能还有一个 key)被追加到第二个区域。分离实际数据和指针以及固定长度的 key 有两个目的：它可以有效的交换固定长度的 entries（key 和指针），还可以减少排序时需要移动的数据。如果排序的 key 是可变长度的数据类型（比如 String），则固定长度的排序 key 必须是前缀 key，比如字符串的前 n 个字符。请注意：并非所有数据类型都提供固定长度的前缀排序 key。将对象序列化到排序缓冲区时，两个内存区域都使用内存池中的 MemorySegments 进行扩展。一旦内存池为空且不能再添加对象时，则排序缓冲区将会被完全填充并可以进行排序。Flink 的排序缓冲区提供了比较和交换元素的方法，这使得实际的排序算法是可插拔的。默认情况下， Flink 使用了 Quicksort（快速排序）实现，可以使用 HeapSort（堆排序）。下图显示了如何比较两个对象：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-051920.jpg" alt=""></p><p>排序缓冲区通过比较它们的二进制固定长度排序 key 来比较两个元素。如果元素的完整 key（不是前缀 key） 或者二进制前缀 key 不相等，则代表比较成功。如果前缀 key 相等(或者排序 key 的数据类型不提供二进制前缀 key)，则排序缓冲区遵循指向实际对象数据的指针，对两个对象进行反序列化并比较对象。根据比较结果，排序算法决定是否交换比较的元素。排序缓冲区通过移动其固定长度 key 和指针来交换两个元素，实际数据不会移动，排序算法完成后，排序缓冲区中的指针被正确排序。下图演示了如何从排序缓冲区返回已排序的数据：</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-051949.jpg" alt=""></p><p>通过顺序读取排序缓冲区的指针区域，跳过排序 key 并按照实际数据的排序指针返回排序数据。此数据要么反序列化并作为对象返回，要么在外部合并排序的情况下复制二进制数据并将其写入磁盘。</p><h3 id="基准测试数据"><a href="#基准测试数据" class="headerlink" title="基准测试数据"></a>基准测试数据</h3><p>那么，对二进制数据进行操作对性能意味着什么？我们将运行一个基准测试，对 1000 万个<code>Tuple2&lt;Integer, String&gt;</code>对象进行排序以找出答案。整数字段的值从均匀分布中采样。String 字段值的长度为 12 个字符，并从长尾分布中进行采样。输入数据由返回可变对象的迭代器提供，即返回具有不同字段值的相同 Tuple 对象实例。Flink 在从内存，网络或磁盘读取数据时使用此技术，以避免不必要的对象实例化。基准测试在具有 900 MB 堆大小的 JVM 中运行，在堆上存储和排序 1000 万个 Tuple 对象并且不会导致触发 OutOfMemoryError 大约需要这么大的内存。我们使用三种排序方法在Integer 字段和 String 字段上对 Tuple 对象进行排序：</p><p>1、<strong>对象存在堆中</strong>：Tuple 对象存储在常用的 <code>java.util.ArrayList</code> 中，初始容量设置为 1000 万，并使用 Java 中常用的集合排序进行排序。</p><ol><li><strong>Flink 序列化</strong>：使用 Flink 的自定义序列化程序将 Tuple 字段序列化为 600 MB 大小的排序缓冲区，如上所述排序，最后再次反序列化。在 Integer 字段上进行排序时，完整的 Integer 用作排序 key，以便排序完全发生在二进制数据上（不需要对象的反序列化）。对于 String 字段的排序，使用 8 字节前缀 key，如果前缀 key 相等，则对 Tuple 对象进行反序列化。</li></ol><p>3、<strong>Kryo 序列化</strong>：使用 Kryo 序列化将 Tuple 字段序列化为 600 MB 大小的排序缓冲区，并在没有二进制排序 key 的情况下进行排序。这意味着每次比较需要对两个对象进行反序列化。</p><p>所有排序方法都使用单线程实现。结果的时间是十次运行结果的平均值。在每次运行之后，我们调用<code>System.gc()</code>请求垃圾收集运行，该运行不会进入测量的执行时间。下图显示了将输入数据存储在内存中，对其进行排序并将其作为对象读回的时间。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-040201.jpg" alt=""></p><p>我们看到 Flink 使用自己的序列化器对二进制数据进行排序明显优于其他两种方法。与存储在堆内存上相比，我们看到将数据加载到内存中要快得多。因为我们实际上是在收集对象，没有机会重用对象实例，但必须重新创建每个 Tuple。这比 Flink 的序列化器（或Kryo序列化）效率低。另一方面，与反序列化相比，从堆中读取对象是无性能消耗的。在我们的基准测试中，对象克隆比序列化和反序列化组合更耗性能。查看排序时间，我们看到对二进制数据的排序也比 Java 的集合排序更快。使用没有二进制排序 key 的 Kryo 序列化的数据排序比其他方法慢得多。这是因为反序列化带来很大的开销。在String 字段上对 Tuple 进行排序比在 Integer 字段上排序更快，因为长尾值分布显着减少了成对比较的数量。为了更好地了解排序过程中发生的状况，我们使用 VisualVM 监控执行的 JVM。以下截图显示了执行 10次 运行时的堆内存使用情况、垃圾收集情况和 CPU 使用情况。</p><p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-25-044335.jpg" alt=""></p><p>测试是在 8 核机器上运行单线程，因此一个核心的完全利用仅对应 12.5％ 的总体利用率。截图显示，对二进制数据进行操作可显著减少垃圾回收活动。对于对象存在堆中，垃圾收集器在排序缓冲区被填满时以非常短的时间间隔运行，并且即使对于单个处理线程也会导致大量 CPU 使用（排序本身不会触发垃圾收集器）。JVM 垃圾收集多个并行线程，解释了高CPU 总体利用率。另一方面，对序列化数据进行操作的方法很少触发垃圾收集器并且 CPU 利用率低得多。实际上，如果使用 Flink 序列化的方式在 Integer 字段上对 Tuple 进行排序，则垃圾收集器根本不运行，因为对于成对比较，不需要反序列化任何对象。Kryo 序列化需要比较多的垃圾收集，因为它不使用二进制排序 key 并且每次排序都要反序列化两个对象。</p><p>内存使用情况上图显示 Flink 序列化和 Kryo 序列化不断的占用大量内存</p><p>存使用情况图表显示flink-serialized和kryo-serialized不断占用大量内存。这是由于 MemorySegments 的预分配。实际内存使用率要低得多，因为排序缓冲区并未完全填充。下表显示了每种方法的内存消耗。1000 万条数据产生大约 280 MB 的二进制数据（对象数据、指针和排序 key），具体取决于使用的序列化程序以及二进制排序 key 的存在和大小。将其与数据存储在堆上的方法进行比较，我们发现对二进制数据进行操作可以显著提高内存效率。在我们的基准测试中，如果序列化为排序缓冲区而不是将其作为堆上的对象保存，则可以在内存中对两倍以上的数据进行排序。</p><table><thead><tr><th>占用内存</th><th>对象存在堆中</th><th>Flink 序列化</th><th>Kryo 序列化</th></tr></thead><tbody><tr><td>对 Integer 排序</td><td>约 700 MB（堆内存）</td><td>277 MB（排序缓冲区）</td><td>266 MB（排序缓冲区）</td></tr><tr><td>对 String 排序</td><td>约 700 MB（堆内存）</td><td>315 MB（排序缓冲区）</td><td>266 MB（排序缓冲区）</td></tr></tbody></table><p>总而言之，测试验证了文章前面说的对二进制数据进行操作的好处。</p><h3 id="展望未来"><a href="#展望未来" class="headerlink" title="展望未来"></a>展望未来</h3><p>Apache Flink 具有相当多的高级技术，可以通过有限的内存资源安全有效地处理大量数据。但是有几点可以使 Flink 更有效率。Flink 社区正在努力将管理内存移动到堆外内存。这将允许更小的 JVM，更低的垃圾收集开销，以及更容易的系统配置。使用 Flink 的 Table API，所有操作（如 aggregation 和 projection）的语义都是已知的（与黑盒用户定义的函数相反）。因此，我们可以为直接对二进制数据进行操作的 Table API 操作生成代码。进一步的改进包括序列化设计，这些设计针对应用于二进制数据的操作和针对序列化器和比较器的代码生成而定制。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li><p>Flink 的主动内存管理减少了因触发 OutOfMemoryErrors 而杀死 JVM 进程和垃圾收集开销的问题。</p></li><li><p>Flink 具有高效的数据序列化和反序列化机制，有助于对二进制数据进行操作，并使更多数据适合内存。</p></li><li><p>Flink 的 DBMS 风格的运算符本身在二进制数据上运行，在必要时可以在内存中高性能地传输到磁盘。</p></li></ul><p>本文地址: <a href="http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/">http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/</a></p><blockquote><p>本文翻译自：<a href="https://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html">https://flink.apache.org/news/2015/05/11/Juggling-with-Bits-and-Bytes.html</a><br>翻译：zhisheng，二次转载请注明地址，否则保留追究法律责任。</p></blockquote><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6Elrml.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/uPBJ5b.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-05-30-145307.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 深度解析 Flink Checkpoint 机制</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/23/Flink-code-checkpoint/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/23/Flink-code-checkpoint/</id>
    <published>2019-03-22T16:00:00.000Z</published>
    <updated>2019-05-22T12:31:47.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/ynQNbeM">https://t.zsxq.com/ynQNbeM</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/ynQNbeM&quot;&gt;https://t.zsxq.com/ynQNbeM&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 深度解析 Flink 序列化机制</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/22/Flink-code-serialize/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/22/Flink-code-serialize/</id>
    <published>2019-03-21T16:00:00.000Z</published>
    <updated>2019-05-22T12:30:48.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/JaQfeMf">https://t.zsxq.com/JaQfeMf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/JaQfeMf&quot;&gt;https://t.zsxq.com/JaQfeMf&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 如何获取 JobGraph？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/21/Flink-code-JobGraph/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/21/Flink-code-JobGraph/</id>
    <published>2019-03-20T16:00:00.000Z</published>
    <updated>2019-04-21T14:18:06.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/naaMf6y">https://t.zsxq.com/naaMf6y</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/naaMf6y&quot;&gt;https://t.zsxq.com/naaMf6y&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 如何获取 StreamGraph？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/20/Flink-code-StreamGraph/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/20/Flink-code-StreamGraph/</id>
    <published>2019-03-19T16:00:00.000Z</published>
    <updated>2019-04-16T15:04:06.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/qRFIm6I">https://t.zsxq.com/qRFIm6I</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/qRFIm6I&quot;&gt;https://t.zsxq.com/qRFIm6I&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/19/Flink-code-streaming-wordcount-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/19/Flink-code-streaming-wordcount-start/</id>
    <published>2019-03-18T16:00:00.000Z</published>
    <updated>2019-04-16T15:02:46.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/qnMFEUJ">https://t.zsxq.com/qnMFEUJ</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/qnMFEUJ&quot;&gt;https://t.zsxq.com/qnMFEUJ&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/18/Flink-code-batch-wordcount-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/18/Flink-code-batch-wordcount-start/</id>
    <published>2019-03-17T16:00:00.000Z</published>
    <updated>2019-04-16T15:00:18.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/YJ2Zrfi">https://t.zsxq.com/YJ2Zrfi</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/YJ2Zrfi&quot;&gt;https://t.zsxq.com/YJ2Zrfi&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/17/Flink-code-Standalone-TaskManager-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/17/Flink-code-Standalone-TaskManager-start/</id>
    <published>2019-03-16T16:00:00.000Z</published>
    <updated>2019-04-16T14:59:00.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/qjEUFau">https://t.zsxq.com/qjEUFau</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/qjEUFau&quot;&gt;https://t.zsxq.com/qjEUFau&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/16/Flink-code-Standalone-JobManager-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/16/Flink-code-Standalone-JobManager-start/</id>
    <published>2019-03-15T16:00:00.000Z</published>
    <updated>2019-04-16T14:57:48.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/AurR3rN">https://t.zsxq.com/AurR3rN</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/AurR3rN&quot;&gt;https://t.zsxq.com/AurR3rN&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— standalonesession 模式启动流程</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/15/Flink-code-Standalone-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/15/Flink-code-Standalone-start/</id>
    <published>2019-03-14T16:00:00.000Z</published>
    <updated>2019-04-16T14:56:44.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/EemAEIi">https://t.zsxq.com/EemAEIi</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/EemAEIi&quot;&gt;https://t.zsxq.com/EemAEIi&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 项目结构一览</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/14/Flink-code-structure/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/14/Flink-code-structure/</id>
    <published>2019-03-13T16:00:00.000Z</published>
    <updated>2019-04-16T14:46:50.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/MNfAYne">https://t.zsxq.com/MNfAYne</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/MNfAYne&quot;&gt;https://t.zsxq.com/MNfAYne&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 从 0 到 1 学习 —— 你上传的 jar 包藏到哪里去了?</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/</id>
    <published>2019-03-12T16:00:00.000Z</published>
    <updated>2019-05-30T15:13:51.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ieWce6.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>写这篇文章其实也是知识星球里面的一个小伙伴问了这样一个问题：</p><blockquote><p>通过 flink UI 仪表盘提交的 jar 是存储在哪个目录下？</p></blockquote><p>这个问题其实我自己也有问过，但是自己因为自己的问题没有啥压力也就没深入去思考，现在可是知识星球的付费小伙伴问的，所以自然要逼着自己去深入然后才能给出正确的答案。</p><p>跟着我一起来看看我的探寻步骤吧！小小的 jar 竟然还敢和我捉迷藏？</p><h3 id="查看配置文件"><a href="#查看配置文件" class="headerlink" title="查看配置文件"></a>查看配置文件</h3><p>首先想到的是这个肯定可以在配置文件中有设置的地方的：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/dAbvto.jpg" alt=""></p><h3 id="谷歌大法好"><a href="#谷歌大法好" class="headerlink" title="谷歌大法好"></a>谷歌大法好</h3><p>虽然有个是 upload 的，但是并不是我们想要的目录！于是，只好动用我的“谷歌大法好”。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/PNdNmo.jpg" alt=""></p><p>找到了一条，点进去看 Issue 如下：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/N4vkGA.jpg" alt=""></p><p>发现这 tm 不就是想要的吗？都支持配置文件来填写上传的 jar 后存储的目录了！赶紧点进去看一波源码：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6uzvWx.jpg" alt=""></p><h3 id="源码确认"><a href="#源码确认" class="headerlink" title="源码确认"></a>源码确认</h3><p>这个 <code>jobmanager.web.upload.dir</code> 是不是？我去看下 1.8 的源码确认一下：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/APCWVr.jpg" alt=""></p><p>发现这个 <code>jobmanager.web.upload.dir</code> 还过期了，用 <code>WebOptions</code> 类中的 <code>UPLOAD_DIR</code> 替代了！</p><p>继续跟进去看看这个 <code>UPLOAD_DIR</code> 是啥玩意？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/i39RJI.jpg" alt=""></p><p>看这注释的意思是说，如果这个配置 <code>web.upload.dir</code> 没有配置具体的路径的话就会使用 <code>JOB_MANAGER_WEB_TMPDIR_KEY</code> 目录，那么我们来看看是否配置了这个目录呢？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/2rEtf8.jpg" alt=""></p><p>确实没有配置这个 jar 文件上传的目录，那么我们来看看这个临时目录 <code>JOB_MANAGER_WEB_TMPDIR_KEY</code> 是在哪里的？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/AhiogN.jpg" alt=""></p><p>又是一个过期的目录，mmp，继续跟下去看下这个目录 <code>TMP_DIR</code>。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/lxOQ3N.jpg" alt=""></p><p>我们查看下配置文件是否有配置这个 <code>web.tmpdir</code> 的值，又是没有：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/W6Y4h6.jpg" alt=""></p><p>so，它肯定使用的是 <code>System.getProperty(&quot;java.io.tmpdir&quot;)</code> 这个目录了，</p><p>我查看了下我本地电脑起的 job 它的配置中有这个配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.io.tmpdir/var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Krx3dH.jpg" alt=""></p><p>再观察了下 job，发现 jobManager 这里有个 <code>web.tmpdir</code> 的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.tmpdir/var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/flink-web-ea909e9e-4bac-452d-8450-b4ff082298c7</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/oGwx4D.jpg" alt=""></p><p>发现这个 <code>web.tmpdir</code> 的就是由 java.io.tmpdir + “flink-web-” + UUID 组成的！</p><h3 id="水落石出"><a href="#水落石出" class="headerlink" title="水落石出"></a>水落石出</h3><p>进入这个目录发现我们上传的 jar 终于被找到了：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/kvmMCC.jpg" alt=""></p><h3 id="配置上传-jar-目录确认"><a href="#配置上传-jar-目录确认" class="headerlink" title="配置上传 jar 目录确认"></a>配置上传 jar 目录确认</h3><p>上面我们虽然已经知道我们上传的 jar 是存储在这个临时目录里，那么我们现在要验证一下，我们在配置文件中配置一下上传 jar 的固定位置，我们先在目录下创建一个 jars 目录，然后在配置文件中加入这个配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.tmpdir: /usr/local/blink-1.5.1/jars</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/aEeHsZ.jpg" alt=""></p><p>更改之后再看 <code>web.tmpdir</code> 是这样的:</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/GnQfIQ.jpg" alt=""></p><p>从 Flink UI 上上传了三个 jar，查看 <code>/usr/local/blink-1.5.1/jars/flink-web-7a98165b-1d56-44be-be8c-d0cd9166b179</code> 目录下就出现了我们的 jar 了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/sN50JN.jpg" alt=""></p><p>我们重启 Flink，发现这三个 jar 又没有了，这也能解释之前我自己也遇到过的问题了，Flink 重启后之前所有上传的 jar 都被删除了！作为生产环境，这样玩，肯定不行的，所以我们还是得固定一个目录来存储所有的上传 jar 包，并且不能够被删除，要配置固定的目录（Flink 重启也不删除的话）需要配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.upload.dir: /usr/local/blink-1.5.1/jars</span><br></pre></td></tr></table></figure><p>这样的话，就可以保证你的 jar 不再会被删除了！</p><p>再来看看源码是咋写的哈：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//从配置文件中找 UPLOAD_DIR</span></span><br><span class="line"><span class="keyword">final</span> Path uploadDir = Paths.get(</span><br><span class="line">config.getString(WebOptions.UPLOAD_DIR,config.getString(WebOptions.TMP_DIR)),</span><br><span class="line"><span class="string">"flink-web-upload"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> RestServerEndpointConfiguration(</span><br><span class="line">restAddress,restBindAddress,port,sslEngineFactory,</span><br><span class="line">uploadDir,maxContentLength,responseHeaders);</span><br></pre></td></tr></table></figure><p>他就是从配置文件中找 <code>UPLOAD_DIR</code>，如果为 <code>null</code> 就找 <code>TMP_DIR</code> 目录来当作 jar 上传的路径！</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文从知识星球一个朋友的问题，从现象到本质再到解决方案的讲解了下如何找到 Flink UI 上上传的 jar 包藏身之处，并提出了如何解决 Flink 上传的 jar 包被删除的问题。</p><p>本篇文章连接是：<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/</a></p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6Elrml.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/uPBJ5b.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ieWce6.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>阿里巴巴开源的 Blink 实时计算框架真香</title>
    <link href="http://www.54tianzhisheng.cn/2019/02/28/blink/"/>
    <id>http://www.54tianzhisheng.cn/2019/02/28/blink/</id>
    <published>2019-02-27T16:00:00.000Z</published>
    <updated>2019-04-25T13:33:57.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-25-133346.jpg" alt=""></p><a id="more"></a><p>Blink 开源了有一段时间了，竟然没发现有人写相关的博客，其实我已经在我的知识星球里开始写了，今天来看看 Blink 为什么香？</p><p>我们先看看 Blink 黑色版本：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/DziJHz.jpg" alt=""></p><p>对比下 Flink 版本你就知道黑色版本多好看了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/sr3sje.jpg" alt=""></p><p>你上传 jar 包的时候是这样的：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/d1MBe7.jpg" alt=""></p><p>我们来看看 Blink 运行的 job 长啥样？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/U07enG.jpg" alt=""></p><p>再来对比一下 Flink 的样子：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/4triEY.jpg" alt=""></p><p>查看 Job Task 的详情，可以看到开始时间、接收记录、并行度、duration、Queue in/out、TPS</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/t644Ll.jpg" alt=""></p><p>查看 subTask，这里可以直接点击这个日志就可以查看 task 日志：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/N5r6pZ.jpg" alt=""></p><p>查看背压：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/cGA6bW.jpg" alt=""></p><p>查看 task metric，可以手动添加，支持的有很多，这点很重要，可以根据每个算子的监控以及时对每个算子进行调优：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Js2sxO.jpg" alt=""></p><p>查看 job 运行时间段的情况：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/pVkBXg.jpg" alt=""></p><p>查看 running 的 job：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/KKrNcP.jpg" alt=""></p><p>查看已经完成的 job：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/oSPEqu.jpg" alt=""></p><p>查看 Task Manager：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/33c6LH.jpg" alt=""></p><p>Task Manager 分配的资源详情：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/zwPTdI.jpg" alt=""></p><p>Task Manager metric 监控信息详情：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/voXrWm.jpg" alt=""></p><p>Task Manager log 文件详情，包含运行产生的日志和 GC 日志：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/nHgGci.jpg" alt=""></p><p>Task Manager 日志详情，支持高亮和分页，特别友好，妈妈再也不担心我看不见 “刷刷刷” 的日志了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/xMFiQq.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>介绍了 Flink 的 Blink 分支编译后运行的界面情况，总体来说很棒，期待后面 Blink 合并到 Flink！</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/02/28/blink/">http://www.54tianzhisheng.cn/2019/02/28/blink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/LM86BR.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/KRGtVb.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-25-133346.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Blink" scheme="http://www.54tianzhisheng.cn/tags/Blink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 源码编译运行</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/</id>
    <published>2019-01-29T16:00:00.000Z</published>
    <updated>2019-04-24T17:09:26.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/VBdn73.jpg" alt=""></p><a id="more"></a><blockquote><p>更新一篇知识星球里面的源码分析文章，去年写的，周末自己录了个视频，大家看下效果好吗？如果好的话，后面补录发在知识星球里面的其他源码解析文章。</p></blockquote><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前自己本地 clone 了 Flink 的源码，编译过，然后 share 到了 GitHub 上去了，自己也写了一些源码的中文注释，并且 push 到了 GitHub 上去了。这几天阿里开源了宣传已久的 Blink，结果我那个分支不能够继续 pull 下新的代码，再加上自己对 Flink 研究了也有点时间了，所以打算将这两个东西对比着来看，这样可能会学到不少更多东西，因为 Blink 是另外一个分支，所以自己干脆再重新 fork 了一份，拉到本地来看源码。</p><h3 id="fork"><a href="#fork" class="headerlink" title="fork"></a>fork</h3><p>执行下面命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:apache/flink.git</span><br></pre></td></tr></table></figure><p>拉取的时候找个网络好点的地方，这样速度可能会更快点。</p><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>因为自己想看下 Blink 分支的代码，所以需要切换到 blink 分支来，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout blink</span><br></pre></td></tr></table></figure><p>这样你就到了 blink 分支了，接下来我们将 blink 源码编译一下，执行如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install -Dmaven.test.skip=true -Dhadoop.version=2.7.6 -Dmaven.javadoc.skip=true -Dcheckstyle.skip=true</span><br></pre></td></tr></table></figure><p>maven 编译的时候跳过测试代码、javadoc 和代码风格检查，这样可以减少不少时间。</p><p>注意：你的 maven 的 settings.xml 文件的 mirror 添加下面这个：(这样下载依赖才能飞起来)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;nexus-aliyun&lt;/id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;*,!jeecg,!jeecg-snapshots,!mapr-releases&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;Nexus aliyun&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br><span class="line"></span><br><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;mapr-public&lt;/id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;mapr-releases&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;mapr-releases&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;https://maven.aliyun.com/repository/mapr-public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure><p>执行完这个命令后，然后呢，你可以掏出手机，打开微信，搜索下微信号：zhisheng_tian , 然后点击一波添加好友，欢迎来探讨技术。</p><p>等了一波时间之后，你可能会遇到这个问题(看到不少童鞋都遇到这个问题，之前编译 Flink 的时候也遇到过)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] Failed to execute goal on project flink-mapr-fs: Could not resolve dependencies for project com.alibaba.blink:flink-mapr-fs:jar:1.5.1: Failure to find com.mapr.hadoop:maprfs:jar:5.2.1-mapr in http://maven.aliyun.com/nexus/content/groups/public was cached in the local repository, resolution will not be reattempted until the update interval of nexus-aliyun has elapsed or updates are forced -&gt; [Help 1]</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/23gzL6.jpg" alt=""></p><p>如果你试了两遍都没编译通过，那么我这里就教大家一种方法（执行完编译命令后啥也没动就 OK 的请跳过，谁叫你运气这么好呢）：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/KGFf05.jpg" alt=""></p><p>在 flink-filesystems 中把 flink-mapr-fs module 给注释掉。</p><p>上图这是我给大家的忠告，特别管用。</p><p>再次执行命令编译起来就没有错误了，如果你还有其他的错误，我猜估计还是网络的问题，导致一些国外的 maven 依赖下载不下来或者不完整，导致的错误，暴力的方法就是和我一样，把这些下载不下来的依赖 module 注释掉，或者你可以像已经编译好的童鞋要下 maven 的 .m2  文件里面已经下载好了的依赖，然后复制粘贴到你的本地路径去，注意路径包名不要弄错了，这样一般可以解决所有的问题了，如果还有问题，我也无能为力了。</p><p>编译成功就长下图这样：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/7ymrpH.jpg" alt=""></p><h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>然后我们的目录是长这样的：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/A71pFo.jpg" alt=""></p><p>标记的那个就是我们的可执行文件，就跟我们在 Flink 官网下载的一样，我们可以将它运行起来看下效果。</p><p>我把它移到了 /usr/local/blink-1.5.1 下了，个人习惯，喜欢把一些安装的软件安装在 /usr/local/ 目录下面。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/DXa2S2.jpg" alt=""></p><p>目录结构和我以前的安装介绍文章类似，就是多了 batch_conf 目录，和 conf 目录是一样的东西，不知道为啥要弄两个配置文件目录，问过负责的人，没理我，哈哈哈。</p><p>那么我们接下来就是运行下 Blink，进入到 bin 目录，执行可执行文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-cluster.sh</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/e8xOxo.jpg" alt=""></p><p>windows 可以点击 start-cluster.bat 启动，这点对 windows 用户比较友好。</p><p>执行完后命令后，在浏览器里访问地址，<figure class="highlight plain"><figcaption><span>, 出现下图这样就代表 Blink 成功启动了：</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/XQLzMv.jpg)</span><br><span class="line"></span><br><span class="line">上图是开源版本的白色主题，骚气的黑色主题通过在 Flink 群里得知如何改之后，编译运行后的效果如下：</span><br><span class="line"></span><br><span class="line">![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/JfcR5E.jpg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">一次好奇的执行了多次上面启动命令，发现也能够正常的运行。</span><br><span class="line"></span><br><span class="line">然后启动的日志是这样的：</span><br><span class="line"></span><br><span class="line">![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/fcAhYL.jpg)</span><br><span class="line"></span><br><span class="line">说明已经启动了 9 个 Task Manager，然后看到我们页面的监控信息如下：</span><br><span class="line"></span><br><span class="line">![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/9Hxhsg.jpg)</span><br><span class="line"></span><br><span class="line">可以看到监控信息里面已经有 40 个可用的 slot，这是因为 Blink 默认的是一个 Task Manager 4 个 slot，我们总共启动了 10 个 Task Manager，所以才会有 40 个可用的 slot，注意：Flink 默认的配置是 1 个 Task Manager 只含有 1 个 slot，不过这个是可以自己分配的。</span><br><span class="line"></span><br><span class="line">注意：开启了多个 Task Manager 后，要关闭的话，得执行同样次数的关闭命令：</span><br><span class="line"></span><br><span class="line">```shell</span><br><span class="line">./stop-cluster.sh</span><br></pre></td></tr></table></figure></p><h3 id="中文源码分析"><a href="#中文源码分析" class="headerlink" title="中文源码分析"></a>中文源码分析</h3><p><a href="https://github.com/zhisheng17/flink">https://github.com/zhisheng17/flink</a></p><h3 id="配套视频解析"><a href="#配套视频解析" class="headerlink" title="配套视频解析"></a>配套视频解析</h3><p>视频录制过程难免说错，还请大家可以指教</p><iframe frameborder="0" src="https://v.qq.com/txp/iframe/player.html?vid=s0858kj6upx" allowFullScreen="true"></iframe><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><p>更多源码解析的文章和 Flink 资料请加知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/IOThpm.jpg" alt=""></p><p>本文地址是：<a href="http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/">http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/</a>，未经允许，禁止转载！</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章是《从1到100深入学习Flink》的第一篇，zhisheng 我带带大家一起如何 clone 项目源码，进行源码编译，然后运行编译后的可执行文件 blink。下篇文章会分析项目源码的结构组成。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/431SkA.jpg" alt=""></p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/VBdn73.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 从 0 到 1 学习 —— Flink 读取 Kafka 数据写入到 RabbitMQ</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/</id>
    <published>2019-01-19T16:00:00.000Z</published>
    <updated>2019-05-30T15:13:51.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/1tNeFo.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前有文章 <a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a> 写过 Flink 将处理后的数据后发到 Kafka 消息队列中去，当然我们常用的消息队列可不止这一种，还有 RocketMQ、RabbitMQ 等，刚好 Flink 也支持将数据写入到 RabbitMQ，所以今天我们就来写篇文章讲讲如何将 Flink 处理后的数据写入到 RabbitMQ。</p><h3 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h3><h4 id="安装-RabbitMQ"><a href="#安装-RabbitMQ" class="headerlink" title="安装 RabbitMQ"></a>安装 RabbitMQ</h4><p>这里我直接用 docker 命令安装吧，先把 docker 在 mac 上启动起来。</p><p>在命令行中执行下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d  -p 15672:15672  -p  5672:5672  -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin --name rabbitmq rabbitmq:3-management</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/1sEuwN.jpg" alt=""></p><p>对这个命令不懂的童鞋可以看看我以前的文章：<a href="http://www.54tianzhisheng.cn/2018/01/26/SpringBoot-RabbitMQ/">http://www.54tianzhisheng.cn/2018/01/26/SpringBoot-RabbitMQ/</a></p><p>登录用户名和密码分别是：admin / admin ，登录进去是这个样子就代表安装成功了：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/pmTM9O.jpg" alt=""></p><h4 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h4><p>pom.xml 中添加 Flink connector rabbitmq 的依赖如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-rabbitmq_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h3><p>这里我们依旧自己写一个工具类一直的往 RabbitMQ 中的某个 queue 中发数据，然后由 Flink 去消费这些数据。</p><p>注意按照我的步骤来一步步操作，否则可能会出现一些错误！</p><p>RabbitMQProducerUtil.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.rabbitmq.client.Channel;</span><br><span class="line"><span class="keyword">import</span> com.rabbitmq.client.Connection;</span><br><span class="line"><span class="keyword">import</span> com.rabbitmq.client.ConnectionFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RabbitMQProducerUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> String QUEUE_NAME = <span class="string">"zhisheng"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//创建连接工厂</span></span><br><span class="line">        ConnectionFactory factory = <span class="keyword">new</span> ConnectionFactory();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置RabbitMQ相关信息</span></span><br><span class="line">        factory.setHost(<span class="string">"localhost"</span>);</span><br><span class="line">        factory.setUsername(<span class="string">"admin"</span>);</span><br><span class="line">        factory.setPassword(<span class="string">"admin"</span>);</span><br><span class="line">        factory.setPort(<span class="number">5672</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个新的连接</span></span><br><span class="line">        Connection connection = factory.newConnection();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个通道</span></span><br><span class="line">        Channel channel = connection.createChannel();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 声明一个队列</span></span><br><span class="line"><span class="comment">//        channel.queueDeclare(QUEUE_NAME, false, false, false, null);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//发送消息到队列中</span></span><br><span class="line">        String message = <span class="string">"Hello zhisheng"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//我们这里演示发送一千条数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++) &#123;</span><br><span class="line">            channel.basicPublish(<span class="string">""</span>, QUEUE_NAME, <span class="keyword">null</span>, (message + i).getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line">            System.out.println(<span class="string">"Producer Send +'"</span> + message + i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭通道和连接</span></span><br><span class="line">        channel.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Flink-主程序"><a href="#Flink-主程序" class="headerlink" title="Flink 主程序"></a>Flink 主程序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.zhisheng.common.utils.ExecutionEnvUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.rabbitmq.RMQSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfig;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从 rabbitmq 读取数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        ParameterTool parameterTool = ExecutionEnvUtil.PARAMETER_TOOL;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//这些配置建议可以放在配置文件中，然后通过 parameterTool 来获取对应的参数值</span></span><br><span class="line">        <span class="keyword">final</span> RMQConnectionConfig connectionConfig = <span class="keyword">new</span> RMQConnectionConfig</span><br><span class="line">                .Builder().setHost(<span class="string">"localhost"</span>).setVirtualHost(<span class="string">"/"</span>)</span><br><span class="line">                .setPort(<span class="number">5672</span>).setUserName(<span class="string">"admin"</span>).setPassword(<span class="string">"admin"</span>)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; zhisheng = env.addSource(<span class="keyword">new</span> RMQSource&lt;&gt;(connectionConfig,</span><br><span class="line">                <span class="string">"zhisheng"</span>,</span><br><span class="line">                <span class="keyword">true</span>,</span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema()))</span><br><span class="line">                .setParallelism(<span class="number">1</span>);</span><br><span class="line">        zhisheng.print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//如果想保证 exactly-once 或 at-least-once 需要把 checkpoint 开启</span></span><br><span class="line"><span class="comment">//        env.enableCheckpointing(10000);</span></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors rabbitmq"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行 RabbitMQProducerUtil 类，再运行 Main 类！</p><p><strong>注意</strong>⚠️：</p><p>1、RMQConnectionConfig 中设置的用户名和密码要设置成 admin/admin，如果你换成是 guest/guest，其实是在 RabbitMQ 里面是没有这个用户名和密码的，所以就会报这个错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nested exception is com.rabbitmq.client.AuthenticationFailureException: ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.</span><br></pre></td></tr></table></figure><p>不出意外的话应该你运行 RabbitMQProducerUtil 类后，立马两个运行的结果都会出来，速度还是很快的。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/F1oBWc.jpg" alt=""></p><p>2、如果你在 RabbitMQProducerUtil 工具类中把注释的那行代码打开的话：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// 声明一个队列</span></span><br><span class="line"><span class="comment">//        channel.queueDeclare(QUEUE_NAME, false, false, false, null);</span></span><br></pre></td></tr></table></figure><p>就会出现这种错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method&lt;channel.close&gt;(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg &apos;durable&apos; for queue &apos;zhisheng&apos; in vhost &apos;/&apos;: received &apos;true&apos; but current is &apos;false&apos;, class-id=50, method-id=10)</span><br></pre></td></tr></table></figure><p>这是因为你打开那个注释的话，一旦你运行了该类就会创建一个叫做 <code>zhisheng</code> 的 Queue，当你再运行 Main 类中的时候，它又会创建这样一个叫 <code>zhisheng</code> 的 Queue，然后因为已经有同名的 Queue 了，所以就有了冲突，解决方法就是把那行代码注释就好了。</p><p>3、该 connector（连接器）中提供了 RMQSource 类去消费 RabbitMQ queue 中的消息和确认 checkpoints 上的消息，它提供了三种不一样的保证：</p><ul><li>Exactly-once(只消费一次): 前提条件有，1 是要开启 checkpoint，因为只有在 checkpoint 完成后，才会返回确认消息给 RabbitMQ（这时，消息才会在 RabbitMQ 队列中删除)；2 是要使用 Correlation ID，在将消息发往 RabbitMQ 时，必须在消息属性中设置 Correlation ID。数据源根据 Correlation ID 把从 checkpoint 恢复的数据进行去重；3 是数据源不能并行，这种限制主要是由于 RabbitMQ 将消息从单个队列分派给多个消费者。</li><li>At-least-once(至少消费一次): 开启了 checkpoint，但未使用相 Correlation ID 或 数据源是并行的时候，那么就只能保证数据至少消费一次了</li><li>No guarantees(无法保证): Flink 接收到数据就返回确认消息给 RabbitMQ</li></ul><h3 id="Sink-数据到-RabbitMQ"><a href="#Sink-数据到-RabbitMQ" class="headerlink" title="Sink 数据到 RabbitMQ"></a>Sink 数据到 RabbitMQ</h3><p>RabbitMQ 除了可以作为数据源，也可以当作下游，Flink 消费数据做了一些处理之后也能把数据发往 RabbitMQ，下面演示下 Flink 消费 Kafka 数据后写入到 RabbitMQ。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main1</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">final</span> RMQConnectionConfig connectionConfig = <span class="keyword">new</span> RMQConnectionConfig</span><br><span class="line">                .Builder().setHost(<span class="string">"localhost"</span>).setVirtualHost(<span class="string">"/"</span>)</span><br><span class="line">                .setPort(<span class="number">5672</span>).setUserName(<span class="string">"admin"</span>).setPassword(<span class="string">"admin"</span>)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//注意，换一个新的 queue，否则也会报错</span></span><br><span class="line">        data.addSink(<span class="keyword">new</span> RMQSink&lt;&gt;(connectionConfig, <span class="string">"zhisheng001"</span>, <span class="keyword">new</span> MetricSchema()));</span><br><span class="line">        env.execute(<span class="string">"flink learning connectors rabbitmq"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>是不是很简单？但是需要注意的是，要换一个之前不存在的 queue，否则是会报错的。</p><p>不出意外的话，你可以看到 RabbitMQ 的监控页面会出现新的一个 queue 出来，如下图：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/d2QROk.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文先把 RabbitMQ 作为数据源，写了个 Flink 消费 RabbitMQ 队列里面的数据进行打印出来，然后又写了个 Flink 消费 Kafka 数据后写入到 RabbitMQ 的例子！</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/pI85H9.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/5WpsW9.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><p>本文的项目代码在 <a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-rabbitmq">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-rabbitmq</a></p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/1tNeFo.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kafka" scheme="http://www.54tianzhisheng.cn/tags/Kafka/"/>
    
      <category term="RabbitMQ" scheme="http://www.54tianzhisheng.cn/tags/RabbitMQ/"/>
    
  </entry>
  
  <entry>
    <title>Flink 从 0 到 1 学习 —— Flink 读取 Kafka 数据批量写入到 MySQL</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/</id>
    <published>2019-01-14T16:00:00.000Z</published>
    <updated>2019-05-30T15:13:51.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/3Cbxab.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前其实在 <a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a> 文章中其实已经写了点将数据写入到 MySQL，但是一些配置化的东西当时是写死的，不能够通用，最近知识星球里有朋友叫我: 写个从 kafka 中读取数据，经过 Flink 做个预聚合，然后创建数据库连接池将数据批量写入到 mysql 的例子。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/XQw9Am.jpg" alt=""></p><p>于是才有了这篇文章，更多提问和想要我写的文章可以在知识星球里像我提问，我会根据提问及时回答和尽可能作出文章的修改。</p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><p>你需要将这两个依赖添加到 pom.xml 中</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.34<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="读取-kafka-数据"><a href="#读取-kafka-数据" class="headerlink" title="读取 kafka 数据"></a>读取 kafka 数据</h3><p>这里我依旧用的以前的 student 类，自己本地起了 kafka 然后造一些测试数据，这里我们测试发送一条数据则 sleep 10s，意味着往 kafka 中一分钟发 6 条数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.connectors.mysql.utils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhisheng.common.utils.GsonUtil;</span><br><span class="line"><span class="keyword">import</span> com.zhisheng.connectors.mysql.model.Student;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 往kafka中写数据,可以使用这个main函数进行测试</span></span><br><span class="line"><span class="comment"> * Created by zhisheng on 2019-02-17</span></span><br><span class="line"><span class="comment"> * Blog: http://www.54tianzhisheng.cn/tags/Flink/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"student"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">100</span>; i++) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(i, <span class="string">"zhisheng"</span> + i, <span class="string">"password"</span> + i, <span class="number">18</span> + i);</span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(student));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + GsonUtil.toJson(student));</span><br><span class="line">            Thread.sleep(<span class="number">10</span> * <span class="number">1000</span>); <span class="comment">//发送一条数据 sleep 10s，相当于 1 分钟 6 条</span></span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        writeToKafka();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从 kafka 中读取数据，然后序列化成 student 对象。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">        <span class="string">"student"</span>,   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">        <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">        props)).setParallelism(<span class="number">1</span>)</span><br><span class="line">        .map(string -&gt; GsonUtil.fromJson(string, Student.class)); <span class="comment">//，解析字符串成 student 对象</span></span><br></pre></td></tr></table></figure><p>因为 RichSinkFunction 中如果 sink 一条数据到 mysql 中就会调用 invoke 方法一次，所以如果要实现批量写的话，我们最好在 sink 之前就把数据聚合一下。那这里我们开个一分钟的窗口去聚合 Student 数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">student.timeWindowAll(Time.minutes(<span class="number">1</span>)).apply(<span class="keyword">new</span> AllWindowFunction&lt;Student, List&lt;Student&gt;, TimeWindow&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(TimeWindow window, Iterable&lt;Student&gt; values, Collector&lt;List&lt;Student&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ArrayList&lt;Student&gt; students = Lists.newArrayList(values);</span><br><span class="line">        <span class="keyword">if</span> (students.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"1 分钟内收集到 student 的数据条数是："</span> + students.size());</span><br><span class="line">            out.collect(students);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="写入数据库"><a href="#写入数据库" class="headerlink" title="写入数据库"></a>写入数据库</h3><p>这里使用 DBCP 连接池连接数据库 mysql，pom.xml 中添加依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-dbcp2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果你想使用其他的数据库连接池请加入对应的依赖。</p><p>这里将数据写入到 MySQL 中，依旧是和之前文章一样继承 RichSinkFunction 类，重写里面的方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.connectors.mysql.sinks;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhisheng.connectors.mysql.model.Student;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.dbcp2.BasicDataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.sql.DataSource;</span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 数据批量 sink 数据到 mysql</span></span><br><span class="line"><span class="comment"> * Created by zhisheng_tian on 2019-02-17</span></span><br><span class="line"><span class="comment"> * Blog: http://www.54tianzhisheng.cn/tags/Flink/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkToMySQL</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">List</span>&lt;<span class="title">Student</span>&gt;&gt; </span>&#123;</span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    BasicDataSource dataSource;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        dataSource = <span class="keyword">new</span> BasicDataSource();</span><br><span class="line">        connection = getConnection(dataSource);</span><br><span class="line">        String sql = <span class="string">"insert into Student(id, name, password, age) values(?, ?, ?, ?);"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 每条数据的插入都要调用一次 invoke() 方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(List&lt;Student&gt; value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//遍历数据集合</span></span><br><span class="line">        <span class="keyword">for</span> (Student student : value) &#123;</span><br><span class="line">            ps.setInt(<span class="number">1</span>, student.getId());</span><br><span class="line">            ps.setString(<span class="number">2</span>, student.getName());</span><br><span class="line">            ps.setString(<span class="number">3</span>, student.getPassword());</span><br><span class="line">            ps.setInt(<span class="number">4</span>, student.getAge());</span><br><span class="line">            ps.addBatch();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span>[] count = ps.executeBatch();<span class="comment">//批量后执行</span></span><br><span class="line">        System.out.println(<span class="string">"成功了插入了"</span> + count.length + <span class="string">"行数据"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">(BasicDataSource dataSource)</span> </span>&#123;</span><br><span class="line">        dataSource.setDriverClassName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">        <span class="comment">//注意，替换成自己本地的 mysql 数据库地址和用户名、密码</span></span><br><span class="line">        dataSource.setUrl(<span class="string">"jdbc:mysql://localhost:3306/test"</span>);</span><br><span class="line">        dataSource.setUsername(<span class="string">"root"</span>);</span><br><span class="line">        dataSource.setPassword(<span class="string">"root123456"</span>);</span><br><span class="line">        <span class="comment">//设置连接池的一些参数</span></span><br><span class="line">        dataSource.setInitialSize(<span class="number">10</span>);</span><br><span class="line">        dataSource.setMaxTotal(<span class="number">50</span>);</span><br><span class="line">        dataSource.setMinIdle(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            con = dataSource.getConnection();</span><br><span class="line">            System.out.println(<span class="string">"创建连接池："</span> + con);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"-----------mysql get connection has exception , msg = "</span> + e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="核心类-Main"><a href="#核心类-Main" class="headerlink" title="核心类 Main"></a>核心类 Main</h3><p>核心程序如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"student"</span>,   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props)).setParallelism(<span class="number">1</span>)</span><br><span class="line">                .map(string -&gt; GsonUtil.fromJson(string, Student.class)); <span class="comment">//</span></span><br><span class="line">        student.timeWindowAll(Time.minutes(<span class="number">1</span>)).apply(<span class="keyword">new</span> AllWindowFunction&lt;Student, List&lt;Student&gt;, TimeWindow&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(TimeWindow window, Iterable&lt;Student&gt; values, Collector&lt;List&lt;Student&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                ArrayList&lt;Student&gt; students = Lists.newArrayList(values);</span><br><span class="line">                <span class="keyword">if</span> (students.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    System.out.println(<span class="string">"1 分钟内收集到 student 的数据条数是："</span> + students.size());</span><br><span class="line">                    out.collect(students);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).addSink(<span class="keyword">new</span> SinkToMySQL());</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors kafka"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运行项目"><a href="#运行项目" class="headerlink" title="运行项目"></a>运行项目</h3><p>运行 Main 类后再运行 KafkaUtils.java 类！</p><p>下图是往 Kafka 中发送的数据：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/N5ryLK.jpg" alt=""></p><p>下图是运行 Main 类的日志，会创建 4 个连接池是因为默认的 4 个并行度，你如果在 addSink 这个算子设置并行度为 1 的话就会创建一个连接池：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ffwhNV.jpg" alt=""></p><p>下图是批量插入数据库的结果：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/P7QApT.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文从知识星球一位朋友的疑问来写的，应该都满足了他的条件（批量/数据库连接池/写入mysql），的确网上很多的例子都是简单的 demo 形式，都是单条数据就创建数据库连接插入 MySQL，如果要写的数据量很大的话，会对 MySQL 的写有很大的压力。这也是我之前在 <a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a> 中，数据写 ES 强调过的，如果要提高性能必定要批量的写。就拿我们现在这篇文章来说，如果数据量大的话，聚合一分钟数据达万条，那么这样批量写会比来一条写一条性能提高不知道有多少。</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/vpyqVe.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/JbuVHA.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><p>本文的项目代码在 <a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-mysql">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-mysql</a></p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/3Cbxab.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="MySQL" scheme="http://www.54tianzhisheng.cn/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>Flink 从 0 到 1 学习 —— Flink parallelism 和 Slot 介绍</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/</id>
    <published>2019-01-13T16:00:00.000Z</published>
    <updated>2019-05-30T15:13:51.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-24-171741.jpg" alt=""><br><a id="more"></a></p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之所以写这个是因为前段时间自己的项目出现过这样的一个问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Caused by: akka.pattern.AskTimeoutException: </span><br><span class="line">Ask timed out on [Actor[akka://flink/user/taskmanager_0#15608456]] after [10000 ms]. </span><br><span class="line">Sender[null] sent message of type &quot;org.apache.flink.runtime.rpc.messages.LocalRpcInvocation&quot;.</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/FkaM6A.jpg" alt=""></p><p>跟着这问题在 Flink 的 Issue 列表里看到了一个类似的问题：<a href="https://issues.apache.org/jira/browse/FLINK-9056">https://issues.apache.org/jira/browse/FLINK-9056</a><br>，看下面的评论差不多就是 TaskManager 的 slot 数量不足的原因，导致 job 提交失败。在 Flink 1.63 中已经修复了变成抛出异常了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/p4Tr9Z.jpg" alt=""></p><p>竟然知道了是因为 slot 不足的原因了，那么我们就要先了解下 slot 是什么东东呢？不过文章这里先介绍下 parallelism。</p><h3 id="什么是-parallelism？"><a href="#什么是-parallelism？" class="headerlink" title="什么是 parallelism？"></a>什么是 parallelism？</h3><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/FaZUcj.jpg" alt=""></p><p>如翻译这样，parallelism 是并行的意思，在 Flink 里面代表每个任务的并行度，适当的提高并行度可以大大提高 job 的执行效率，比如你的 job 消费 kafka 数据过慢，适当调大可能就消费正常了。</p><p>那么在 Flink 中怎么设置并行度呢？</p><h3 id="如何设置-parallelism？"><a href="#如何设置-parallelism？" class="headerlink" title="如何设置 parallelism？"></a>如何设置 parallelism？</h3><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/3DvvDE.jpg" alt=""></p><p>如上图，在 flink 配置文件中可以查看到默认并行度是 1，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat flink-conf.yaml | grep parallelism</span><br><span class="line"></span><br><span class="line"># The parallelism used for programs that did not specify and other parallelism.</span><br><span class="line">parallelism.default: 1</span><br></pre></td></tr></table></figure><p>所以你如何在你的 flink job 里面不设置任何的 parallelism 的话，那么他也会有一个默认的 parallelism = 1。那也意味着你可以修改这个配置文件的默认并行度。</p><p>如果你是用命令行启动你的 Flink job，那么你也可以这样设置并行度(使用 -p 并行度)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -p 10 ../word-count.jar</span><br></pre></td></tr></table></figure><p>你也可以通过这样来设置你整个程序的并行度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(10);</span><br></pre></td></tr></table></figure><p>注意：这样设置的并行度是你整个程序的并行度，那么后面如果你的每个算子不单独设置并行度覆盖的话，那么后面每个算子的并行度就都是这里设置的并行度的值了。</p><p>如何给每个算子单独设置并行度呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="keyword">new</span> xxxKey())</span><br><span class="line">    .flatMap(<span class="keyword">new</span> XxxFlatMapFunction()).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .map(<span class="keyword">new</span> XxxMapFunction).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .addSink(<span class="keyword">new</span> XxxSink()).setParallelism(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>如上，就是在每个算子后面单独的设置并行度，这样的话，就算你前面设置了 env.setParallelism(10) 也是会被覆盖的。</p><p>这也说明优先级是：算子设置并行度 &gt; env 设置并行度 &gt; 配置文件默认并行度</p><p>并行度讲到这里应该都懂了，下面 zhisheng 就继续跟你讲讲 什么是 slot？</p><h3 id="什么是-slot？"><a href="#什么是-slot？" class="headerlink" title="什么是 slot？"></a>什么是 slot？</h3><p>其实什么是 slot 这个问题之前在第一篇文章 <a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a> 中就介绍过了，这里再讲细一点。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/r19yJh.jpg" alt=""></p><p>图中 Task Manager 是从 Job Manager 处接收需要部署的 Task，任务的并行性由每个 Task Manager 上可用的 slot 决定。每个任务代表分配给任务槽的一组资源，slot 在 Flink 里面可以认为是资源组，Flink 将每个任务分成子任务并且将这些子任务分配到 slot 来并行执行程序。</p><p>例如，如果 Task Manager 有四个 slot，那么它将为每个 slot 分配 25％ 的内存。 可以在一个 slot 中运行一个或多个线程。 同一 slot 中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。</p><p>文字说的比较干，zhisheng 这里我就拿下面的图片来讲解：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ECv5y2.jpg" alt=""></p><p>上面图片中有两个 Task Manager，每个 Task Manager 有三个 slot，这样我们的算子最大并行度那么就可以达到 6 个，在同一个 slot 里面可以执行 1 至多个子任务。</p><p>那么再看上面的图片，source/map/keyby/window/apply 最大可以有 6 个并行度，sink 只用了 1 个并行。</p><p>每个 Flink TaskManager 在集群中提供 slot。 slot 的数量通常与每个 TaskManager 的可用 CPU 内核数成比例。一般情况下你的 slot 数是你每个 TaskManager 的 cpu 的核数。</p><p>但是 flink 配置文件中设置的 task manager 默认的 slot 是 1。 </p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/VBxaDH.jpg" alt=""></p><h3 id="slot-和-parallelism"><a href="#slot-和-parallelism" class="headerlink" title="slot 和 parallelism"></a>slot 和 parallelism</h3><p>下面给出官方的图片来更加深刻的理解下 slot：</p><p>1、slot 是指 taskmanager 的并发执行能力</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/zpX2sh.jpg" alt=""></p><p>taskmanager.numberOfTaskSlots:3</p><p>每一个 taskmanager 中的分配 3 个 TaskSlot, 3 个 taskmanager 一共有 9 个 TaskSlot。</p><p>2、parallelism 是指 taskmanager 实际使用的并发能力</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/npq4kW.jpg" alt=""></p><p>parallelism.default:1 </p><p>运行程序默认的并行度为 1，9 个 TaskSlot 只用了 1 个，有 8 个空闲。设置合适的并行度才能提高效率。</p><p>3、parallelism 是可配置、可指定的</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/xAuHJn.jpg" alt=""></p><p>上图中 example2 每个算子设置的并行度是 2， example3 每个算子设置的并行度是 9。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/syrCLs.jpg" alt=""></p><p>example4 除了 sink 是设置的并行度为 1，其他算子设置的并行度都是 9。</p><p>好了，既然并行度和 slot zhisheng 都带大家过了一遍了，那么再来看文章开头的问题：slot 资源不够。</p><h3 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h3><p>现在这个问题的答案其实就已经很明显了，就是我们设置的并行度 parallelism 超过了 Task Manager 能提供的最大 slot 数量，所以才会报这个错误。</p><p>再来拿我的代码来看吧，当时我就是只设置了整个项目的并行度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.setParallelism(15);</span><br></pre></td></tr></table></figure><p>为什么要设置 15 呢，因为我项目消费的 Kafka topic 有 15 个 parttion，就想着让一个并行去消费一个 parttion，没曾想到 Flink 资源的不够，稍微降低下 并行度为 10 后就没出现这个错误了。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文由自己项目生产环境的一个问题来讲解了自己对 Flink parallelism 和 slot 的理解，并告诉大家如何去设置这两个参数，最后也指出了问题的原因所在。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/</a> , 未经允许禁止转载。</p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Rtt4TP.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/mb4tJ7.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-24-171741.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
</feed>
