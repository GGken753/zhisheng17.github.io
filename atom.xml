<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zhisheng的博客</title>
  
  <subtitle>坑要一个个填，路要一步步走！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.54tianzhisheng.cn/"/>
  <updated>2019-05-25T05:41:35.000Z</updated>
  <id>http://www.54tianzhisheng.cn/</id>
  
  <author>
    <name>zhisheng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink 源码解析 —— 深度解析 Flink 是如何管理好内存的？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/24/Flink-code-memory-management/</id>
    <published>2019-03-23T16:00:00.000Z</published>
    <updated>2019-05-25T05:41:35.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/zjQvjeM">https://t.zsxq.com/zjQvjeM</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/zjQvjeM&quot;&gt;https://t.zsxq.com/zjQvjeM&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 深度解析 Flink Checkpoint 机制</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/23/Flink-code-checkpoint/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/23/Flink-code-checkpoint/</id>
    <published>2019-03-22T16:00:00.000Z</published>
    <updated>2019-05-22T12:31:47.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/ynQNbeM">https://t.zsxq.com/ynQNbeM</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/ynQNbeM&quot;&gt;https://t.zsxq.com/ynQNbeM&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 深度解析 Flink 序列化机制</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/22/Flink-code-serialize/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/22/Flink-code-serialize/</id>
    <published>2019-03-21T16:00:00.000Z</published>
    <updated>2019-05-22T12:30:48.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/JaQfeMf">https://t.zsxq.com/JaQfeMf</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/JaQfeMf&quot;&gt;https://t.zsxq.com/JaQfeMf&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 如何获取 JobGraph？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/21/Flink-code-JobGraph/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/21/Flink-code-JobGraph/</id>
    <published>2019-03-20T16:00:00.000Z</published>
    <updated>2019-04-21T14:18:06.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/naaMf6y">https://t.zsxq.com/naaMf6y</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/naaMf6y&quot;&gt;https://t.zsxq.com/naaMf6y&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 如何获取 StreamGraph？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/20/Flink-code-StreamGraph/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/20/Flink-code-StreamGraph/</id>
    <published>2019-03-19T16:00:00.000Z</published>
    <updated>2019-04-16T15:04:06.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/qRFIm6I">https://t.zsxq.com/qRFIm6I</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/qRFIm6I&quot;&gt;https://t.zsxq.com/qRFIm6I&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/19/Flink-code-streaming-wordcount-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/19/Flink-code-streaming-wordcount-start/</id>
    <published>2019-03-18T16:00:00.000Z</published>
    <updated>2019-04-16T15:02:46.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/qnMFEUJ">https://t.zsxq.com/qnMFEUJ</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/qnMFEUJ&quot;&gt;https://t.zsxq.com/qnMFEUJ&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/18/Flink-code-batch-wordcount-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/18/Flink-code-batch-wordcount-start/</id>
    <published>2019-03-17T16:00:00.000Z</published>
    <updated>2019-04-16T15:00:18.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/YJ2Zrfi">https://t.zsxq.com/YJ2Zrfi</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/YJ2Zrfi&quot;&gt;https://t.zsxq.com/YJ2Zrfi&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/17/Flink-code-Standalone-TaskManager-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/17/Flink-code-Standalone-TaskManager-start/</id>
    <published>2019-03-16T16:00:00.000Z</published>
    <updated>2019-04-16T14:59:00.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/qjEUFau">https://t.zsxq.com/qjEUFau</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/qjEUFau&quot;&gt;https://t.zsxq.com/qjEUFau&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/16/Flink-code-Standalone-JobManager-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/16/Flink-code-Standalone-JobManager-start/</id>
    <published>2019-03-15T16:00:00.000Z</published>
    <updated>2019-04-16T14:57:48.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/AurR3rN">https://t.zsxq.com/AurR3rN</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/AurR3rN&quot;&gt;https://t.zsxq.com/AurR3rN&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— standalonesession 模式启动流程</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/15/Flink-code-Standalone-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/15/Flink-code-Standalone-start/</id>
    <published>2019-03-14T16:00:00.000Z</published>
    <updated>2019-04-16T14:56:44.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/EemAEIi">https://t.zsxq.com/EemAEIi</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/EemAEIi&quot;&gt;https://t.zsxq.com/EemAEIi&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 项目结构一览</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/14/Flink-code-structure/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/14/Flink-code-structure/</id>
    <published>2019-03-13T16:00:00.000Z</published>
    <updated>2019-04-16T14:46:50.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/MNfAYne">https://t.zsxq.com/MNfAYne</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/MNfAYne&quot;&gt;https://t.zsxq.com/MNfAYne&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/</id>
    <published>2019-03-12T16:00:00.000Z</published>
    <updated>2019-04-24T17:10:16.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ieWce6.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>写这篇文章其实也是知识星球里面的一个小伙伴问了这样一个问题：</p><blockquote><p>通过 flink UI 仪表盘提交的 jar 是存储在哪个目录下？</p></blockquote><p>这个问题其实我自己也有问过，但是自己因为自己的问题没有啥压力也就没深入去思考，现在可是知识星球的付费小伙伴问的，所以自然要逼着自己去深入然后才能给出正确的答案。</p><p>跟着我一起来看看我的探寻步骤吧！小小的 jar 竟然还敢和我捉迷藏？</p><h3 id="查看配置文件"><a href="#查看配置文件" class="headerlink" title="查看配置文件"></a>查看配置文件</h3><p>首先想到的是这个肯定可以在配置文件中有设置的地方的：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/dAbvto.jpg" alt=""></p><h3 id="谷歌大法好"><a href="#谷歌大法好" class="headerlink" title="谷歌大法好"></a>谷歌大法好</h3><p>虽然有个是 upload 的，但是并不是我们想要的目录！于是，只好动用我的“谷歌大法好”。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/PNdNmo.jpg" alt=""></p><p>找到了一条，点进去看 Issue 如下：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/N4vkGA.jpg" alt=""></p><p>发现这 tm 不就是想要的吗？都支持配置文件来填写上传的 jar 后存储的目录了！赶紧点进去看一波源码：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6uzvWx.jpg" alt=""></p><h3 id="源码确认"><a href="#源码确认" class="headerlink" title="源码确认"></a>源码确认</h3><p>这个 <code>jobmanager.web.upload.dir</code> 是不是？我去看下 1.8 的源码确认一下：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/APCWVr.jpg" alt=""></p><p>发现这个 <code>jobmanager.web.upload.dir</code> 还过期了，用 <code>WebOptions</code> 类中的 <code>UPLOAD_DIR</code> 替代了！</p><p>继续跟进去看看这个 <code>UPLOAD_DIR</code> 是啥玩意？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/i39RJI.jpg" alt=""></p><p>看这注释的意思是说，如果这个配置 <code>web.upload.dir</code> 没有配置具体的路径的话就会使用 <code>JOB_MANAGER_WEB_TMPDIR_KEY</code> 目录，那么我们来看看是否配置了这个目录呢？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/2rEtf8.jpg" alt=""></p><p>确实没有配置这个 jar 文件上传的目录，那么我们来看看这个临时目录 <code>JOB_MANAGER_WEB_TMPDIR_KEY</code> 是在哪里的？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/AhiogN.jpg" alt=""></p><p>又是一个过期的目录，mmp，继续跟下去看下这个目录 <code>TMP_DIR</code>。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/lxOQ3N.jpg" alt=""></p><p>我们查看下配置文件是否有配置这个 <code>web.tmpdir</code> 的值，又是没有：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/W6Y4h6.jpg" alt=""></p><p>so，它肯定使用的是 <code>System.getProperty(&quot;java.io.tmpdir&quot;)</code> 这个目录了，</p><p>我查看了下我本地电脑起的 job 它的配置中有这个配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.io.tmpdir/var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Krx3dH.jpg" alt=""></p><p>再观察了下 job，发现 jobManager 这里有个 <code>web.tmpdir</code> 的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.tmpdir/var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/flink-web-ea909e9e-4bac-452d-8450-b4ff082298c7</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/oGwx4D.jpg" alt=""></p><p>发现这个 <code>web.tmpdir</code> 的就是由 java.io.tmpdir + “flink-web-” + UUID 组成的！</p><h3 id="水落石出"><a href="#水落石出" class="headerlink" title="水落石出"></a>水落石出</h3><p>进入这个目录发现我们上传的 jar 终于被找到了：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/kvmMCC.jpg" alt=""></p><h3 id="配置上传-jar-目录确认"><a href="#配置上传-jar-目录确认" class="headerlink" title="配置上传 jar 目录确认"></a>配置上传 jar 目录确认</h3><p>上面我们虽然已经知道我们上传的 jar 是存储在这个临时目录里，那么我们现在要验证一下，我们在配置文件中配置一下上传 jar 的固定位置，我们先在目录下创建一个 jars 目录，然后在配置文件中加入这个配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.tmpdir: /usr/local/blink-1.5.1/jars</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/aEeHsZ.jpg" alt=""></p><p>更改之后再看 <code>web.tmpdir</code> 是这样的:</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/GnQfIQ.jpg" alt=""></p><p>从 Flink UI 上上传了三个 jar，查看 <code>/usr/local/blink-1.5.1/jars/flink-web-7a98165b-1d56-44be-be8c-d0cd9166b179</code> 目录下就出现了我们的 jar 了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/sN50JN.jpg" alt=""></p><p>我们重启 Flink，发现这三个 jar 又没有了，这也能解释之前我自己也遇到过的问题了，Flink 重启后之前所有上传的 jar 都被删除了！作为生产环境，这样玩，肯定不行的，所以我们还是得固定一个目录来存储所有的上传 jar 包，并且不能够被删除，要配置固定的目录（Flink 重启也不删除的话）需要配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.upload.dir: /usr/local/blink-1.5.1/jars</span><br></pre></td></tr></table></figure><p>这样的话，就可以保证你的 jar 不再会被删除了！</p><p>再来看看源码是咋写的哈：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//从配置文件中找 UPLOAD_DIR</span></span><br><span class="line"><span class="keyword">final</span> Path uploadDir = Paths.get(</span><br><span class="line">config.getString(WebOptions.UPLOAD_DIR,config.getString(WebOptions.TMP_DIR)),</span><br><span class="line"><span class="string">"flink-web-upload"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> RestServerEndpointConfiguration(</span><br><span class="line">restAddress,restBindAddress,port,sslEngineFactory,</span><br><span class="line">uploadDir,maxContentLength,responseHeaders);</span><br></pre></td></tr></table></figure><p>他就是从配置文件中找 <code>UPLOAD_DIR</code>，如果为 <code>null</code> 就找 <code>TMP_DIR</code> 目录来当作 jar 上传的路径！</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文从知识星球一个朋友的问题，从现象到本质再到解决方案的讲解了下如何找到 Flink UI 上上传的 jar 包藏身之处，并提出了如何解决 Flink 上传的 jar 包被删除的问题。</p><p>本篇文章连接是：<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/</a></p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6Elrml.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/uPBJ5b.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ieWce6.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>阿里巴巴开源的 Blink 实时计算框架真香</title>
    <link href="http://www.54tianzhisheng.cn/2019/02/28/blink/"/>
    <id>http://www.54tianzhisheng.cn/2019/02/28/blink/</id>
    <published>2019-02-27T16:00:00.000Z</published>
    <updated>2019-04-25T13:33:57.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-25-133346.jpg" alt=""></p><a id="more"></a><p>Blink 开源了有一段时间了，竟然没发现有人写相关的博客，其实我已经在我的知识星球里开始写了，今天来看看 Blink 为什么香？</p><p>我们先看看 Blink 黑色版本：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/DziJHz.jpg" alt=""></p><p>对比下 Flink 版本你就知道黑色版本多好看了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/sr3sje.jpg" alt=""></p><p>你上传 jar 包的时候是这样的：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/d1MBe7.jpg" alt=""></p><p>我们来看看 Blink 运行的 job 长啥样？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/U07enG.jpg" alt=""></p><p>再来对比一下 Flink 的样子：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/4triEY.jpg" alt=""></p><p>查看 Job Task 的详情，可以看到开始时间、接收记录、并行度、duration、Queue in/out、TPS</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/t644Ll.jpg" alt=""></p><p>查看 subTask，这里可以直接点击这个日志就可以查看 task 日志：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/N5r6pZ.jpg" alt=""></p><p>查看背压：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/cGA6bW.jpg" alt=""></p><p>查看 task metric，可以手动添加，支持的有很多，这点很重要，可以根据每个算子的监控以及时对每个算子进行调优：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Js2sxO.jpg" alt=""></p><p>查看 job 运行时间段的情况：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/pVkBXg.jpg" alt=""></p><p>查看 running 的 job：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/KKrNcP.jpg" alt=""></p><p>查看已经完成的 job：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/oSPEqu.jpg" alt=""></p><p>查看 Task Manager：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/33c6LH.jpg" alt=""></p><p>Task Manager 分配的资源详情：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/zwPTdI.jpg" alt=""></p><p>Task Manager metric 监控信息详情：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/voXrWm.jpg" alt=""></p><p>Task Manager log 文件详情，包含运行产生的日志和 GC 日志：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/nHgGci.jpg" alt=""></p><p>Task Manager 日志详情，支持高亮和分页，特别友好，妈妈再也不担心我看不见 “刷刷刷” 的日志了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/xMFiQq.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>介绍了 Flink 的 Blink 分支编译后运行的界面情况，总体来说很棒，期待后面 Blink 合并到 Flink！</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/02/28/blink/">http://www.54tianzhisheng.cn/2019/02/28/blink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/LM86BR.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/KRGtVb.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-25-133346.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Blink" scheme="http://www.54tianzhisheng.cn/tags/Blink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 源码编译运行</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/</id>
    <published>2019-01-29T16:00:00.000Z</published>
    <updated>2019-04-24T17:09:26.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/VBdn73.jpg" alt=""></p><a id="more"></a><blockquote><p>更新一篇知识星球里面的源码分析文章，去年写的，周末自己录了个视频，大家看下效果好吗？如果好的话，后面补录发在知识星球里面的其他源码解析文章。</p></blockquote><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前自己本地 clone 了 Flink 的源码，编译过，然后 share 到了 GitHub 上去了，自己也写了一些源码的中文注释，并且 push 到了 GitHub 上去了。这几天阿里开源了宣传已久的 Blink，结果我那个分支不能够继续 pull 下新的代码，再加上自己对 Flink 研究了也有点时间了，所以打算将这两个东西对比着来看，这样可能会学到不少更多东西，因为 Blink 是另外一个分支，所以自己干脆再重新 fork 了一份，拉到本地来看源码。</p><h3 id="fork"><a href="#fork" class="headerlink" title="fork"></a>fork</h3><p>执行下面命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:apache/flink.git</span><br></pre></td></tr></table></figure><p>拉取的时候找个网络好点的地方，这样速度可能会更快点。</p><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>因为自己想看下 Blink 分支的代码，所以需要切换到 blink 分支来，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout blink</span><br></pre></td></tr></table></figure><p>这样你就到了 blink 分支了，接下来我们将 blink 源码编译一下，执行如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install -Dmaven.test.skip=true -Dhadoop.version=2.7.6 -Dmaven.javadoc.skip=true -Dcheckstyle.skip=true</span><br></pre></td></tr></table></figure><p>maven 编译的时候跳过测试代码、javadoc 和代码风格检查，这样可以减少不少时间。</p><p>注意：你的 maven 的 settings.xml 文件的 mirror 添加下面这个：(这样下载依赖才能飞起来)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;nexus-aliyun&lt;/id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;*,!jeecg,!jeecg-snapshots,!mapr-releases&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;Nexus aliyun&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br><span class="line"></span><br><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;mapr-public&lt;/id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;mapr-releases&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;mapr-releases&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;https://maven.aliyun.com/repository/mapr-public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure><p>执行完这个命令后，然后呢，你可以掏出手机，打开微信，搜索下微信号：zhisheng_tian , 然后点击一波添加好友，欢迎来探讨技术。</p><p>等了一波时间之后，你可能会遇到这个问题(看到不少童鞋都遇到这个问题，之前编译 Flink 的时候也遇到过)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] Failed to execute goal on project flink-mapr-fs: Could not resolve dependencies for project com.alibaba.blink:flink-mapr-fs:jar:1.5.1: Failure to find com.mapr.hadoop:maprfs:jar:5.2.1-mapr in http://maven.aliyun.com/nexus/content/groups/public was cached in the local repository, resolution will not be reattempted until the update interval of nexus-aliyun has elapsed or updates are forced -&gt; [Help 1]</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/23gzL6.jpg" alt=""></p><p>如果你试了两遍都没编译通过，那么我这里就教大家一种方法（执行完编译命令后啥也没动就 OK 的请跳过，谁叫你运气这么好呢）：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/KGFf05.jpg" alt=""></p><p>在 flink-filesystems 中把 flink-mapr-fs module 给注释掉。</p><p>上图这是我给大家的忠告，特别管用。</p><p>再次执行命令编译起来就没有错误了，如果你还有其他的错误，我猜估计还是网络的问题，导致一些国外的 maven 依赖下载不下来或者不完整，导致的错误，暴力的方法就是和我一样，把这些下载不下来的依赖 module 注释掉，或者你可以像已经编译好的童鞋要下 maven 的 .m2  文件里面已经下载好了的依赖，然后复制粘贴到你的本地路径去，注意路径包名不要弄错了，这样一般可以解决所有的问题了，如果还有问题，我也无能为力了。</p><p>编译成功就长下图这样：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/7ymrpH.jpg" alt=""></p><h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>然后我们的目录是长这样的：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/A71pFo.jpg" alt=""></p><p>标记的那个就是我们的可执行文件，就跟我们在 Flink 官网下载的一样，我们可以将它运行起来看下效果。</p><p>我把它移到了 /usr/local/blink-1.5.1 下了，个人习惯，喜欢把一些安装的软件安装在 /usr/local/ 目录下面。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/DXa2S2.jpg" alt=""></p><p>目录结构和我以前的安装介绍文章类似，就是多了 batch_conf 目录，和 conf 目录是一样的东西，不知道为啥要弄两个配置文件目录，问过负责的人，没理我，哈哈哈。</p><p>那么我们接下来就是运行下 Blink，进入到 bin 目录，执行可执行文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-cluster.sh</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/e8xOxo.jpg" alt=""></p><p>windows 可以点击 start-cluster.bat 启动，这点对 windows 用户比较友好。</p><p>执行完后命令后，在浏览器里访问地址，<figure class="highlight plain"><figcaption><span>, 出现下图这样就代表 Blink 成功启动了：</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/XQLzMv.jpg)</span><br><span class="line"></span><br><span class="line">上图是开源版本的白色主题，骚气的黑色主题通过在 Flink 群里得知如何改之后，编译运行后的效果如下：</span><br><span class="line"></span><br><span class="line">![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/JfcR5E.jpg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">一次好奇的执行了多次上面启动命令，发现也能够正常的运行。</span><br><span class="line"></span><br><span class="line">然后启动的日志是这样的：</span><br><span class="line"></span><br><span class="line">![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/fcAhYL.jpg)</span><br><span class="line"></span><br><span class="line">说明已经启动了 9 个 Task Manager，然后看到我们页面的监控信息如下：</span><br><span class="line"></span><br><span class="line">![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/9Hxhsg.jpg)</span><br><span class="line"></span><br><span class="line">可以看到监控信息里面已经有 40 个可用的 slot，这是因为 Blink 默认的是一个 Task Manager 4 个 slot，我们总共启动了 10 个 Task Manager，所以才会有 40 个可用的 slot，注意：Flink 默认的配置是 1 个 Task Manager 只含有 1 个 slot，不过这个是可以自己分配的。</span><br><span class="line"></span><br><span class="line">注意：开启了多个 Task Manager 后，要关闭的话，得执行同样次数的关闭命令：</span><br><span class="line"></span><br><span class="line">```shell</span><br><span class="line">./stop-cluster.sh</span><br></pre></td></tr></table></figure></p><h3 id="中文源码分析"><a href="#中文源码分析" class="headerlink" title="中文源码分析"></a>中文源码分析</h3><p><a href="https://github.com/zhisheng17/flink">https://github.com/zhisheng17/flink</a></p><h3 id="配套视频解析"><a href="#配套视频解析" class="headerlink" title="配套视频解析"></a>配套视频解析</h3><p>视频录制过程难免说错，还请大家可以指教</p><iframe frameborder="0" src="https://v.qq.com/txp/iframe/player.html?vid=s0858kj6upx" allowFullScreen="true"></iframe><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><p>更多源码解析的文章和 Flink 资料请加知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/IOThpm.jpg" alt=""></p><p>本文地址是：<a href="http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/">http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/</a>，未经允许，禁止转载！</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章是《从1到100深入学习Flink》的第一篇，zhisheng 我带带大家一起如何 clone 项目源码，进行源码编译，然后运行编译后的可执行文件 blink。下篇文章会分析项目源码的结构组成。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/431SkA.jpg" alt=""></p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/VBdn73.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/</id>
    <published>2019-01-19T16:00:00.000Z</published>
    <updated>2019-04-24T17:07:02.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/1tNeFo.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前有文章 <a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a> 写过 Flink 将处理后的数据后发到 Kafka 消息队列中去，当然我们常用的消息队列可不止这一种，还有 RocketMQ、RabbitMQ 等，刚好 Flink 也支持将数据写入到 RabbitMQ，所以今天我们就来写篇文章讲讲如何将 Flink 处理后的数据写入到 RabbitMQ。</p><h3 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h3><h4 id="安装-RabbitMQ"><a href="#安装-RabbitMQ" class="headerlink" title="安装 RabbitMQ"></a>安装 RabbitMQ</h4><p>这里我直接用 docker 命令安装吧，先把 docker 在 mac 上启动起来。</p><p>在命令行中执行下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d  -p 15672:15672  -p  5672:5672  -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin --name rabbitmq rabbitmq:3-management</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/1sEuwN.jpg" alt=""></p><p>对这个命令不懂的童鞋可以看看我以前的文章：<a href="http://www.54tianzhisheng.cn/2018/01/26/SpringBoot-RabbitMQ/">http://www.54tianzhisheng.cn/2018/01/26/SpringBoot-RabbitMQ/</a></p><p>登录用户名和密码分别是：admin / admin ，登录进去是这个样子就代表安装成功了：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/pmTM9O.jpg" alt=""></p><h4 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h4><p>pom.xml 中添加 Flink connector rabbitmq 的依赖如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-rabbitmq_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h3><p>这里我们依旧自己写一个工具类一直的往 RabbitMQ 中的某个 queue 中发数据，然后由 Flink 去消费这些数据。</p><p>注意按照我的步骤来一步步操作，否则可能会出现一些错误！</p><p>RabbitMQProducerUtil.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.rabbitmq.client.Channel;</span><br><span class="line"><span class="keyword">import</span> com.rabbitmq.client.Connection;</span><br><span class="line"><span class="keyword">import</span> com.rabbitmq.client.ConnectionFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RabbitMQProducerUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> String QUEUE_NAME = <span class="string">"zhisheng"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//创建连接工厂</span></span><br><span class="line">        ConnectionFactory factory = <span class="keyword">new</span> ConnectionFactory();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置RabbitMQ相关信息</span></span><br><span class="line">        factory.setHost(<span class="string">"localhost"</span>);</span><br><span class="line">        factory.setUsername(<span class="string">"admin"</span>);</span><br><span class="line">        factory.setPassword(<span class="string">"admin"</span>);</span><br><span class="line">        factory.setPort(<span class="number">5672</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个新的连接</span></span><br><span class="line">        Connection connection = factory.newConnection();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个通道</span></span><br><span class="line">        Channel channel = connection.createChannel();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 声明一个队列</span></span><br><span class="line"><span class="comment">//        channel.queueDeclare(QUEUE_NAME, false, false, false, null);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//发送消息到队列中</span></span><br><span class="line">        String message = <span class="string">"Hello zhisheng"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//我们这里演示发送一千条数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++) &#123;</span><br><span class="line">            channel.basicPublish(<span class="string">""</span>, QUEUE_NAME, <span class="keyword">null</span>, (message + i).getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line">            System.out.println(<span class="string">"Producer Send +'"</span> + message + i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭通道和连接</span></span><br><span class="line">        channel.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Flink-主程序"><a href="#Flink-主程序" class="headerlink" title="Flink 主程序"></a>Flink 主程序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.zhisheng.common.utils.ExecutionEnvUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.rabbitmq.RMQSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfig;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从 rabbitmq 读取数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        ParameterTool parameterTool = ExecutionEnvUtil.PARAMETER_TOOL;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//这些配置建议可以放在配置文件中，然后通过 parameterTool 来获取对应的参数值</span></span><br><span class="line">        <span class="keyword">final</span> RMQConnectionConfig connectionConfig = <span class="keyword">new</span> RMQConnectionConfig</span><br><span class="line">                .Builder().setHost(<span class="string">"localhost"</span>).setVirtualHost(<span class="string">"/"</span>)</span><br><span class="line">                .setPort(<span class="number">5672</span>).setUserName(<span class="string">"admin"</span>).setPassword(<span class="string">"admin"</span>)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; zhisheng = env.addSource(<span class="keyword">new</span> RMQSource&lt;&gt;(connectionConfig,</span><br><span class="line">                <span class="string">"zhisheng"</span>,</span><br><span class="line">                <span class="keyword">true</span>,</span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema()))</span><br><span class="line">                .setParallelism(<span class="number">1</span>);</span><br><span class="line">        zhisheng.print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//如果想保证 exactly-once 或 at-least-once 需要把 checkpoint 开启</span></span><br><span class="line"><span class="comment">//        env.enableCheckpointing(10000);</span></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors rabbitmq"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行 RabbitMQProducerUtil 类，再运行 Main 类！</p><p><strong>注意</strong>⚠️：</p><p>1、RMQConnectionConfig 中设置的用户名和密码要设置成 admin/admin，如果你换成是 guest/guest，其实是在 RabbitMQ 里面是没有这个用户名和密码的，所以就会报这个错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nested exception is com.rabbitmq.client.AuthenticationFailureException: ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.</span><br></pre></td></tr></table></figure><p>不出意外的话应该你运行 RabbitMQProducerUtil 类后，立马两个运行的结果都会出来，速度还是很快的。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/F1oBWc.jpg" alt=""></p><p>2、如果你在 RabbitMQProducerUtil 工具类中把注释的那行代码打开的话：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// 声明一个队列</span></span><br><span class="line"><span class="comment">//        channel.queueDeclare(QUEUE_NAME, false, false, false, null);</span></span><br></pre></td></tr></table></figure><p>就会出现这种错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method&lt;channel.close&gt;(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg &apos;durable&apos; for queue &apos;zhisheng&apos; in vhost &apos;/&apos;: received &apos;true&apos; but current is &apos;false&apos;, class-id=50, method-id=10)</span><br></pre></td></tr></table></figure><p>这是因为你打开那个注释的话，一旦你运行了该类就会创建一个叫做 <figure class="highlight plain"><figcaption><span>的 Queue，当你再运行 Main 类中的时候，它又会创建这样一个叫 ```zhisheng``` 的 Queue，然后因为已经有同名的 Queue 了，所以就有了冲突，解决方法就是把那行代码注释就好了。</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">3、该 connector（连接器）中提供了 RMQSource 类去消费 RabbitMQ queue 中的消息和确认 checkpoints 上的消息，它提供了三种不一样的保证：</span><br><span class="line"></span><br><span class="line">+ Exactly-once(只消费一次): 前提条件有，1 是要开启 checkpoint，因为只有在 checkpoint 完成后，才会返回确认消息给 RabbitMQ（这时，消息才会在 RabbitMQ 队列中删除)；2 是要使用 Correlation ID，在将消息发往 RabbitMQ 时，必须在消息属性中设置 Correlation ID。数据源根据 Correlation ID 把从 checkpoint 恢复的数据进行去重；3 是数据源不能并行，这种限制主要是由于 RabbitMQ 将消息从单个队列分派给多个消费者。</span><br><span class="line">+ At-least-once(至少消费一次): 开启了 checkpoint，但未使用相 Correlation ID 或 数据源是并行的时候，那么就只能保证数据至少消费一次了</span><br><span class="line">+ No guarantees(无法保证): Flink 接收到数据就返回确认消息给 RabbitMQ</span><br><span class="line"></span><br><span class="line">### Sink 数据到 RabbitMQ</span><br><span class="line"></span><br><span class="line">RabbitMQ 除了可以作为数据源，也可以当作下游，Flink 消费数据做了一些处理之后也能把数据发往 RabbitMQ，下面演示下 Flink 消费 Kafka 数据后写入到 RabbitMQ。</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">public class Main1 &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        final ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        final RMQConnectionConfig connectionConfig = new RMQConnectionConfig</span><br><span class="line">                .Builder().setHost(&quot;localhost&quot;).setVirtualHost(&quot;/&quot;)</span><br><span class="line">                .setPort(5672).setUserName(&quot;admin&quot;).setPassword(&quot;admin&quot;)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        //注意，换一个新的 queue，否则也会报错</span><br><span class="line">        data.addSink(new RMQSink&lt;&gt;(connectionConfig, &quot;zhisheng001&quot;, new MetricSchema()));</span><br><span class="line">        env.execute(&quot;flink learning connectors rabbitmq&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>是不是很简单？但是需要注意的是，要换一个之前不存在的 queue，否则是会报错的。</p><p>不出意外的话，你可以看到 RabbitMQ 的监控页面会出现新的一个 queue 出来，如下图：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/d2QROk.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文先把 RabbitMQ 作为数据源，写了个 Flink 消费 RabbitMQ 队列里面的数据进行打印出来，然后又写了个 Flink 消费 Kafka 数据后写入到 RabbitMQ 的例子！</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/pI85H9.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/5WpsW9.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><p>本文的项目代码在 <a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-rabbitmq">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-rabbitmq</a></p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/1tNeFo.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kafka" scheme="http://www.54tianzhisheng.cn/tags/Kafka/"/>
    
      <category term="RabbitMQ" scheme="http://www.54tianzhisheng.cn/tags/RabbitMQ/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/</id>
    <published>2019-01-14T16:00:00.000Z</published>
    <updated>2019-04-24T17:07:48.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/3Cbxab.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前其实在 <a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a> 文章中其实已经写了点将数据写入到 MySQL，但是一些配置化的东西当时是写死的，不能够通用，最近知识星球里有朋友叫我: 写个从 kafka 中读取数据，经过 Flink 做个预聚合，然后创建数据库连接池将数据批量写入到 mysql 的例子。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/XQw9Am.jpg" alt=""></p><p>于是才有了这篇文章，更多提问和想要我写的文章可以在知识星球里像我提问，我会根据提问及时回答和尽可能作出文章的修改。</p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><p>你需要将这两个依赖添加到 pom.xml 中</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.34<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="读取-kafka-数据"><a href="#读取-kafka-数据" class="headerlink" title="读取 kafka 数据"></a>读取 kafka 数据</h3><p>这里我依旧用的以前的 student 类，自己本地起了 kafka 然后造一些测试数据，这里我们测试发送一条数据则 sleep 10s，意味着往 kafka 中一分钟发 6 条数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.connectors.mysql.utils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhisheng.common.utils.GsonUtil;</span><br><span class="line"><span class="keyword">import</span> com.zhisheng.connectors.mysql.model.Student;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 往kafka中写数据,可以使用这个main函数进行测试</span></span><br><span class="line"><span class="comment"> * Created by zhisheng on 2019-02-17</span></span><br><span class="line"><span class="comment"> * Blog: http://www.54tianzhisheng.cn/tags/Flink/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"student"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">100</span>; i++) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(i, <span class="string">"zhisheng"</span> + i, <span class="string">"password"</span> + i, <span class="number">18</span> + i);</span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(student));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + GsonUtil.toJson(student));</span><br><span class="line">            Thread.sleep(<span class="number">10</span> * <span class="number">1000</span>); <span class="comment">//发送一条数据 sleep 10s，相当于 1 分钟 6 条</span></span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        writeToKafka();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从 kafka 中读取数据，然后序列化成 student 对象。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">        <span class="string">"student"</span>,   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">        <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">        props)).setParallelism(<span class="number">1</span>)</span><br><span class="line">        .map(string -&gt; GsonUtil.fromJson(string, Student.class)); <span class="comment">//，解析字符串成 student 对象</span></span><br></pre></td></tr></table></figure><p>因为 RichSinkFunction 中如果 sink 一条数据到 mysql 中就会调用 invoke 方法一次，所以如果要实现批量写的话，我们最好在 sink 之前就把数据聚合一下。那这里我们开个一分钟的窗口去聚合 Student 数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">student.timeWindowAll(Time.minutes(<span class="number">1</span>)).apply(<span class="keyword">new</span> AllWindowFunction&lt;Student, List&lt;Student&gt;, TimeWindow&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(TimeWindow window, Iterable&lt;Student&gt; values, Collector&lt;List&lt;Student&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ArrayList&lt;Student&gt; students = Lists.newArrayList(values);</span><br><span class="line">        <span class="keyword">if</span> (students.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"1 分钟内收集到 student 的数据条数是："</span> + students.size());</span><br><span class="line">            out.collect(students);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="写入数据库"><a href="#写入数据库" class="headerlink" title="写入数据库"></a>写入数据库</h3><p>这里使用 DBCP 连接池连接数据库 mysql，pom.xml 中添加依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-dbcp2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果你想使用其他的数据库连接池请加入对应的依赖。</p><p>这里将数据写入到 MySQL 中，依旧是和之前文章一样继承 RichSinkFunction 类，重写里面的方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.connectors.mysql.sinks;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhisheng.connectors.mysql.model.Student;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.dbcp2.BasicDataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.sql.DataSource;</span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 数据批量 sink 数据到 mysql</span></span><br><span class="line"><span class="comment"> * Created by zhisheng_tian on 2019-02-17</span></span><br><span class="line"><span class="comment"> * Blog: http://www.54tianzhisheng.cn/tags/Flink/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkToMySQL</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">List</span>&lt;<span class="title">Student</span>&gt;&gt; </span>&#123;</span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    BasicDataSource dataSource;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        dataSource = <span class="keyword">new</span> BasicDataSource();</span><br><span class="line">        connection = getConnection(dataSource);</span><br><span class="line">        String sql = <span class="string">"insert into Student(id, name, password, age) values(?, ?, ?, ?);"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 每条数据的插入都要调用一次 invoke() 方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(List&lt;Student&gt; value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//遍历数据集合</span></span><br><span class="line">        <span class="keyword">for</span> (Student student : value) &#123;</span><br><span class="line">            ps.setInt(<span class="number">1</span>, student.getId());</span><br><span class="line">            ps.setString(<span class="number">2</span>, student.getName());</span><br><span class="line">            ps.setString(<span class="number">3</span>, student.getPassword());</span><br><span class="line">            ps.setInt(<span class="number">4</span>, student.getAge());</span><br><span class="line">            ps.addBatch();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span>[] count = ps.executeBatch();<span class="comment">//批量后执行</span></span><br><span class="line">        System.out.println(<span class="string">"成功了插入了"</span> + count.length + <span class="string">"行数据"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">(BasicDataSource dataSource)</span> </span>&#123;</span><br><span class="line">        dataSource.setDriverClassName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">        <span class="comment">//注意，替换成自己本地的 mysql 数据库地址和用户名、密码</span></span><br><span class="line">        dataSource.setUrl(<span class="string">"jdbc:mysql://localhost:3306/test"</span>);</span><br><span class="line">        dataSource.setUsername(<span class="string">"root"</span>);</span><br><span class="line">        dataSource.setPassword(<span class="string">"root123456"</span>);</span><br><span class="line">        <span class="comment">//设置连接池的一些参数</span></span><br><span class="line">        dataSource.setInitialSize(<span class="number">10</span>);</span><br><span class="line">        dataSource.setMaxTotal(<span class="number">50</span>);</span><br><span class="line">        dataSource.setMinIdle(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            con = dataSource.getConnection();</span><br><span class="line">            System.out.println(<span class="string">"创建连接池："</span> + con);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"-----------mysql get connection has exception , msg = "</span> + e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="核心类-Main"><a href="#核心类-Main" class="headerlink" title="核心类 Main"></a>核心类 Main</h3><p>核心程序如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"student"</span>,   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props)).setParallelism(<span class="number">1</span>)</span><br><span class="line">                .map(string -&gt; GsonUtil.fromJson(string, Student.class)); <span class="comment">//</span></span><br><span class="line">        student.timeWindowAll(Time.minutes(<span class="number">1</span>)).apply(<span class="keyword">new</span> AllWindowFunction&lt;Student, List&lt;Student&gt;, TimeWindow&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(TimeWindow window, Iterable&lt;Student&gt; values, Collector&lt;List&lt;Student&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                ArrayList&lt;Student&gt; students = Lists.newArrayList(values);</span><br><span class="line">                <span class="keyword">if</span> (students.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    System.out.println(<span class="string">"1 分钟内收集到 student 的数据条数是："</span> + students.size());</span><br><span class="line">                    out.collect(students);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).addSink(<span class="keyword">new</span> SinkToMySQL());</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors kafka"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运行项目"><a href="#运行项目" class="headerlink" title="运行项目"></a>运行项目</h3><p>运行 Main 类后再运行 KafkaUtils.java 类！</p><p>下图是往 Kafka 中发送的数据：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/N5ryLK.jpg" alt=""></p><p>下图是运行 Main 类的日志，会创建 4 个连接池是因为默认的 4 个并行度，你如果在 addSink 这个算子设置并行度为 1 的话就会创建一个连接池：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ffwhNV.jpg" alt=""></p><p>下图是批量插入数据库的结果：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/P7QApT.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文从知识星球一位朋友的疑问来写的，应该都满足了他的条件（批量/数据库连接池/写入mysql），的确网上很多的例子都是简单的 demo 形式，都是单条数据就创建数据库连接插入 MySQL，如果要写的数据量很大的话，会对 MySQL 的写有很大的压力。这也是我之前在 <a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a> 中，数据写 ES 强调过的，如果要提高性能必定要批量的写。就拿我们现在这篇文章来说，如果数据量大的话，聚合一分钟数据达万条，那么这样批量写会比来一条写一条性能提高不知道有多少。</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/vpyqVe.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/JbuVHA.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><p>本文的项目代码在 <a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-mysql">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-mysql</a></p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/3Cbxab.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="MySQL" scheme="http://www.54tianzhisheng.cn/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/</id>
    <published>2019-01-13T16:00:00.000Z</published>
    <updated>2019-04-24T17:17:52.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-24-171741.jpg" alt=""><br><a id="more"></a></p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之所以写这个是因为前段时间自己的项目出现过这样的一个问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Caused by: akka.pattern.AskTimeoutException: </span><br><span class="line">Ask timed out on [Actor[akka://flink/user/taskmanager_0#15608456]] after [10000 ms]. </span><br><span class="line">Sender[null] sent message of type &quot;org.apache.flink.runtime.rpc.messages.LocalRpcInvocation&quot;.</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/FkaM6A.jpg" alt=""></p><p>跟着这问题在 Flink 的 Issue 列表里看到了一个类似的问题：<a href="https://issues.apache.org/jira/browse/FLINK-9056">https://issues.apache.org/jira/browse/FLINK-9056</a><br>，看下面的评论差不多就是 TaskManager 的 slot 数量不足的原因，导致 job 提交失败。在 Flink 1.63 中已经修复了变成抛出异常了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/p4Tr9Z.jpg" alt=""></p><p>竟然知道了是因为 slot 不足的原因了，那么我们就要先了解下 slot 是什么东东呢？不过文章这里先介绍下 parallelism。</p><h3 id="什么是-parallelism？"><a href="#什么是-parallelism？" class="headerlink" title="什么是 parallelism？"></a>什么是 parallelism？</h3><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/FaZUcj.jpg" alt=""></p><p>如翻译这样，parallelism 是并行的意思，在 Flink 里面代表每个任务的并行度，适当的提高并行度可以大大提高 job 的执行效率，比如你的 job 消费 kafka 数据过慢，适当调大可能就消费正常了。</p><p>那么在 Flink 中怎么设置并行度呢？</p><h3 id="如何设置-parallelism？"><a href="#如何设置-parallelism？" class="headerlink" title="如何设置 parallelism？"></a>如何设置 parallelism？</h3><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/3DvvDE.jpg" alt=""></p><p>如上图，在 flink 配置文件中可以查看到默认并行度是 1，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat flink-conf.yaml | grep parallelism</span><br><span class="line"></span><br><span class="line"># The parallelism used for programs that did not specify and other parallelism.</span><br><span class="line">parallelism.default: 1</span><br></pre></td></tr></table></figure><p>所以你如何在你的 flink job 里面不设置任何的 parallelism 的话，那么他也会有一个默认的 parallelism = 1。那也意味着你可以修改这个配置文件的默认并行度。</p><p>如果你是用命令行启动你的 Flink job，那么你也可以这样设置并行度(使用 -p 并行度)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -p 10 ../word-count.jar</span><br></pre></td></tr></table></figure><p>你也可以通过这样来设置你整个程序的并行度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(10);</span><br></pre></td></tr></table></figure><p>注意：这样设置的并行度是你整个程序的并行度，那么后面如果你的每个算子不单独设置并行度覆盖的话，那么后面每个算子的并行度就都是这里设置的并行度的值了。</p><p>如何给每个算子单独设置并行度呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="keyword">new</span> xxxKey())</span><br><span class="line">    .flatMap(<span class="keyword">new</span> XxxFlatMapFunction()).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .map(<span class="keyword">new</span> XxxMapFunction).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .addSink(<span class="keyword">new</span> XxxSink()).setParallelism(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>如上，就是在每个算子后面单独的设置并行度，这样的话，就算你前面设置了 env.setParallelism(10) 也是会被覆盖的。</p><p>这也说明优先级是：算子设置并行度 &gt; env 设置并行度 &gt; 配置文件默认并行度</p><p>并行度讲到这里应该都懂了，下面 zhisheng 就继续跟你讲讲 什么是 slot？</p><h3 id="什么是-slot？"><a href="#什么是-slot？" class="headerlink" title="什么是 slot？"></a>什么是 slot？</h3><p>其实什么是 slot 这个问题之前在第一篇文章 <a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a> 中就介绍过了，这里再讲细一点。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/r19yJh.jpg" alt=""></p><p>图中 Task Manager 是从 Job Manager 处接收需要部署的 Task，任务的并行性由每个 Task Manager 上可用的 slot 决定。每个任务代表分配给任务槽的一组资源，slot 在 Flink 里面可以认为是资源组，Flink 将每个任务分成子任务并且将这些子任务分配到 slot 来并行执行程序。</p><p>例如，如果 Task Manager 有四个 slot，那么它将为每个 slot 分配 25％ 的内存。 可以在一个 slot 中运行一个或多个线程。 同一 slot 中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。</p><p>文字说的比较干，zhisheng 这里我就拿下面的图片来讲解：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ECv5y2.jpg" alt=""></p><p>上面图片中有两个 Task Manager，每个 Task Manager 有三个 slot，这样我们的算子最大并行度那么就可以达到 6 个，在同一个 slot 里面可以执行 1 至多个子任务。</p><p>那么再看上面的图片，source/map/keyby/window/apply 最大可以有 6 个并行度，sink 只用了 1 个并行。</p><p>每个 Flink TaskManager 在集群中提供 slot。 slot 的数量通常与每个 TaskManager 的可用 CPU 内核数成比例。一般情况下你的 slot 数是你每个 TaskManager 的 cpu 的核数。</p><p>但是 flink 配置文件中设置的 task manager 默认的 slot 是 1。 </p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/VBxaDH.jpg" alt=""></p><h3 id="slot-和-parallelism"><a href="#slot-和-parallelism" class="headerlink" title="slot 和 parallelism"></a>slot 和 parallelism</h3><p>下面给出官方的图片来更加深刻的理解下 slot：</p><p>1、slot 是指 taskmanager 的并发执行能力</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/zpX2sh.jpg" alt=""></p><p>taskmanager.numberOfTaskSlots:3</p><p>每一个 taskmanager 中的分配 3 个 TaskSlot, 3 个 taskmanager 一共有 9 个 TaskSlot。</p><p>2、parallelism 是指 taskmanager 实际使用的并发能力</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/npq4kW.jpg" alt=""></p><p>parallelism.default:1 </p><p>运行程序默认的并行度为 1，9 个 TaskSlot 只用了 1 个，有 8 个空闲。设置合适的并行度才能提高效率。</p><p>3、parallelism 是可配置、可指定的</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/xAuHJn.jpg" alt=""></p><p>上图中 example2 每个算子设置的并行度是 2， example3 每个算子设置的并行度是 9。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/syrCLs.jpg" alt=""></p><p>example4 除了 sink 是设置的并行度为 1，其他算子设置的并行度都是 9。</p><p>好了，既然并行度和 slot zhisheng 都带大家过了一遍了，那么再来看文章开头的问题：slot 资源不够。</p><h3 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h3><p>现在这个问题的答案其实就已经很明显了，就是我们设置的并行度 parallelism 超过了 Task Manager 能提供的最大 slot 数量，所以才会报这个错误。</p><p>再来拿我的代码来看吧，当时我就是只设置了整个项目的并行度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.setParallelism(15);</span><br></pre></td></tr></table></figure><p>为什么要设置 15 呢，因为我项目消费的 Kafka topic 有 15 个 parttion，就想着让一个并行去消费一个 parttion，没曾想到 Flink 资源的不够，稍微降低下 并行度为 10 后就没出现这个错误了。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文由自己项目生产环境的一个问题来讲解了自己对 Flink parallelism 和 slot 的理解，并告诉大家如何去设置这两个参数，最后也指出了问题的原因所在。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/</a> , 未经允许禁止转载。</p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Rtt4TP.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/mb4tJ7.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-24-171741.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink JobManager 高可用性配置</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/</id>
    <published>2019-01-12T16:00:00.000Z</published>
    <updated>2019-04-24T17:07:08.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/duhRAK.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前在 <a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a> 讲过 Flink 的配置，但是后面陆续有人来问我一些配置相关的东西，在加上我现在对 Flink 也更熟悉了些，这里我就再写下 Flink JobManager 的配置相关信息。</p><p>在 <a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a> 一文中介绍过了 Flink Job 的运行架构图：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/svX7Ft.jpg" alt=""></p><p>JobManager 协调每个 Flink 作业的部署。它负责调度和资源管理。</p><p>默认情况下，每个 Flink 集群都有一个 JobManager 实例。这会产生单点故障（SPOF）：如果 JobManager 崩溃，则无法提交新作业且运行中的作业也会失败。</p><p>如果我们使用 JobManager 高可用模式，可以避免这个问题。您可以为 standalone 集群和 YARN 集群配置高可用模式。</p><h2 id="standalone-集群高可用性"><a href="#standalone-集群高可用性" class="headerlink" title="standalone 集群高可用性"></a>standalone 集群高可用性</h2><p>standalone 集群的 JobManager 高可用性的概念是，任何时候都有一个主 JobManager 和 多个备 JobManagers，以便在主节点失败时有新的 JobNamager 接管集群。这样就保证了没有单点故障，一旦备 JobManager 接管集群，作业就可以依旧正常运行。主备 JobManager 实例之间没有明确的区别。每个 JobManager 都可以充当主备节点。</p><p>例如，请考虑以下三个 JobManager 实例的设置：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/lsiuRC.jpg" alt=""></p><h3 id="如何配置"><a href="#如何配置" class="headerlink" title="如何配置"></a>如何配置</h3><p>要启用 JobManager 高可用性功能，您必须将高可用性模式设置为 zookeeper，配置 ZooKeeper quorum，将所有 JobManagers 主机及其 Web UI 端口写入配置文件。</p><p>Flink 利用 ZooKeeper 在所有正在运行的 JobManager 实例之间进行分布式协调。ZooKeeper 是独立于 Flink 的服务，通过 leader 选举和轻量级一致性状态存储提供高可靠的分布式协调服务。Flink 包含用于 Bootstrap ZooKeeper 安装的脚本。<br>他在我们的 Flink 安装路径下面 /conf/zoo.cfg 。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ImJPIC.jpg" alt=""></p><h3 id="Masters-文件"><a href="#Masters-文件" class="headerlink" title="Masters 文件"></a>Masters 文件</h3><p>要启动 HA 集群，请在以下位置配置 Masters 文件 conf/masters：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">localhost:8081</span><br><span class="line">xxx.xxx.xxx.xxx:8081</span><br></pre></td></tr></table></figure><p>masters 文件包含启动 JobManagers 的所有主机以及 Web 用户界面绑定的端口，上面一行写一个。</p><p>默认情况下，job manager 选一个随机端口作为进程通信端口。您可以通过 high-availability.jobmanager.port 更改此设置。此配置接受单个端口（例如 50010），范围（50000-50025）或两者的组合（50010,50011,50020-50025,50050-50075）。</p><h3 id="配置文件-flink-conf-yaml"><a href="#配置文件-flink-conf-yaml" class="headerlink" title="配置文件 (flink-conf.yaml)"></a>配置文件 (flink-conf.yaml)</h3><p>要启动 HA 集群，请将以下配置键添加到 conf/flink-conf.yaml：</p><p>高可用性模式（必需）：在 conf/flink-conf.yaml中，必须将高可用性模式设置为 zookeeper，以打开高可用模式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">high-availability: zookeeper</span><br></pre></td></tr></table></figure><p>ZooKeeper quorum（必需）：ZooKeeper quorum 是一组 ZooKeeper 服务器，它提供分布式协调服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">high-availability.zookeeper.quorum: ip1:2181 [,...],ip2:2181</span><br></pre></td></tr></table></figure><p>每个 ip:port 都是一个 ZooKeeper 服务器的 ip 及其端口，Flink 可以通过指定的地址和端口访问 zookeeper。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/UfuoZZ.jpg" alt=""></p><p>另外就是高可用存储目录，JobManager 元数据保存在文件系统 storageDir 中，在 ZooKeeper 中仅保存了指向此状态的指针, 推荐这个目录是 HDFS, S3, Ceph, nfs 等，该 storageDir 中保存了 JobManager 恢复状态需要的所有元数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">high-availability.storageDir: hdfs:///flink/ha/</span><br></pre></td></tr></table></figure><p>配置 master 文件和 ZooKeeper 配置后，您可以使用提供的集群启动脚本。他们将启动 HA 集群。请注意，启动 Flink HA 集群前，必须启动 Zookeeper 集群，并确保为要启动的每个 HA 集群配置单独的 ZooKeeper 根路径。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>具有 2 个 JobManagers 的 Standalone 集群：</p><p>1、在 conf/flink-conf.yaml 中配置高可用模式和 Zookeeper :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.zookeeper.quorum: localhost:2181</span><br><span class="line">high-availability.storageDir: hdfs:///flink/recovery</span><br></pre></td></tr></table></figure><p>2、在 conf/masters 中 配置 masters:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">localhost:8081</span><br><span class="line">localhost:8082</span><br></pre></td></tr></table></figure><p>3、在 conf/zoo.cfg 中配置 Zookeeper 服务:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server.0=localhost:2888:3888</span><br></pre></td></tr></table></figure><p>4、启动 ZooKeeper 集群:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-zookeeper-quorum.sh</span><br><span class="line">Starting zookeeper daemon on host localhost.</span><br></pre></td></tr></table></figure><p>5、启动一个 Flink HA 集群:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-cluster.sh</span><br><span class="line">Starting HA cluster with 2 masters and 1 peers in ZooKeeper quorum.</span><br><span class="line">Starting jobmanager daemon on host localhost.</span><br><span class="line">Starting jobmanager daemon on host localhost.</span><br><span class="line">Starting taskmanager daemon on host localhost.</span><br></pre></td></tr></table></figure><p>6、停止 ZooKeeper 和集群:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ bin/stop-cluster.sh</span><br><span class="line">Stopping taskmanager daemon (pid: 7647) on localhost.</span><br><span class="line">Stopping jobmanager daemon (pid: 7495) on host localhost.</span><br><span class="line">Stopping jobmanager daemon (pid: 7349) on host localhost.</span><br><span class="line"></span><br><span class="line">$ bin/stop-zookeeper-quorum.sh</span><br><span class="line">Stopping zookeeper daemon (pid: 7101) on host localhost.</span><br></pre></td></tr></table></figure><p>上面的执行脚本如下图可见：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/th7tmL.jpg" alt=""></p><h2 id="YARN-集群高可用性"><a href="#YARN-集群高可用性" class="headerlink" title="YARN 集群高可用性"></a>YARN 集群高可用性</h2><p>当运行高可用的 YARN 集群时，我们不会运行多个 JobManager 实例，而只会运行一个，该 JobManager 实例失败时，YARN 会将其重新启动。Yarn 的具体行为取决于您使用的 YARN 版本。</p><h3 id="如何配置？"><a href="#如何配置？" class="headerlink" title="如何配置？"></a>如何配置？</h3><h3 id="Application-Master-最大重试次数-yarn-site-xml"><a href="#Application-Master-最大重试次数-yarn-site-xml" class="headerlink" title="Application Master 最大重试次数 (yarn-site.xml)"></a>Application Master 最大重试次数 (yarn-site.xml)</h3><p>在 YARN 配置文件 yarn-site.xml 中，需要配置 application master 的最大重试次数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;</span><br><span class="line">    The maximum number of application master execution attempts.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>当前 YARN 版本的默认值为 2（表示允许单个 JobManager 失败两次）。</p><h3 id="Application-Attempts-flink-conf-yaml"><a href="#Application-Attempts-flink-conf-yaml" class="headerlink" title="Application Attempts (flink-conf.yaml)"></a>Application Attempts (flink-conf.yaml)</h3><p>除了上面可以配置最大重试次数外，你还可以在 flink-conf.yaml 配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn.application-attempts: 10</span><br></pre></td></tr></table></figure><p>这意味着在如果程序启动失败，YARN 会再重试 9 次（9 次重试 + 1 次启动），如果启动 10 次作业还失败，yarn 才会将该任务的状态置为失败。如果因为节点硬件故障或重启，NodeManager 重新同步等操作，需要 YARN 继续尝试启动应用。这些重启尝试不计入 yarn.application-attempts 个数中。</p><h3 id="容器关闭行为"><a href="#容器关闭行为" class="headerlink" title="容器关闭行为"></a>容器关闭行为</h3><ul><li>YARN 2.3.0 &lt; 版本 &lt; 2.4.0. 如果 application master 进程失败，则所有的 container 都会重启。</li><li>YARN 2.4.0 &lt; 版本 &lt; 2.6.0. TaskManager container 在 application master 故障期间，会继续工作。这具有以下优点：作业恢复时间更快，且缩短所有 task manager 启动时申请资源的时间。</li><li>YARN 2.6.0 &lt;= version: 将尝试失败有效性间隔设置为 Flink 的 Akka 超时值。尝试失败有效性间隔表示只有在系统在一个间隔期间看到最大应用程序尝试次数后才会终止应用程序。这避免了持久的工作会耗尽它的应用程序尝试。</li></ul><h3 id="示例：高可用的-YARN-Session"><a href="#示例：高可用的-YARN-Session" class="headerlink" title="示例：高可用的 YARN Session"></a>示例：高可用的 YARN Session</h3><p>1、配置 HA 模式和 Zookeeper 集群 在 conf/flink-conf.yaml:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.zookeeper.quorum: localhost:2181</span><br><span class="line">yarn.application-attempts: 10</span><br></pre></td></tr></table></figure></p><p>2、配置 ZooKeeper 服务 在 conf/zoo.cfg：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server.0=localhost:2888:3888</span><br></pre></td></tr></table></figure></p><p>3、启动 Zookeeper 集群:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-zookeeper-quorum.sh</span><br><span class="line">Starting zookeeper daemon on host localhost.</span><br></pre></td></tr></table></figure></p><p>4、启动 HA 集群:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/yarn-session.sh -n 2</span><br></pre></td></tr></table></figure></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章再次写了下 Flink JobManager 的高可用配置，如何在 standalone 集群和 YARN 集群中配置高可用。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/</a> , 未经允许禁止转载。</p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/fMnpK6.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/VYEi11.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/duhRAK.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 写入数据到 Kafka</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/</id>
    <published>2019-01-05T16:00:00.000Z</published>
    <updated>2019-04-24T17:08:02.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/XYl9Ug.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前文章 <a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a> 写了如何将 Kafka 中的数据存储到 ElasticSearch 中，里面其实就已经用到了 Flink 自带的 Kafka source connector（FlinkKafkaConsumer）。存入到 ES 只是其中一种情况，那么如果我们有多个地方需要这份通过 Flink 转换后的数据，是不是又要我们继续写个 sink 的插件呢？确实，所以 Flink 里面就默认支持了不少 sink，比如也支持 Kafka sink connector（FlinkKafkaProducer），那么这篇文章我们就讲讲如何将数据写入到 Kafka。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/cRMkpP.jpg" alt=""></p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><p>Flink 里面支持 Kafka 0.8、0.9、0.10、0.11 ，以后有时间可以分析下源码的实现。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/NWXmgY.jpg" alt=""></p><p>这里我们需要安装下 Kafka，请对应添加对应的 Flink Kafka connector 依赖的版本，这里我们使用的是 0.11 版本：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="Kafka-安装"><a href="#Kafka-安装" class="headerlink" title="Kafka 安装"></a>Kafka 安装</h4><p>这里就不写这块内容了，可以参考我以前的文章 <a href="http://www.54tianzhisheng.cn/2018/01/04/Kafka/">Kafka 安装及快速入门</a>。</p><p>这里我们演示把其他 Kafka 集群中 topic 数据原样写入到自己本地起的 Kafka 中去。</p><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kafka.brokers=xxx:9092,xxx:9092,xxx:9092</span><br><span class="line">kafka.group.id=metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=xxx:2181</span><br><span class="line">metrics.topic=xxx</span><br><span class="line">stream.parallelism=5</span><br><span class="line">kafka.sink.brokers=localhost:9092</span><br><span class="line">kafka.sink.topic=metric-test</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure><p>目前我们先看下本地 Kafka 是否有这个 metric-test topic 呢？需要执行下这个命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6KFHKT.jpg" alt=""></p><p>可以看到本地的 Kafka 是没有任何 topic 的，如果等下我们的程序运行起来后，再次执行这个命令出现 metric-test topic，那么证明我的程序确实起作用了，已经将其他集群的 Kafka 数据写入到本地 Kafka 了。</p><h4 id="程序代码"><a href="#程序代码" class="headerlink" title="程序代码"></a>程序代码</h4><p>Main.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        data.addSink(<span class="keyword">new</span> FlinkKafkaProducer011&lt;Metrics&gt;(</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.brokers"</span>),</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.topic"</span>),</span><br><span class="line">                <span class="keyword">new</span> MetricSchema()</span><br><span class="line">                )).name(<span class="string">"flink-connectors-kafka"</span>)</span><br><span class="line">                .setParallelism(parameterTool.getInt(<span class="string">"stream.sink.parallelism"</span>));</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors kafka"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><p>启动程序，查看运行结果，不段执行上面命令，查看是否有新的 topic 出来：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/nxqZmZ.jpg" alt=""></p><p>执行命令可以查看该 topic 的信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic metric-test</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/y5vPRR.jpg" alt=""></p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>上面代码我们使用 Flink Kafka Producer 只传了三个参数：brokerList、topicId、serializationSchema（序列化）</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/KN766D.jpg" alt=""></p><p>其实也可以传入多个参数进去，现在有的参数用的是默认参数，因为这个内容比较多，后面可以抽出一篇文章单独来讲。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章写了 Flink 读取其他 Kafka 集群的数据，然后写入到本地的 Kafka 上。我在 Flink 这层没做什么数据转换，只是原样的将数据转发了下，如果你们有什么其他的需求，是可以在 Flink 这层将数据进行各种转换操作，比如这篇文章中的一些转换：<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a>，然后将转换后的数据发到 Kafka 上去。</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/5AtPqZ.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/iaNuzk.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/XYl9Ug.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kafka" scheme="http://www.54tianzhisheng.cn/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 项目如何运行？</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/05/Flink-run/</id>
    <published>2019-01-04T16:00:00.000Z</published>
    <updated>2019-04-24T17:07:26.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/HXmHJo.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前写了不少 Flink 文章了，也有不少 demo，但是文章写的时候都是在本地直接运行 Main 类的 main 方法，其实 Flink 是支持在 UI 上上传 Flink Job 的 jar 包，然后运行得。最开始在第一篇 <a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install/">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a> 中其实提到过了 Flink 自带的 UI 界面，今天我们就来看看如何将我们的项目打包在这里发布运行。</p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><h4 id="编译打包"><a href="#编译打包" class="headerlink" title="编译打包"></a>编译打包</h4><p>项目代码就拿我之前的文章 <a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a> 吧，代码地址是在 GitHub 仓库地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6</a> ，如果感兴趣的可以直接拿来打包试试水。</p><p>我们在整个项目 （flink-learning）pom.xml 所在文件夹执行以下命令打包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ap2P8l.jpg" alt=""></p><p>然后你会发现在 flink-learning-connectors-es6 的 target 目录下有 flink-learning-connectors-es6-1.0-SNAPSHOT.jar 。</p><h4 id="启动-ES"><a href="#启动-ES" class="headerlink" title="启动 ES"></a>启动 ES</h4><p>注意你的 Kafka 数据源和 ES 都已经启动好了, 清空了下 ES 目录下的 data 数据，为了就是查看是不是真的有数据存入进来了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/lmspd0.jpg" alt=""></p><h4 id="提交-jar-包"><a href="#提交-jar-包" class="headerlink" title="提交 jar 包"></a>提交 jar 包</h4><p>将此文件提交到 Flinkserver 上，如下图：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/O9hKsX.jpg" alt=""></p><p>点击下图红框中的”Upload”按钮：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/lfsYoh.jpg" alt=""></p><p>如下图，选中刚刚上传的文件，填写类名，再点击”Submit”按钮即可启动 Job：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/mhpBAN.jpg" alt=""></p><h3 id="查看运行结果"><a href="#查看运行结果" class="headerlink" title="查看运行结果"></a>查看运行结果</h3><p>如下图，在 Overview 页面可见正在运行的任务：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/vS3hIG.jpg" alt=""></p><p>你可以看到 Task Manager 中关于任务的 metric 数据<br>、日志信息以及 Stdout 信息。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/krIFlu.jpg" alt=""></p><p>查看 Kibana ，此时 ES 中已经有数据了：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/BSvuej.jpg" alt=""></p><p>我们可以在 flink ui 界面上的 overview cancel 这个 job，那么可以看到 job 的日志：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/oH0rF4.jpg" alt=""></p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/xXLOdJ.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章写了下如何将我们的 job 编译打包并提交到 Flink 自带到 Server UI 上面去运行，也算是对前面文章的一个补充，当然了，Flink job 不仅支持这种模式的运行，它还可以运行在 K8s，Mesos，等上面，等以后我接触到再写写。</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">http://www.54tianzhisheng.cn/2019/01/05/Flink-run/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/uz6cl1.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/OEhLqC.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/HXmHJo.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
</feed>
