<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>zhisheng的博客</title>
  
  <subtitle>坑要一个个填，路要一步步走！</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://www.54tianzhisheng.cn/"/>
  <updated>2019-04-21T14:18:06.000Z</updated>
  <id>http://www.54tianzhisheng.cn/</id>
  
  <author>
    <name>zhisheng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Flink 源码解析 —— 如何获取 JobGraph？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/21/Flink-code-JobGraph/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/21/Flink-code-JobGraph/</id>
    <published>2019-03-20T16:00:00.000Z</published>
    <updated>2019-04-21T14:18:06.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/naaMf6y">https://t.zsxq.com/naaMf6y</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/naaMf6y&quot;&gt;https://t.zsxq.com/naaMf6y&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 如何获取 StreamGraph？</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/20/Flink-code-StreamGraph/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/20/Flink-code-StreamGraph/</id>
    <published>2019-03-19T16:00:00.000Z</published>
    <updated>2019-04-16T15:04:06.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/qRFIm6I">https://t.zsxq.com/qRFIm6I</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/qRFIm6I&quot;&gt;https://t.zsxq.com/qRFIm6I&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 分析 Streaming WordCount 程序的执行过程</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/19/Flink-code-streaming-wordcount-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/19/Flink-code-streaming-wordcount-start/</id>
    <published>2019-03-18T16:00:00.000Z</published>
    <updated>2019-04-16T15:02:46.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/qnMFEUJ">https://t.zsxq.com/qnMFEUJ</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/qnMFEUJ&quot;&gt;https://t.zsxq.com/qnMFEUJ&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 分析 Batch WordCount 程序的执行过程</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/18/Flink-code-batch-wordcount-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/18/Flink-code-batch-wordcount-start/</id>
    <published>2019-03-17T16:00:00.000Z</published>
    <updated>2019-04-16T15:00:18.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/YJ2Zrfi">https://t.zsxq.com/YJ2Zrfi</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/YJ2Zrfi&quot;&gt;https://t.zsxq.com/YJ2Zrfi&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Task Manager 启动</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/17/Flink-code-Standalone-TaskManager-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/17/Flink-code-Standalone-TaskManager-start/</id>
    <published>2019-03-16T16:00:00.000Z</published>
    <updated>2019-04-16T14:59:00.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/qjEUFau">https://t.zsxq.com/qjEUFau</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/qjEUFau&quot;&gt;https://t.zsxq.com/qjEUFau&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— Standalone Session Cluster 启动流程深度分析之 Job Manager 启动</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/16/Flink-code-Standalone-JobManager-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/16/Flink-code-Standalone-JobManager-start/</id>
    <published>2019-03-15T16:00:00.000Z</published>
    <updated>2019-04-16T14:57:48.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/AurR3rN">https://t.zsxq.com/AurR3rN</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/AurR3rN&quot;&gt;https://t.zsxq.com/AurR3rN&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— standalonesession 模式启动流程</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/15/Flink-code-Standalone-start/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/15/Flink-code-Standalone-start/</id>
    <published>2019-03-14T16:00:00.000Z</published>
    <updated>2019-04-16T14:56:44.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/EemAEIi">https://t.zsxq.com/EemAEIi</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/EemAEIi&quot;&gt;https://t.zsxq.com/EemAEIi&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 项目结构一览</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/14/Flink-code-structure/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/14/Flink-code-structure/</id>
    <published>2019-03-13T16:00:00.000Z</published>
    <updated>2019-04-16T14:46:50.000Z</updated>
    
    <content type="html"><![CDATA[<a id="more"></a><p><a href="https://t.zsxq.com/MNfAYne">https://t.zsxq.com/MNfAYne</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;a id=&quot;more&quot;&gt;&lt;/a&gt;
&lt;p&gt;&lt;a href=&quot;https://t.zsxq.com/MNfAYne&quot;&gt;https://t.zsxq.com/MNfAYne&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</title>
    <link href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/"/>
    <id>http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/</id>
    <published>2019-03-12T16:00:00.000Z</published>
    <updated>2019-04-24T17:10:16.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ieWce6.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>写这篇文章其实也是知识星球里面的一个小伙伴问了这样一个问题：</p><blockquote><p>通过 flink UI 仪表盘提交的 jar 是存储在哪个目录下？</p></blockquote><p>这个问题其实我自己也有问过，但是自己因为自己的问题没有啥压力也就没深入去思考，现在可是知识星球的付费小伙伴问的，所以自然要逼着自己去深入然后才能给出正确的答案。</p><p>跟着我一起来看看我的探寻步骤吧！小小的 jar 竟然还敢和我捉迷藏？</p><h3 id="查看配置文件"><a href="#查看配置文件" class="headerlink" title="查看配置文件"></a>查看配置文件</h3><p>首先想到的是这个肯定可以在配置文件中有设置的地方的：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/dAbvto.jpg" alt=""></p><h3 id="谷歌大法好"><a href="#谷歌大法好" class="headerlink" title="谷歌大法好"></a>谷歌大法好</h3><p>虽然有个是 upload 的，但是并不是我们想要的目录！于是，只好动用我的“谷歌大法好”。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/PNdNmo.jpg" alt=""></p><p>找到了一条，点进去看 Issue 如下：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/N4vkGA.jpg" alt=""></p><p>发现这 tm 不就是想要的吗？都支持配置文件来填写上传的 jar 后存储的目录了！赶紧点进去看一波源码：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6uzvWx.jpg" alt=""></p><h3 id="源码确认"><a href="#源码确认" class="headerlink" title="源码确认"></a>源码确认</h3><p>这个 <code>jobmanager.web.upload.dir</code> 是不是？我去看下 1.8 的源码确认一下：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/APCWVr.jpg" alt=""></p><p>发现这个 <code>jobmanager.web.upload.dir</code> 还过期了，用 <code>WebOptions</code> 类中的 <code>UPLOAD_DIR</code> 替代了！</p><p>继续跟进去看看这个 <code>UPLOAD_DIR</code> 是啥玩意？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/i39RJI.jpg" alt=""></p><p>看这注释的意思是说，如果这个配置 <code>web.upload.dir</code> 没有配置具体的路径的话就会使用 <code>JOB_MANAGER_WEB_TMPDIR_KEY</code> 目录，那么我们来看看是否配置了这个目录呢？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/2rEtf8.jpg" alt=""></p><p>确实没有配置这个 jar 文件上传的目录，那么我们来看看这个临时目录 <code>JOB_MANAGER_WEB_TMPDIR_KEY</code> 是在哪里的？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/AhiogN.jpg" alt=""></p><p>又是一个过期的目录，mmp，继续跟下去看下这个目录 <code>TMP_DIR</code>。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/lxOQ3N.jpg" alt=""></p><p>我们查看下配置文件是否有配置这个 <code>web.tmpdir</code> 的值，又是没有：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/W6Y4h6.jpg" alt=""></p><p>so，它肯定使用的是 <code>System.getProperty(&quot;java.io.tmpdir&quot;)</code> 这个目录了，</p><p>我查看了下我本地电脑起的 job 它的配置中有这个配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.io.tmpdir/var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Krx3dH.jpg" alt=""></p><p>再观察了下 job，发现 jobManager 这里有个 <code>web.tmpdir</code> 的配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.tmpdir/var/folders/mb/3vpbvkkx13l2jmpt2kmmt0fr0000gn/T/flink-web-ea909e9e-4bac-452d-8450-b4ff082298c7</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/oGwx4D.jpg" alt=""></p><p>发现这个 <code>web.tmpdir</code> 的就是由 java.io.tmpdir + “flink-web-” + UUID 组成的！</p><h3 id="水落石出"><a href="#水落石出" class="headerlink" title="水落石出"></a>水落石出</h3><p>进入这个目录发现我们上传的 jar 终于被找到了：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/kvmMCC.jpg" alt=""></p><h3 id="配置上传-jar-目录确认"><a href="#配置上传-jar-目录确认" class="headerlink" title="配置上传 jar 目录确认"></a>配置上传 jar 目录确认</h3><p>上面我们虽然已经知道我们上传的 jar 是存储在这个临时目录里，那么我们现在要验证一下，我们在配置文件中配置一下上传 jar 的固定位置，我们先在目录下创建一个 jars 目录，然后在配置文件中加入这个配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.tmpdir: /usr/local/blink-1.5.1/jars</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/aEeHsZ.jpg" alt=""></p><p>更改之后再看 <code>web.tmpdir</code> 是这样的:</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/GnQfIQ.jpg" alt=""></p><p>从 Flink UI 上上传了三个 jar，查看 <code>/usr/local/blink-1.5.1/jars/flink-web-7a98165b-1d56-44be-be8c-d0cd9166b179</code> 目录下就出现了我们的 jar 了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/sN50JN.jpg" alt=""></p><p>我们重启 Flink，发现这三个 jar 又没有了，这也能解释之前我自己也遇到过的问题了，Flink 重启后之前所有上传的 jar 都被删除了！作为生产环境，这样玩，肯定不行的，所以我们还是得固定一个目录来存储所有的上传 jar 包，并且不能够被删除，要配置固定的目录（Flink 重启也不删除的话）需要配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">web.upload.dir: /usr/local/blink-1.5.1/jars</span><br></pre></td></tr></table></figure><p>这样的话，就可以保证你的 jar 不再会被删除了！</p><p>再来看看源码是咋写的哈：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//从配置文件中找 UPLOAD_DIR</span></span><br><span class="line"><span class="keyword">final</span> Path uploadDir = Paths.get(</span><br><span class="line">config.getString(WebOptions.UPLOAD_DIR,config.getString(WebOptions.TMP_DIR)),</span><br><span class="line"><span class="string">"flink-web-upload"</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="keyword">new</span> RestServerEndpointConfiguration(</span><br><span class="line">restAddress,restBindAddress,port,sslEngineFactory,</span><br><span class="line">uploadDir,maxContentLength,responseHeaders);</span><br></pre></td></tr></table></figure><p>他就是从配置文件中找 <code>UPLOAD_DIR</code>，如果为 <code>null</code> 就找 <code>TMP_DIR</code> 目录来当作 jar 上传的路径！</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文从知识星球一个朋友的问题，从现象到本质再到解决方案的讲解了下如何找到 Flink UI 上上传的 jar 包藏身之处，并提出了如何解决 Flink 上传的 jar 包被删除的问题。</p><p>本篇文章连接是：<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/</a></p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6Elrml.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/uPBJ5b.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ieWce6.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>阿里巴巴开源的 Blink 实时计算框架真香</title>
    <link href="http://www.54tianzhisheng.cn/2019/02/28/blink/"/>
    <id>http://www.54tianzhisheng.cn/2019/02/28/blink/</id>
    <published>2019-02-27T16:00:00.000Z</published>
    <updated>2019-04-24T17:10:28.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://ws3.sinaimg.cn/large/006tNbRwly1fy7f3ylm7hj30zk0pf11z.jpg" alt=""></p><a id="more"></a><p>Blink 开源了有一段时间了，竟然没发现有人写相关的博客，其实我已经在我的知识星球里开始写了，今天来看看 Blink 为什么香？</p><p>我们先看看 Blink 黑色版本：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/DziJHz.jpg" alt=""></p><p>对比下 Flink 版本你就知道黑色版本多好看了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/sr3sje.jpg" alt=""></p><p>你上传 jar 包的时候是这样的：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/d1MBe7.jpg" alt=""></p><p>我们来看看 Blink 运行的 job 长啥样？</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/U07enG.jpg" alt=""></p><p>再来对比一下 Flink 的样子：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/4triEY.jpg" alt=""></p><p>查看 Job Task 的详情，可以看到开始时间、接收记录、并行度、duration、Queue in/out、TPS</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/t644Ll.jpg" alt=""></p><p>查看 subTask，这里可以直接点击这个日志就可以查看 task 日志：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/N5r6pZ.jpg" alt=""></p><p>查看背压：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/cGA6bW.jpg" alt=""></p><p>查看 task metric，可以手动添加，支持的有很多，这点很重要，可以根据每个算子的监控以及时对每个算子进行调优：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Js2sxO.jpg" alt=""></p><p>查看 job 运行时间段的情况：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/pVkBXg.jpg" alt=""></p><p>查看 running 的 job：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/KKrNcP.jpg" alt=""></p><p>查看已经完成的 job：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/oSPEqu.jpg" alt=""></p><p>查看 Task Manager：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/33c6LH.jpg" alt=""></p><p>Task Manager 分配的资源详情：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/zwPTdI.jpg" alt=""></p><p>Task Manager metric 监控信息详情：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/voXrWm.jpg" alt=""></p><p>Task Manager log 文件详情，包含运行产生的日志和 GC 日志：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/nHgGci.jpg" alt=""></p><p>Task Manager 日志详情，支持高亮和分页，特别友好，妈妈再也不担心我看不见 “刷刷刷” 的日志了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/xMFiQq.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>介绍了 Flink 的 Blink 分支编译后运行的界面情况，总体来说很棒，期待后面 Blink 合并到 Flink！</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/02/28/blink/">http://www.54tianzhisheng.cn/2019/02/28/blink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/LM86BR.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/KRGtVb.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://ws3.sinaimg.cn/large/006tNbRwly1fy7f3ylm7hj30zk0pf11z.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Blink" scheme="http://www.54tianzhisheng.cn/tags/Blink/"/>
    
  </entry>
  
  <entry>
    <title>Flink 源码解析 —— 源码编译运行</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/</id>
    <published>2019-01-29T16:00:00.000Z</published>
    <updated>2019-04-24T17:09:26.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/VBdn73.jpg" alt=""></p><a id="more"></a><blockquote><p>更新一篇知识星球里面的源码分析文章，去年写的，周末自己录了个视频，大家看下效果好吗？如果好的话，后面补录发在知识星球里面的其他源码解析文章。</p></blockquote><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前自己本地 clone 了 Flink 的源码，编译过，然后 share 到了 GitHub 上去了，自己也写了一些源码的中文注释，并且 push 到了 GitHub 上去了。这几天阿里开源了宣传已久的 Blink，结果我那个分支不能够继续 pull 下新的代码，再加上自己对 Flink 研究了也有点时间了，所以打算将这两个东西对比着来看，这样可能会学到不少更多东西，因为 Blink 是另外一个分支，所以自己干脆再重新 fork 了一份，拉到本地来看源码。</p><h3 id="fork"><a href="#fork" class="headerlink" title="fork"></a>fork</h3><p>执行下面命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone git@github.com:apache/flink.git</span><br></pre></td></tr></table></figure><p>拉取的时候找个网络好点的地方，这样速度可能会更快点。</p><h3 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h3><p>因为自己想看下 Blink 分支的代码，所以需要切换到 blink 分支来，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git checkout blink</span><br></pre></td></tr></table></figure><p>这样你就到了 blink 分支了，接下来我们将 blink 源码编译一下，执行如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install -Dmaven.test.skip=true -Dhadoop.version=2.7.6 -Dmaven.javadoc.skip=true -Dcheckstyle.skip=true</span><br></pre></td></tr></table></figure><p>maven 编译的时候跳过测试代码、javadoc 和代码风格检查，这样可以减少不少时间。</p><p>注意：你的 maven 的 settings.xml 文件的 mirror 添加下面这个：(这样下载依赖才能飞起来)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;nexus-aliyun&lt;/id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;*,!jeecg,!jeecg-snapshots,!mapr-releases&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;Nexus aliyun&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br><span class="line"></span><br><span class="line">&lt;mirror&gt;</span><br><span class="line">  &lt;id&gt;mapr-public&lt;/id&gt;</span><br><span class="line">  &lt;mirrorOf&gt;mapr-releases&lt;/mirrorOf&gt;</span><br><span class="line">  &lt;name&gt;mapr-releases&lt;/name&gt;</span><br><span class="line">  &lt;url&gt;https://maven.aliyun.com/repository/mapr-public&lt;/url&gt;</span><br><span class="line">&lt;/mirror&gt;</span><br></pre></td></tr></table></figure><p>执行完这个命令后，然后呢，你可以掏出手机，打开微信，搜索下微信号：zhisheng_tian , 然后点击一波添加好友，欢迎来探讨技术。</p><p>等了一波时间之后，你可能会遇到这个问题(看到不少童鞋都遇到这个问题，之前编译 Flink 的时候也遇到过)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] Failed to execute goal on project flink-mapr-fs: Could not resolve dependencies for project com.alibaba.blink:flink-mapr-fs:jar:1.5.1: Failure to find com.mapr.hadoop:maprfs:jar:5.2.1-mapr in http://maven.aliyun.com/nexus/content/groups/public was cached in the local repository, resolution will not be reattempted until the update interval of nexus-aliyun has elapsed or updates are forced -&gt; [Help 1]</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/23gzL6.jpg" alt=""></p><p>如果你试了两遍都没编译通过，那么我这里就教大家一种方法（执行完编译命令后啥也没动就 OK 的请跳过，谁叫你运气这么好呢）：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/KGFf05.jpg" alt=""></p><p>在 flink-filesystems 中把 flink-mapr-fs module 给注释掉。</p><p>上图这是我给大家的忠告，特别管用。</p><p>再次执行命令编译起来就没有错误了，如果你还有其他的错误，我猜估计还是网络的问题，导致一些国外的 maven 依赖下载不下来或者不完整，导致的错误，暴力的方法就是和我一样，把这些下载不下来的依赖 module 注释掉，或者你可以像已经编译好的童鞋要下 maven 的 .m2  文件里面已经下载好了的依赖，然后复制粘贴到你的本地路径去，注意路径包名不要弄错了，这样一般可以解决所有的问题了，如果还有问题，我也无能为力了。</p><p>编译成功就长下图这样：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/7ymrpH.jpg" alt=""></p><h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>然后我们的目录是长这样的：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/A71pFo.jpg" alt=""></p><p>标记的那个就是我们的可执行文件，就跟我们在 Flink 官网下载的一样，我们可以将它运行起来看下效果。</p><p>我把它移到了 /usr/local/blink-1.5.1 下了，个人习惯，喜欢把一些安装的软件安装在 /usr/local/ 目录下面。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/DXa2S2.jpg" alt=""></p><p>目录结构和我以前的安装介绍文章类似，就是多了 batch_conf 目录，和 conf 目录是一样的东西，不知道为啥要弄两个配置文件目录，问过负责的人，没理我，哈哈哈。</p><p>那么我们接下来就是运行下 Blink，进入到 bin 目录，执行可执行文件：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-cluster.sh</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/e8xOxo.jpg" alt=""></p><p>windows 可以点击 start-cluster.bat 启动，这点对 windows 用户比较友好。</p><p>执行完后命令后，在浏览器里访问地址，<figure class="highlight plain"><figcaption><span>, 出现下图这样就代表 Blink 成功启动了：</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/XQLzMv.jpg)</span><br><span class="line"></span><br><span class="line">上图是开源版本的白色主题，骚气的黑色主题通过在 Flink 群里得知如何改之后，编译运行后的效果如下：</span><br><span class="line"></span><br><span class="line">![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/JfcR5E.jpg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">一次好奇的执行了多次上面启动命令，发现也能够正常的运行。</span><br><span class="line"></span><br><span class="line">然后启动的日志是这样的：</span><br><span class="line"></span><br><span class="line">![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/fcAhYL.jpg)</span><br><span class="line"></span><br><span class="line">说明已经启动了 9 个 Task Manager，然后看到我们页面的监控信息如下：</span><br><span class="line"></span><br><span class="line">![](https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/9Hxhsg.jpg)</span><br><span class="line"></span><br><span class="line">可以看到监控信息里面已经有 40 个可用的 slot，这是因为 Blink 默认的是一个 Task Manager 4 个 slot，我们总共启动了 10 个 Task Manager，所以才会有 40 个可用的 slot，注意：Flink 默认的配置是 1 个 Task Manager 只含有 1 个 slot，不过这个是可以自己分配的。</span><br><span class="line"></span><br><span class="line">注意：开启了多个 Task Manager 后，要关闭的话，得执行同样次数的关闭命令：</span><br><span class="line"></span><br><span class="line">```shell</span><br><span class="line">./stop-cluster.sh</span><br></pre></td></tr></table></figure></p><h3 id="中文源码分析"><a href="#中文源码分析" class="headerlink" title="中文源码分析"></a>中文源码分析</h3><p><a href="https://github.com/zhisheng17/flink">https://github.com/zhisheng17/flink</a></p><h3 id="配套视频解析"><a href="#配套视频解析" class="headerlink" title="配套视频解析"></a>配套视频解析</h3><p>视频录制过程难免说错，还请大家可以指教</p><iframe frameborder="0" src="https://v.qq.com/txp/iframe/player.html?vid=s0858kj6upx" allowFullScreen="true"></iframe><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><p>更多源码解析的文章和 Flink 资料请加知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/IOThpm.jpg" alt=""></p><p>本文地址是：<a href="http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/">http://www.54tianzhisheng.cn/2019/01/30/Flink-code-compile/</a>，未经允许，禁止转载！</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章是《从1到100深入学习Flink》的第一篇，zhisheng 我带带大家一起如何 clone 项目源码，进行源码编译，然后运行编译后的可执行文件 blink。下篇文章会分析项目源码的结构组成。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/431SkA.jpg" alt=""></p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/VBdn73.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/</id>
    <published>2019-01-19T16:00:00.000Z</published>
    <updated>2019-04-24T17:07:02.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/1tNeFo.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前有文章 <a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a> 写过 Flink 将处理后的数据后发到 Kafka 消息队列中去，当然我们常用的消息队列可不止这一种，还有 RocketMQ、RabbitMQ 等，刚好 Flink 也支持将数据写入到 RabbitMQ，所以今天我们就来写篇文章讲讲如何将 Flink 处理后的数据写入到 RabbitMQ。</p><h3 id="前提准备"><a href="#前提准备" class="headerlink" title="前提准备"></a>前提准备</h3><h4 id="安装-RabbitMQ"><a href="#安装-RabbitMQ" class="headerlink" title="安装 RabbitMQ"></a>安装 RabbitMQ</h4><p>这里我直接用 docker 命令安装吧，先把 docker 在 mac 上启动起来。</p><p>在命令行中执行下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -d  -p 15672:15672  -p  5672:5672  -e RABBITMQ_DEFAULT_USER=admin -e RABBITMQ_DEFAULT_PASS=admin --name rabbitmq rabbitmq:3-management</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/1sEuwN.jpg" alt=""></p><p>对这个命令不懂的童鞋可以看看我以前的文章：<a href="http://www.54tianzhisheng.cn/2018/01/26/SpringBoot-RabbitMQ/">http://www.54tianzhisheng.cn/2018/01/26/SpringBoot-RabbitMQ/</a></p><p>登录用户名和密码分别是：admin / admin ，登录进去是这个样子就代表安装成功了：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/pmTM9O.jpg" alt=""></p><h4 id="依赖"><a href="#依赖" class="headerlink" title="依赖"></a>依赖</h4><p>pom.xml 中添加 Flink connector rabbitmq 的依赖如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-rabbitmq_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="生产者"><a href="#生产者" class="headerlink" title="生产者"></a>生产者</h3><p>这里我们依旧自己写一个工具类一直的往 RabbitMQ 中的某个 queue 中发数据，然后由 Flink 去消费这些数据。</p><p>注意按照我的步骤来一步步操作，否则可能会出现一些错误！</p><p>RabbitMQProducerUtil.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.rabbitmq.client.Channel;</span><br><span class="line"><span class="keyword">import</span> com.rabbitmq.client.Connection;</span><br><span class="line"><span class="keyword">import</span> com.rabbitmq.client.ConnectionFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RabbitMQProducerUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">final</span> <span class="keyword">static</span> String QUEUE_NAME = <span class="string">"zhisheng"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//创建连接工厂</span></span><br><span class="line">        ConnectionFactory factory = <span class="keyword">new</span> ConnectionFactory();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置RabbitMQ相关信息</span></span><br><span class="line">        factory.setHost(<span class="string">"localhost"</span>);</span><br><span class="line">        factory.setUsername(<span class="string">"admin"</span>);</span><br><span class="line">        factory.setPassword(<span class="string">"admin"</span>);</span><br><span class="line">        factory.setPort(<span class="number">5672</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个新的连接</span></span><br><span class="line">        Connection connection = factory.newConnection();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//创建一个通道</span></span><br><span class="line">        Channel channel = connection.createChannel();</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 声明一个队列</span></span><br><span class="line"><span class="comment">//        channel.queueDeclare(QUEUE_NAME, false, false, false, null);</span></span><br><span class="line"></span><br><span class="line">        <span class="comment">//发送消息到队列中</span></span><br><span class="line">        String message = <span class="string">"Hello zhisheng"</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//我们这里演示发送一千条数据</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">1000</span>; i++) &#123;</span><br><span class="line">            channel.basicPublish(<span class="string">""</span>, QUEUE_NAME, <span class="keyword">null</span>, (message + i).getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line">            System.out.println(<span class="string">"Producer Send +'"</span> + message + i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//关闭通道和连接</span></span><br><span class="line">        channel.close();</span><br><span class="line">        connection.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Flink-主程序"><a href="#Flink-主程序" class="headerlink" title="Flink 主程序"></a>Flink 主程序</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> com.zhisheng.common.utils.ExecutionEnvUtil;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.common.serialization.SimpleStringSchema;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.api.java.utils.ParameterTool;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.datastream.DataStreamSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.rabbitmq.RMQSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.connectors.rabbitmq.common.RMQConnectionConfig;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 从 rabbitmq 读取数据</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        ParameterTool parameterTool = ExecutionEnvUtil.PARAMETER_TOOL;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//这些配置建议可以放在配置文件中，然后通过 parameterTool 来获取对应的参数值</span></span><br><span class="line">        <span class="keyword">final</span> RMQConnectionConfig connectionConfig = <span class="keyword">new</span> RMQConnectionConfig</span><br><span class="line">                .Builder().setHost(<span class="string">"localhost"</span>).setVirtualHost(<span class="string">"/"</span>)</span><br><span class="line">                .setPort(<span class="number">5672</span>).setUserName(<span class="string">"admin"</span>).setPassword(<span class="string">"admin"</span>)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        DataStreamSource&lt;String&gt; zhisheng = env.addSource(<span class="keyword">new</span> RMQSource&lt;&gt;(connectionConfig,</span><br><span class="line">                <span class="string">"zhisheng"</span>,</span><br><span class="line">                <span class="keyword">true</span>,</span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema()))</span><br><span class="line">                .setParallelism(<span class="number">1</span>);</span><br><span class="line">        zhisheng.print();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//如果想保证 exactly-once 或 at-least-once 需要把 checkpoint 开启</span></span><br><span class="line"><span class="comment">//        env.enableCheckpointing(10000);</span></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors rabbitmq"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行 RabbitMQProducerUtil 类，再运行 Main 类！</p><p><strong>注意</strong>⚠️：</p><p>1、RMQConnectionConfig 中设置的用户名和密码要设置成 admin/admin，如果你换成是 guest/guest，其实是在 RabbitMQ 里面是没有这个用户名和密码的，所以就会报这个错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nested exception is com.rabbitmq.client.AuthenticationFailureException: ACCESS_REFUSED - Login was refused using authentication mechanism PLAIN. For details see the broker logfile.</span><br></pre></td></tr></table></figure><p>不出意外的话应该你运行 RabbitMQProducerUtil 类后，立马两个运行的结果都会出来，速度还是很快的。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/F1oBWc.jpg" alt=""></p><p>2、如果你在 RabbitMQProducerUtil 工具类中把注释的那行代码打开的话：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">// 声明一个队列</span></span><br><span class="line"><span class="comment">//        channel.queueDeclare(QUEUE_NAME, false, false, false, null);</span></span><br></pre></td></tr></table></figure><p>就会出现这种错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Caused by: com.rabbitmq.client.ShutdownSignalException: channel error; protocol method: #method&lt;channel.close&gt;(reply-code=406, reply-text=PRECONDITION_FAILED - inequivalent arg &apos;durable&apos; for queue &apos;zhisheng&apos; in vhost &apos;/&apos;: received &apos;true&apos; but current is &apos;false&apos;, class-id=50, method-id=10)</span><br></pre></td></tr></table></figure><p>这是因为你打开那个注释的话，一旦你运行了该类就会创建一个叫做 <figure class="highlight plain"><figcaption><span>的 Queue，当你再运行 Main 类中的时候，它又会创建这样一个叫 ```zhisheng``` 的 Queue，然后因为已经有同名的 Queue 了，所以就有了冲突，解决方法就是把那行代码注释就好了。</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">3、该 connector（连接器）中提供了 RMQSource 类去消费 RabbitMQ queue 中的消息和确认 checkpoints 上的消息，它提供了三种不一样的保证：</span><br><span class="line"></span><br><span class="line">+ Exactly-once(只消费一次): 前提条件有，1 是要开启 checkpoint，因为只有在 checkpoint 完成后，才会返回确认消息给 RabbitMQ（这时，消息才会在 RabbitMQ 队列中删除)；2 是要使用 Correlation ID，在将消息发往 RabbitMQ 时，必须在消息属性中设置 Correlation ID。数据源根据 Correlation ID 把从 checkpoint 恢复的数据进行去重；3 是数据源不能并行，这种限制主要是由于 RabbitMQ 将消息从单个队列分派给多个消费者。</span><br><span class="line">+ At-least-once(至少消费一次): 开启了 checkpoint，但未使用相 Correlation ID 或 数据源是并行的时候，那么就只能保证数据至少消费一次了</span><br><span class="line">+ No guarantees(无法保证): Flink 接收到数据就返回确认消息给 RabbitMQ</span><br><span class="line"></span><br><span class="line">### Sink 数据到 RabbitMQ</span><br><span class="line"></span><br><span class="line">RabbitMQ 除了可以作为数据源，也可以当作下游，Flink 消费数据做了一些处理之后也能把数据发往 RabbitMQ，下面演示下 Flink 消费 Kafka 数据后写入到 RabbitMQ。</span><br><span class="line"></span><br><span class="line">```java</span><br><span class="line">public class Main1 &#123;</span><br><span class="line">    public static void main(String[] args) throws Exception &#123;</span><br><span class="line">        final ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        final RMQConnectionConfig connectionConfig = new RMQConnectionConfig</span><br><span class="line">                .Builder().setHost(&quot;localhost&quot;).setVirtualHost(&quot;/&quot;)</span><br><span class="line">                .setPort(5672).setUserName(&quot;admin&quot;).setPassword(&quot;admin&quot;)</span><br><span class="line">                .build();</span><br><span class="line"></span><br><span class="line">        //注意，换一个新的 queue，否则也会报错</span><br><span class="line">        data.addSink(new RMQSink&lt;&gt;(connectionConfig, &quot;zhisheng001&quot;, new MetricSchema()));</span><br><span class="line">        env.execute(&quot;flink learning connectors rabbitmq&quot;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>是不是很简单？但是需要注意的是，要换一个之前不存在的 queue，否则是会报错的。</p><p>不出意外的话，你可以看到 RabbitMQ 的监控页面会出现新的一个 queue 出来，如下图：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/d2QROk.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文先把 RabbitMQ 作为数据源，写了个 Flink 消费 RabbitMQ 队列里面的数据进行打印出来，然后又写了个 Flink 消费 Kafka 数据后写入到 RabbitMQ 的例子！</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/pI85H9.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/5WpsW9.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><p>本文的项目代码在 <a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-rabbitmq">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-rabbitmq</a></p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/1tNeFo.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kafka" scheme="http://www.54tianzhisheng.cn/tags/Kafka/"/>
    
      <category term="RabbitMQ" scheme="http://www.54tianzhisheng.cn/tags/RabbitMQ/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/</id>
    <published>2019-01-14T16:00:00.000Z</published>
    <updated>2019-04-24T17:07:48.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/3Cbxab.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前其实在 <a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a> 文章中其实已经写了点将数据写入到 MySQL，但是一些配置化的东西当时是写死的，不能够通用，最近知识星球里有朋友叫我: 写个从 kafka 中读取数据，经过 Flink 做个预聚合，然后创建数据库连接池将数据批量写入到 mysql 的例子。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/XQw9Am.jpg" alt=""></p><p>于是才有了这篇文章，更多提问和想要我写的文章可以在知识星球里像我提问，我会根据提问及时回答和尽可能作出文章的修改。</p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><p>你需要将这两个依赖添加到 pom.xml 中</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>mysql<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>mysql-connector-java<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>5.1.34<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="读取-kafka-数据"><a href="#读取-kafka-数据" class="headerlink" title="读取 kafka 数据"></a>读取 kafka 数据</h3><p>这里我依旧用的以前的 student 类，自己本地起了 kafka 然后造一些测试数据，这里我们测试发送一条数据则 sleep 10s，意味着往 kafka 中一分钟发 6 条数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.connectors.mysql.utils;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhisheng.common.utils.GsonUtil;</span><br><span class="line"><span class="keyword">import</span> com.zhisheng.connectors.mysql.model.Student;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 往kafka中写数据,可以使用这个main函数进行测试</span></span><br><span class="line"><span class="comment"> * Created by zhisheng on 2019-02-17</span></span><br><span class="line"><span class="comment"> * Blog: http://www.54tianzhisheng.cn/tags/Flink/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaUtil</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String broker_list = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"student"</span>;  <span class="comment">//kafka topic 需要和 flink 程序用同一个 topic</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">writeToKafka</span><span class="params">()</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, broker_list);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        KafkaProducer producer = <span class="keyword">new</span> KafkaProducer&lt;String, String&gt;(props);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= <span class="number">100</span>; i++) &#123;</span><br><span class="line">            Student student = <span class="keyword">new</span> Student(i, <span class="string">"zhisheng"</span> + i, <span class="string">"password"</span> + i, <span class="number">18</span> + i);</span><br><span class="line">            ProducerRecord record = <span class="keyword">new</span> ProducerRecord&lt;String, String&gt;(topic, <span class="keyword">null</span>, <span class="keyword">null</span>, GsonUtil.toJson(student));</span><br><span class="line">            producer.send(record);</span><br><span class="line">            System.out.println(<span class="string">"发送数据: "</span> + GsonUtil.toJson(student));</span><br><span class="line">            Thread.sleep(<span class="number">10</span> * <span class="number">1000</span>); <span class="comment">//发送一条数据 sleep 10s，相当于 1 分钟 6 条</span></span><br><span class="line">        &#125;</span><br><span class="line">        producer.flush();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">        writeToKafka();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从 kafka 中读取数据，然后序列化成 student 对象。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">        <span class="string">"student"</span>,   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">        <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">        props)).setParallelism(<span class="number">1</span>)</span><br><span class="line">        .map(string -&gt; GsonUtil.fromJson(string, Student.class)); <span class="comment">//，解析字符串成 student 对象</span></span><br></pre></td></tr></table></figure><p>因为 RichSinkFunction 中如果 sink 一条数据到 mysql 中就会调用 invoke 方法一次，所以如果要实现批量写的话，我们最好在 sink 之前就把数据聚合一下。那这里我们开个一分钟的窗口去聚合 Student 数据。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">student.timeWindowAll(Time.minutes(<span class="number">1</span>)).apply(<span class="keyword">new</span> AllWindowFunction&lt;Student, List&lt;Student&gt;, TimeWindow&gt;() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(TimeWindow window, Iterable&lt;Student&gt; values, Collector&lt;List&lt;Student&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        ArrayList&lt;Student&gt; students = Lists.newArrayList(values);</span><br><span class="line">        <span class="keyword">if</span> (students.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">"1 分钟内收集到 student 的数据条数是："</span> + students.size());</span><br><span class="line">            out.collect(students);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="写入数据库"><a href="#写入数据库" class="headerlink" title="写入数据库"></a>写入数据库</h3><p>这里使用 DBCP 连接池连接数据库 mysql，pom.xml 中添加依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.commons<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>commons-dbcp2<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.1.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>如果你想使用其他的数据库连接池请加入对应的依赖。</p><p>这里将数据写入到 MySQL 中，依旧是和之前文章一样继承 RichSinkFunction 类，重写里面的方法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhisheng.connectors.mysql.sinks;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhisheng.connectors.mysql.model.Student;</span><br><span class="line"><span class="keyword">import</span> org.apache.commons.dbcp2.BasicDataSource;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.configuration.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.flink.streaming.api.functions.sink.RichSinkFunction;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> javax.sql.DataSource;</span><br><span class="line"><span class="keyword">import</span> java.sql.Connection;</span><br><span class="line"><span class="keyword">import</span> java.sql.DriverManager;</span><br><span class="line"><span class="keyword">import</span> java.sql.PreparedStatement;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Desc: 数据批量 sink 数据到 mysql</span></span><br><span class="line"><span class="comment"> * Created by zhisheng_tian on 2019-02-17</span></span><br><span class="line"><span class="comment"> * Blog: http://www.54tianzhisheng.cn/tags/Flink/</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SinkToMySQL</span> <span class="keyword">extends</span> <span class="title">RichSinkFunction</span>&lt;<span class="title">List</span>&lt;<span class="title">Student</span>&gt;&gt; </span>&#123;</span><br><span class="line">    PreparedStatement ps;</span><br><span class="line">    BasicDataSource dataSource;</span><br><span class="line">    <span class="keyword">private</span> Connection connection;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * open() 方法中建立连接，这样不用每次 invoke 的时候都要建立连接和释放连接</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parameters</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">open</span><span class="params">(Configuration parameters)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.open(parameters);</span><br><span class="line">        dataSource = <span class="keyword">new</span> BasicDataSource();</span><br><span class="line">        connection = getConnection(dataSource);</span><br><span class="line">        String sql = <span class="string">"insert into Student(id, name, password, age) values(?, ?, ?, ?);"</span>;</span><br><span class="line">        ps = <span class="keyword">this</span>.connection.prepareStatement(sql);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.close();</span><br><span class="line">        <span class="comment">//关闭连接和释放资源</span></span><br><span class="line">        <span class="keyword">if</span> (connection != <span class="keyword">null</span>) &#123;</span><br><span class="line">            connection.close();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (ps != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ps.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 每条数据的插入都要调用一次 invoke() 方法</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> context</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">invoke</span><span class="params">(List&lt;Student&gt; value, Context context)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//遍历数据集合</span></span><br><span class="line">        <span class="keyword">for</span> (Student student : value) &#123;</span><br><span class="line">            ps.setInt(<span class="number">1</span>, student.getId());</span><br><span class="line">            ps.setString(<span class="number">2</span>, student.getName());</span><br><span class="line">            ps.setString(<span class="number">3</span>, student.getPassword());</span><br><span class="line">            ps.setInt(<span class="number">4</span>, student.getAge());</span><br><span class="line">            ps.addBatch();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">int</span>[] count = ps.executeBatch();<span class="comment">//批量后执行</span></span><br><span class="line">        System.out.println(<span class="string">"成功了插入了"</span> + count.length + <span class="string">"行数据"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Connection <span class="title">getConnection</span><span class="params">(BasicDataSource dataSource)</span> </span>&#123;</span><br><span class="line">        dataSource.setDriverClassName(<span class="string">"com.mysql.jdbc.Driver"</span>);</span><br><span class="line">        <span class="comment">//注意，替换成自己本地的 mysql 数据库地址和用户名、密码</span></span><br><span class="line">        dataSource.setUrl(<span class="string">"jdbc:mysql://localhost:3306/test"</span>);</span><br><span class="line">        dataSource.setUsername(<span class="string">"root"</span>);</span><br><span class="line">        dataSource.setPassword(<span class="string">"root123456"</span>);</span><br><span class="line">        <span class="comment">//设置连接池的一些参数</span></span><br><span class="line">        dataSource.setInitialSize(<span class="number">10</span>);</span><br><span class="line">        dataSource.setMaxTotal(<span class="number">50</span>);</span><br><span class="line">        dataSource.setMinIdle(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">        Connection con = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            con = dataSource.getConnection();</span><br><span class="line">            System.out.println(<span class="string">"创建连接池："</span> + con);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            System.out.println(<span class="string">"-----------mysql get connection has exception , msg = "</span> + e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> con;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="核心类-Main"><a href="#核心类-Main" class="headerlink" title="核心类 Main"></a>核心类 Main</h3><p>核心程序如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"zookeeper.connect"</span>, <span class="string">"localhost:2181"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"metric-group"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line"></span><br><span class="line">        SingleOutputStreamOperator&lt;Student&gt; student = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer011&lt;&gt;(</span><br><span class="line">                <span class="string">"student"</span>,   <span class="comment">//这个 kafka topic 需要和上面的工具类的 topic 一致</span></span><br><span class="line">                <span class="keyword">new</span> SimpleStringSchema(),</span><br><span class="line">                props)).setParallelism(<span class="number">1</span>)</span><br><span class="line">                .map(string -&gt; GsonUtil.fromJson(string, Student.class)); <span class="comment">//</span></span><br><span class="line">        student.timeWindowAll(Time.minutes(<span class="number">1</span>)).apply(<span class="keyword">new</span> AllWindowFunction&lt;Student, List&lt;Student&gt;, TimeWindow&gt;() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">apply</span><span class="params">(TimeWindow window, Iterable&lt;Student&gt; values, Collector&lt;List&lt;Student&gt;&gt; out)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">                ArrayList&lt;Student&gt; students = Lists.newArrayList(values);</span><br><span class="line">                <span class="keyword">if</span> (students.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                    System.out.println(<span class="string">"1 分钟内收集到 student 的数据条数是："</span> + students.size());</span><br><span class="line">                    out.collect(students);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).addSink(<span class="keyword">new</span> SinkToMySQL());</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors kafka"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运行项目"><a href="#运行项目" class="headerlink" title="运行项目"></a>运行项目</h3><p>运行 Main 类后再运行 KafkaUtils.java 类！</p><p>下图是往 Kafka 中发送的数据：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/N5ryLK.jpg" alt=""></p><p>下图是运行 Main 类的日志，会创建 4 个连接池是因为默认的 4 个并行度，你如果在 addSink 这个算子设置并行度为 1 的话就会创建一个连接池：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ffwhNV.jpg" alt=""></p><p>下图是批量插入数据库的结果：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/P7QApT.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文从知识星球一位朋友的疑问来写的，应该都满足了他的条件（批量/数据库连接池/写入mysql），的确网上很多的例子都是简单的 demo 形式，都是单条数据就创建数据库连接插入 MySQL，如果要写的数据量很大的话，会对 MySQL 的写有很大的压力。这也是我之前在 <a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a> 中，数据写 ES 强调过的，如果要提高性能必定要批量的写。就拿我们现在这篇文章来说，如果数据量大的话，聚合一分钟数据达万条，那么这样批量写会比来一条写一条性能提高不知道有多少。</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/vpyqVe.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/JbuVHA.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客。</p><p>本文的项目代码在 <a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-mysql">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-mysql</a></p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/3Cbxab.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="MySQL" scheme="http://www.54tianzhisheng.cn/tags/MySQL/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/</id>
    <published>2019-01-13T16:00:00.000Z</published>
    <updated>2019-04-24T17:17:52.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-24-171741.jpg" alt=""><br><a id="more"></a></p><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之所以写这个是因为前段时间自己的项目出现过这样的一个问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Caused by: akka.pattern.AskTimeoutException: </span><br><span class="line">Ask timed out on [Actor[akka://flink/user/taskmanager_0#15608456]] after [10000 ms]. </span><br><span class="line">Sender[null] sent message of type &quot;org.apache.flink.runtime.rpc.messages.LocalRpcInvocation&quot;.</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/FkaM6A.jpg" alt=""></p><p>跟着这问题在 Flink 的 Issue 列表里看到了一个类似的问题：<a href="https://issues.apache.org/jira/browse/FLINK-9056">https://issues.apache.org/jira/browse/FLINK-9056</a><br>，看下面的评论差不多就是 TaskManager 的 slot 数量不足的原因，导致 job 提交失败。在 Flink 1.63 中已经修复了变成抛出异常了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/p4Tr9Z.jpg" alt=""></p><p>竟然知道了是因为 slot 不足的原因了，那么我们就要先了解下 slot 是什么东东呢？不过文章这里先介绍下 parallelism。</p><h3 id="什么是-parallelism？"><a href="#什么是-parallelism？" class="headerlink" title="什么是 parallelism？"></a>什么是 parallelism？</h3><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/FaZUcj.jpg" alt=""></p><p>如翻译这样，parallelism 是并行的意思，在 Flink 里面代表每个任务的并行度，适当的提高并行度可以大大提高 job 的执行效率，比如你的 job 消费 kafka 数据过慢，适当调大可能就消费正常了。</p><p>那么在 Flink 中怎么设置并行度呢？</p><h3 id="如何设置-parallelism？"><a href="#如何设置-parallelism？" class="headerlink" title="如何设置 parallelism？"></a>如何设置 parallelism？</h3><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/3DvvDE.jpg" alt=""></p><p>如上图，在 flink 配置文件中可以查看到默认并行度是 1，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cat flink-conf.yaml | grep parallelism</span><br><span class="line"></span><br><span class="line"># The parallelism used for programs that did not specify and other parallelism.</span><br><span class="line">parallelism.default: 1</span><br></pre></td></tr></table></figure><p>所以你如何在你的 flink job 里面不设置任何的 parallelism 的话，那么他也会有一个默认的 parallelism = 1。那也意味着你可以修改这个配置文件的默认并行度。</p><p>如果你是用命令行启动你的 Flink job，那么你也可以这样设置并行度(使用 -p 并行度)：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./bin/flink run -p 10 ../word-count.jar</span><br></pre></td></tr></table></figure><p>你也可以通过这样来设置你整个程序的并行度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line">env.setParallelism(10);</span><br></pre></td></tr></table></figure><p>注意：这样设置的并行度是你整个程序的并行度，那么后面如果你的每个算子不单独设置并行度覆盖的话，那么后面每个算子的并行度就都是这里设置的并行度的值了。</p><p>如何给每个算子单独设置并行度呢？</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="keyword">new</span> xxxKey())</span><br><span class="line">    .flatMap(<span class="keyword">new</span> XxxFlatMapFunction()).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .map(<span class="keyword">new</span> XxxMapFunction).setParallelism(<span class="number">5</span>)</span><br><span class="line">    .addSink(<span class="keyword">new</span> XxxSink()).setParallelism(<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>如上，就是在每个算子后面单独的设置并行度，这样的话，就算你前面设置了 env.setParallelism(10) 也是会被覆盖的。</p><p>这也说明优先级是：算子设置并行度 &gt; env 设置并行度 &gt; 配置文件默认并行度</p><p>并行度讲到这里应该都懂了，下面 zhisheng 就继续跟你讲讲 什么是 slot？</p><h3 id="什么是-slot？"><a href="#什么是-slot？" class="headerlink" title="什么是 slot？"></a>什么是 slot？</h3><p>其实什么是 slot 这个问题之前在第一篇文章 <a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a> 中就介绍过了，这里再讲细一点。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/r19yJh.jpg" alt=""></p><p>图中 Task Manager 是从 Job Manager 处接收需要部署的 Task，任务的并行性由每个 Task Manager 上可用的 slot 决定。每个任务代表分配给任务槽的一组资源，slot 在 Flink 里面可以认为是资源组，Flink 将每个任务分成子任务并且将这些子任务分配到 slot 来并行执行程序。</p><p>例如，如果 Task Manager 有四个 slot，那么它将为每个 slot 分配 25％ 的内存。 可以在一个 slot 中运行一个或多个线程。 同一 slot 中的线程共享相同的 JVM。 同一 JVM 中的任务共享 TCP 连接和心跳消息。Task Manager 的一个 Slot 代表一个可用线程，该线程具有固定的内存，注意 Slot 只对内存隔离，没有对 CPU 隔离。默认情况下，Flink 允许子任务共享 Slot，即使它们是不同 task 的 subtask，只要它们来自相同的 job。这种共享可以有更好的资源利用率。</p><p>文字说的比较干，zhisheng 这里我就拿下面的图片来讲解：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ECv5y2.jpg" alt=""></p><p>上面图片中有两个 Task Manager，每个 Task Manager 有三个 slot，这样我们的算子最大并行度那么就可以达到 6 个，在同一个 slot 里面可以执行 1 至多个子任务。</p><p>那么再看上面的图片，source/map/keyby/window/apply 最大可以有 6 个并行度，sink 只用了 1 个并行。</p><p>每个 Flink TaskManager 在集群中提供 slot。 slot 的数量通常与每个 TaskManager 的可用 CPU 内核数成比例。一般情况下你的 slot 数是你每个 TaskManager 的 cpu 的核数。</p><p>但是 flink 配置文件中设置的 task manager 默认的 slot 是 1。 </p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/VBxaDH.jpg" alt=""></p><h3 id="slot-和-parallelism"><a href="#slot-和-parallelism" class="headerlink" title="slot 和 parallelism"></a>slot 和 parallelism</h3><p>下面给出官方的图片来更加深刻的理解下 slot：</p><p>1、slot 是指 taskmanager 的并发执行能力</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/zpX2sh.jpg" alt=""></p><p>taskmanager.numberOfTaskSlots:3</p><p>每一个 taskmanager 中的分配 3 个 TaskSlot, 3 个 taskmanager 一共有 9 个 TaskSlot。</p><p>2、parallelism 是指 taskmanager 实际使用的并发能力</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/npq4kW.jpg" alt=""></p><p>parallelism.default:1 </p><p>运行程序默认的并行度为 1，9 个 TaskSlot 只用了 1 个，有 8 个空闲。设置合适的并行度才能提高效率。</p><p>3、parallelism 是可配置、可指定的</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/xAuHJn.jpg" alt=""></p><p>上图中 example2 每个算子设置的并行度是 2， example3 每个算子设置的并行度是 9。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/syrCLs.jpg" alt=""></p><p>example4 除了 sink 是设置的并行度为 1，其他算子设置的并行度都是 9。</p><p>好了，既然并行度和 slot zhisheng 都带大家过了一遍了，那么再来看文章开头的问题：slot 资源不够。</p><h3 id="问题原因"><a href="#问题原因" class="headerlink" title="问题原因"></a>问题原因</h3><p>现在这个问题的答案其实就已经很明显了，就是我们设置的并行度 parallelism 超过了 Task Manager 能提供的最大 slot 数量，所以才会报这个错误。</p><p>再来拿我的代码来看吧，当时我就是只设置了整个项目的并行度：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">env.setParallelism(15);</span><br></pre></td></tr></table></figure><p>为什么要设置 15 呢，因为我项目消费的 Kafka topic 有 15 个 parttion，就想着让一个并行去消费一个 parttion，没曾想到 Flink 资源的不够，稍微降低下 并行度为 10 后就没出现这个错误了。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文由自己项目生产环境的一个问题来讲解了自己对 Flink parallelism 和 slot 的理解，并告诉大家如何去设置这两个参数，最后也指出了问题的原因所在。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/</a> , 未经允许禁止转载。</p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Rtt4TP.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/mb4tJ7.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;http://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/img/2019-04-24-171741.jpg&quot; alt=&quot;&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink JobManager 高可用性配置</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/</id>
    <published>2019-01-12T16:00:00.000Z</published>
    <updated>2019-04-24T17:07:08.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/duhRAK.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前在 <a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a> 讲过 Flink 的配置，但是后面陆续有人来问我一些配置相关的东西，在加上我现在对 Flink 也更熟悉了些，这里我就再写下 Flink JobManager 的配置相关信息。</p><p>在 <a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a> 一文中介绍过了 Flink Job 的运行架构图：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/svX7Ft.jpg" alt=""></p><p>JobManager 协调每个 Flink 作业的部署。它负责调度和资源管理。</p><p>默认情况下，每个 Flink 集群都有一个 JobManager 实例。这会产生单点故障（SPOF）：如果 JobManager 崩溃，则无法提交新作业且运行中的作业也会失败。</p><p>如果我们使用 JobManager 高可用模式，可以避免这个问题。您可以为 standalone 集群和 YARN 集群配置高可用模式。</p><h2 id="standalone-集群高可用性"><a href="#standalone-集群高可用性" class="headerlink" title="standalone 集群高可用性"></a>standalone 集群高可用性</h2><p>standalone 集群的 JobManager 高可用性的概念是，任何时候都有一个主 JobManager 和 多个备 JobManagers，以便在主节点失败时有新的 JobNamager 接管集群。这样就保证了没有单点故障，一旦备 JobManager 接管集群，作业就可以依旧正常运行。主备 JobManager 实例之间没有明确的区别。每个 JobManager 都可以充当主备节点。</p><p>例如，请考虑以下三个 JobManager 实例的设置：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/lsiuRC.jpg" alt=""></p><h3 id="如何配置"><a href="#如何配置" class="headerlink" title="如何配置"></a>如何配置</h3><p>要启用 JobManager 高可用性功能，您必须将高可用性模式设置为 zookeeper，配置 ZooKeeper quorum，将所有 JobManagers 主机及其 Web UI 端口写入配置文件。</p><p>Flink 利用 ZooKeeper 在所有正在运行的 JobManager 实例之间进行分布式协调。ZooKeeper 是独立于 Flink 的服务，通过 leader 选举和轻量级一致性状态存储提供高可靠的分布式协调服务。Flink 包含用于 Bootstrap ZooKeeper 安装的脚本。<br>他在我们的 Flink 安装路径下面 /conf/zoo.cfg 。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ImJPIC.jpg" alt=""></p><h3 id="Masters-文件"><a href="#Masters-文件" class="headerlink" title="Masters 文件"></a>Masters 文件</h3><p>要启动 HA 集群，请在以下位置配置 Masters 文件 conf/masters：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">localhost:8081</span><br><span class="line">xxx.xxx.xxx.xxx:8081</span><br></pre></td></tr></table></figure><p>masters 文件包含启动 JobManagers 的所有主机以及 Web 用户界面绑定的端口，上面一行写一个。</p><p>默认情况下，job manager 选一个随机端口作为进程通信端口。您可以通过 high-availability.jobmanager.port 更改此设置。此配置接受单个端口（例如 50010），范围（50000-50025）或两者的组合（50010,50011,50020-50025,50050-50075）。</p><h3 id="配置文件-flink-conf-yaml"><a href="#配置文件-flink-conf-yaml" class="headerlink" title="配置文件 (flink-conf.yaml)"></a>配置文件 (flink-conf.yaml)</h3><p>要启动 HA 集群，请将以下配置键添加到 conf/flink-conf.yaml：</p><p>高可用性模式（必需）：在 conf/flink-conf.yaml中，必须将高可用性模式设置为 zookeeper，以打开高可用模式。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">high-availability: zookeeper</span><br></pre></td></tr></table></figure><p>ZooKeeper quorum（必需）：ZooKeeper quorum 是一组 ZooKeeper 服务器，它提供分布式协调服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">high-availability.zookeeper.quorum: ip1:2181 [,...],ip2:2181</span><br></pre></td></tr></table></figure><p>每个 ip:port 都是一个 ZooKeeper 服务器的 ip 及其端口，Flink 可以通过指定的地址和端口访问 zookeeper。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/UfuoZZ.jpg" alt=""></p><p>另外就是高可用存储目录，JobManager 元数据保存在文件系统 storageDir 中，在 ZooKeeper 中仅保存了指向此状态的指针, 推荐这个目录是 HDFS, S3, Ceph, nfs 等，该 storageDir 中保存了 JobManager 恢复状态需要的所有元数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">high-availability.storageDir: hdfs:///flink/ha/</span><br></pre></td></tr></table></figure><p>配置 master 文件和 ZooKeeper 配置后，您可以使用提供的集群启动脚本。他们将启动 HA 集群。请注意，启动 Flink HA 集群前，必须启动 Zookeeper 集群，并确保为要启动的每个 HA 集群配置单独的 ZooKeeper 根路径。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>具有 2 个 JobManagers 的 Standalone 集群：</p><p>1、在 conf/flink-conf.yaml 中配置高可用模式和 Zookeeper :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.zookeeper.quorum: localhost:2181</span><br><span class="line">high-availability.storageDir: hdfs:///flink/recovery</span><br></pre></td></tr></table></figure><p>2、在 conf/masters 中 配置 masters:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">localhost:8081</span><br><span class="line">localhost:8082</span><br></pre></td></tr></table></figure><p>3、在 conf/zoo.cfg 中配置 Zookeeper 服务:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server.0=localhost:2888:3888</span><br></pre></td></tr></table></figure><p>4、启动 ZooKeeper 集群:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-zookeeper-quorum.sh</span><br><span class="line">Starting zookeeper daemon on host localhost.</span><br></pre></td></tr></table></figure><p>5、启动一个 Flink HA 集群:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-cluster.sh</span><br><span class="line">Starting HA cluster with 2 masters and 1 peers in ZooKeeper quorum.</span><br><span class="line">Starting jobmanager daemon on host localhost.</span><br><span class="line">Starting jobmanager daemon on host localhost.</span><br><span class="line">Starting taskmanager daemon on host localhost.</span><br></pre></td></tr></table></figure><p>6、停止 ZooKeeper 和集群:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ bin/stop-cluster.sh</span><br><span class="line">Stopping taskmanager daemon (pid: 7647) on localhost.</span><br><span class="line">Stopping jobmanager daemon (pid: 7495) on host localhost.</span><br><span class="line">Stopping jobmanager daemon (pid: 7349) on host localhost.</span><br><span class="line"></span><br><span class="line">$ bin/stop-zookeeper-quorum.sh</span><br><span class="line">Stopping zookeeper daemon (pid: 7101) on host localhost.</span><br></pre></td></tr></table></figure><p>上面的执行脚本如下图可见：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/th7tmL.jpg" alt=""></p><h2 id="YARN-集群高可用性"><a href="#YARN-集群高可用性" class="headerlink" title="YARN 集群高可用性"></a>YARN 集群高可用性</h2><p>当运行高可用的 YARN 集群时，我们不会运行多个 JobManager 实例，而只会运行一个，该 JobManager 实例失败时，YARN 会将其重新启动。Yarn 的具体行为取决于您使用的 YARN 版本。</p><h3 id="如何配置？"><a href="#如何配置？" class="headerlink" title="如何配置？"></a>如何配置？</h3><h3 id="Application-Master-最大重试次数-yarn-site-xml"><a href="#Application-Master-最大重试次数-yarn-site-xml" class="headerlink" title="Application Master 最大重试次数 (yarn-site.xml)"></a>Application Master 最大重试次数 (yarn-site.xml)</h3><p>在 YARN 配置文件 yarn-site.xml 中，需要配置 application master 的最大重试次数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;4&lt;/value&gt;</span><br><span class="line">  &lt;description&gt;</span><br><span class="line">    The maximum number of application master execution attempts.</span><br><span class="line">  &lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>当前 YARN 版本的默认值为 2（表示允许单个 JobManager 失败两次）。</p><h3 id="Application-Attempts-flink-conf-yaml"><a href="#Application-Attempts-flink-conf-yaml" class="headerlink" title="Application Attempts (flink-conf.yaml)"></a>Application Attempts (flink-conf.yaml)</h3><p>除了上面可以配置最大重试次数外，你还可以在 flink-conf.yaml 配置如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn.application-attempts: 10</span><br></pre></td></tr></table></figure><p>这意味着在如果程序启动失败，YARN 会再重试 9 次（9 次重试 + 1 次启动），如果启动 10 次作业还失败，yarn 才会将该任务的状态置为失败。如果因为节点硬件故障或重启，NodeManager 重新同步等操作，需要 YARN 继续尝试启动应用。这些重启尝试不计入 yarn.application-attempts 个数中。</p><h3 id="容器关闭行为"><a href="#容器关闭行为" class="headerlink" title="容器关闭行为"></a>容器关闭行为</h3><ul><li>YARN 2.3.0 &lt; 版本 &lt; 2.4.0. 如果 application master 进程失败，则所有的 container 都会重启。</li><li>YARN 2.4.0 &lt; 版本 &lt; 2.6.0. TaskManager container 在 application master 故障期间，会继续工作。这具有以下优点：作业恢复时间更快，且缩短所有 task manager 启动时申请资源的时间。</li><li>YARN 2.6.0 &lt;= version: 将尝试失败有效性间隔设置为 Flink 的 Akka 超时值。尝试失败有效性间隔表示只有在系统在一个间隔期间看到最大应用程序尝试次数后才会终止应用程序。这避免了持久的工作会耗尽它的应用程序尝试。</li></ul><h3 id="示例：高可用的-YARN-Session"><a href="#示例：高可用的-YARN-Session" class="headerlink" title="示例：高可用的 YARN Session"></a>示例：高可用的 YARN Session</h3><p>1、配置 HA 模式和 Zookeeper 集群 在 conf/flink-conf.yaml:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">high-availability: zookeeper</span><br><span class="line">high-availability.zookeeper.quorum: localhost:2181</span><br><span class="line">yarn.application-attempts: 10</span><br></pre></td></tr></table></figure></p><p>2、配置 ZooKeeper 服务 在 conf/zoo.cfg：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">server.0=localhost:2888:3888</span><br></pre></td></tr></table></figure></p><p>3、启动 Zookeeper 集群:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/start-zookeeper-quorum.sh</span><br><span class="line">Starting zookeeper daemon on host localhost.</span><br></pre></td></tr></table></figure></p><p>4、启动 HA 集群:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/yarn-session.sh -n 2</span><br></pre></td></tr></table></figure></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章再次写了下 Flink JobManager 的高可用配置，如何在 standalone 集群和 YARN 集群中配置高可用。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/</a> , 未经允许禁止转载。</p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/fMnpK6.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/VYEi11.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/duhRAK.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 写入数据到 Kafka</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/</id>
    <published>2019-01-05T16:00:00.000Z</published>
    <updated>2019-04-24T17:08:02.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/XYl9Ug.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前文章 <a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a> 写了如何将 Kafka 中的数据存储到 ElasticSearch 中，里面其实就已经用到了 Flink 自带的 Kafka source connector（FlinkKafkaConsumer）。存入到 ES 只是其中一种情况，那么如果我们有多个地方需要这份通过 Flink 转换后的数据，是不是又要我们继续写个 sink 的插件呢？确实，所以 Flink 里面就默认支持了不少 sink，比如也支持 Kafka sink connector（FlinkKafkaProducer），那么这篇文章我们就讲讲如何将数据写入到 Kafka。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/cRMkpP.jpg" alt=""></p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><p>Flink 里面支持 Kafka 0.8、0.9、0.10、0.11 ，以后有时间可以分析下源码的实现。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/NWXmgY.jpg" alt=""></p><p>这里我们需要安装下 Kafka，请对应添加对应的 Flink Kafka connector 依赖的版本，这里我们使用的是 0.11 版本：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-kafka-0.11_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="Kafka-安装"><a href="#Kafka-安装" class="headerlink" title="Kafka 安装"></a>Kafka 安装</h4><p>这里就不写这块内容了，可以参考我以前的文章 <a href="http://www.54tianzhisheng.cn/2018/01/04/Kafka/">Kafka 安装及快速入门</a>。</p><p>这里我们演示把其他 Kafka 集群中 topic 数据原样写入到自己本地起的 Kafka 中去。</p><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kafka.brokers=xxx:9092,xxx:9092,xxx:9092</span><br><span class="line">kafka.group.id=metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=xxx:2181</span><br><span class="line">metrics.topic=xxx</span><br><span class="line">stream.parallelism=5</span><br><span class="line">kafka.sink.brokers=localhost:9092</span><br><span class="line">kafka.sink.topic=metric-test</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure><p>目前我们先看下本地 Kafka 是否有这个 metric-test topic 呢？需要执行下这个命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --list --zookeeper localhost:2181</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/6KFHKT.jpg" alt=""></p><p>可以看到本地的 Kafka 是没有任何 topic 的，如果等下我们的程序运行起来后，再次执行这个命令出现 metric-test topic，那么证明我的程序确实起作用了，已经将其他集群的 Kafka 数据写入到本地 Kafka 了。</p><h4 id="程序代码"><a href="#程序代码" class="headerlink" title="程序代码"></a>程序代码</h4><p>Main.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        data.addSink(<span class="keyword">new</span> FlinkKafkaProducer011&lt;Metrics&gt;(</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.brokers"</span>),</span><br><span class="line">                parameterTool.get(<span class="string">"kafka.sink.topic"</span>),</span><br><span class="line">                <span class="keyword">new</span> MetricSchema()</span><br><span class="line">                )).name(<span class="string">"flink-connectors-kafka"</span>)</span><br><span class="line">                .setParallelism(parameterTool.getInt(<span class="string">"stream.sink.parallelism"</span>));</span><br><span class="line"></span><br><span class="line">        env.execute(<span class="string">"flink learning connectors kafka"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><p>启动程序，查看运行结果，不段执行上面命令，查看是否有新的 topic 出来：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/nxqZmZ.jpg" alt=""></p><p>执行命令可以查看该 topic 的信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic metric-test</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/y5vPRR.jpg" alt=""></p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>上面代码我们使用 Flink Kafka Producer 只传了三个参数：brokerList、topicId、serializationSchema（序列化）</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/KN766D.jpg" alt=""></p><p>其实也可以传入多个参数进去，现在有的参数用的是默认参数，因为这个内容比较多，后面可以抽出一篇文章单独来讲。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章写了 Flink 读取其他 Kafka 集群的数据，然后写入到本地的 Kafka 上。我在 Flink 这层没做什么数据转换，只是原样的将数据转发了下，如果你们有什么其他的需求，是可以在 Flink 这层将数据进行各种转换操作，比如这篇文章中的一些转换：<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a>，然后将转换后的数据发到 Kafka 上去。</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/5AtPqZ.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/iaNuzk.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/XYl9Ug.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Kafka" scheme="http://www.54tianzhisheng.cn/tags/Kafka/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 项目如何运行？</title>
    <link href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/"/>
    <id>http://www.54tianzhisheng.cn/2019/01/05/Flink-run/</id>
    <published>2019-01-04T16:00:00.000Z</published>
    <updated>2019-04-24T17:07:26.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/HXmHJo.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>之前写了不少 Flink 文章了，也有不少 demo，但是文章写的时候都是在本地直接运行 Main 类的 main 方法，其实 Flink 是支持在 UI 上上传 Flink Job 的 jar 包，然后运行得。最开始在第一篇 <a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install/">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a> 中其实提到过了 Flink 自带的 UI 界面，今天我们就来看看如何将我们的项目打包在这里发布运行。</p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><h4 id="编译打包"><a href="#编译打包" class="headerlink" title="编译打包"></a>编译打包</h4><p>项目代码就拿我之前的文章 <a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a> 吧，代码地址是在 GitHub 仓库地址：<a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6</a> ，如果感兴趣的可以直接拿来打包试试水。</p><p>我们在整个项目 （flink-learning）pom.xml 所在文件夹执行以下命令打包：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn clean install</span><br></pre></td></tr></table></figure><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ap2P8l.jpg" alt=""></p><p>然后你会发现在 flink-learning-connectors-es6 的 target 目录下有 flink-learning-connectors-es6-1.0-SNAPSHOT.jar 。</p><h4 id="启动-ES"><a href="#启动-ES" class="headerlink" title="启动 ES"></a>启动 ES</h4><p>注意你的 Kafka 数据源和 ES 都已经启动好了, 清空了下 ES 目录下的 data 数据，为了就是查看是不是真的有数据存入进来了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/lmspd0.jpg" alt=""></p><h4 id="提交-jar-包"><a href="#提交-jar-包" class="headerlink" title="提交 jar 包"></a>提交 jar 包</h4><p>将此文件提交到 Flinkserver 上，如下图：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/O9hKsX.jpg" alt=""></p><p>点击下图红框中的”Upload”按钮：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/lfsYoh.jpg" alt=""></p><p>如下图，选中刚刚上传的文件，填写类名，再点击”Submit”按钮即可启动 Job：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/mhpBAN.jpg" alt=""></p><h3 id="查看运行结果"><a href="#查看运行结果" class="headerlink" title="查看运行结果"></a>查看运行结果</h3><p>如下图，在 Overview 页面可见正在运行的任务：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/vS3hIG.jpg" alt=""></p><p>你可以看到 Task Manager 中关于任务的 metric 数据<br>、日志信息以及 Stdout 信息。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/krIFlu.jpg" alt=""></p><p>查看 Kibana ，此时 ES 中已经有数据了：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/BSvuej.jpg" alt=""></p><p>我们可以在 flink ui 界面上的 overview cancel 这个 job，那么可以看到 job 的日志：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/oH0rF4.jpg" alt=""></p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/xXLOdJ.jpg" alt=""></p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本篇文章写了下如何将我们的 job 编译打包并提交到 Flink 自带到 Server UI 上面去运行，也算是对前面文章的一个补充，当然了，Flink job 不仅支持这种模式的运行，它还可以运行在 K8s，Mesos，等上面，等以后我接触到再写写。</p><p>本文原创地址是: <a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">http://www.54tianzhisheng.cn/2019/01/05/Flink-run/</a> , 未经允许禁止转载。</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/uz6cl1.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/OEhLqC.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/HXmHJo.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</title>
    <link href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/"/>
    <id>http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/</id>
    <published>2018-12-29T16:00:00.000Z</published>
    <updated>2019-04-24T17:10:14.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/9qJpA4.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>前面 FLink 的文章中我们已经介绍了说 Flink 已经有很多自带的 Connector。</p><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>其中包括了 Source 和 Sink 的，后面我也讲了下如何自定义自己的 Source 和 Sink。</p><p>那么今天要做的事情是啥呢？就是介绍一下 Flink 自带的 ElasticSearch Connector，我们今天就用他来做 Sink，将 Kafka 中的数据经过 Flink 处理后然后存储到 ElasticSearch。</p><h3 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h3><p>安装 ElasticSearch，这里就忽略，自己找我以前的文章，建议安装 ElasticSearch 6.0 版本以上的，毕竟要跟上时代的节奏。</p><p>下面就讲解一下生产环境中如何使用 Elasticsearch Sink 以及一些注意点，及其内部实现机制。</p><h3 id="Elasticsearch-Sink"><a href="#Elasticsearch-Sink" class="headerlink" title="Elasticsearch Sink"></a>Elasticsearch Sink</h3><h4 id="添加依赖"><a href="#添加依赖" class="headerlink" title="添加依赖"></a>添加依赖</h4><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.flink<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>flink-connector-elasticsearch6_$&#123;scala.binary.version&#125;<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>$&#123;flink.version&#125;<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上面这依赖版本号请自己根据使用的版本对应改变下。</p><p>下面所有的代码都没有把 import 引入到这里来，如果需要查看更详细的代码，请查看我的 GitHub 仓库地址：</p><p><a href="https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6">https://github.com/zhisheng17/flink-learning/tree/master/flink-learning-connectors/flink-learning-connectors-es6</a></p><p>这个 module 含有本文的所有代码实现，当然越写到后面自己可能会做一些抽象，所以如果有代码改变很正常，请直接查看全部项目代码。</p><h4 id="ElasticSearchSinkUtil-工具类"><a href="#ElasticSearchSinkUtil-工具类" class="headerlink" title="ElasticSearchSinkUtil 工具类"></a>ElasticSearchSinkUtil 工具类</h4><p>这个工具类是自己封装的，getEsAddresses 方法将传入的配置文件 es 地址解析出来，可以是域名方式，也可以是 ip + port 形式。addSink 方法是利用了 Flink 自带的 ElasticsearchSink 来封装了一层，传入了一些必要的调优参数和 es 配置参数，下面文章还会再讲些其他的配置。</p><p>ElasticSearchSinkUtil.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ElasticSearchSinkUtil</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * es sink</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts es hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> bulkFlushMaxActions bulk flush size</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> parallelism 并行数</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> data 数据</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> func</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> &lt;T&gt;</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> &lt;T&gt; <span class="function"><span class="keyword">void</span> <span class="title">addSink</span><span class="params">(List&lt;HttpHost&gt; hosts, <span class="keyword">int</span> bulkFlushMaxActions, <span class="keyword">int</span> parallelism,</span></span></span><br><span class="line"><span class="function"><span class="params">                                   SingleOutputStreamOperator&lt;T&gt; data, ElasticsearchSinkFunction&lt;T&gt; func)</span> </span>&#123;</span><br><span class="line">        ElasticsearchSink.Builder&lt;T&gt; esSinkBuilder = <span class="keyword">new</span> ElasticsearchSink.Builder&lt;&gt;(hosts, func);</span><br><span class="line">        esSinkBuilder.setBulkFlushMaxActions(bulkFlushMaxActions);</span><br><span class="line">        data.addSink(esSinkBuilder.build()).setParallelism(parallelism);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 解析配置文件的 es hosts</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> hosts</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span></span></span><br><span class="line"><span class="comment">     * <span class="doctag">@throws</span> MalformedURLException</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;HttpHost&gt; <span class="title">getEsAddresses</span><span class="params">(String hosts)</span> <span class="keyword">throws</span> MalformedURLException </span>&#123;</span><br><span class="line">        String[] hostList = hosts.split(<span class="string">","</span>);</span><br><span class="line">        List&lt;HttpHost&gt; addresses = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">        <span class="keyword">for</span> (String host : hostList) &#123;</span><br><span class="line">            <span class="keyword">if</span> (host.startsWith(<span class="string">"http"</span>)) &#123;</span><br><span class="line">                URL url = <span class="keyword">new</span> URL(host);</span><br><span class="line">                addresses.add(<span class="keyword">new</span> HttpHost(url.getHost(), url.getPort()));</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                String[] parts = host.split(<span class="string">":"</span>, <span class="number">2</span>);</span><br><span class="line">                <span class="keyword">if</span> (parts.length &gt; <span class="number">1</span>) &#123;</span><br><span class="line">                    addresses.add(<span class="keyword">new</span> HttpHost(parts[<span class="number">0</span>], Integer.parseInt(parts[<span class="number">1</span>])));</span><br><span class="line">                &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                    <span class="keyword">throw</span> <span class="keyword">new</span> MalformedURLException(<span class="string">"invalid elasticsearch hosts format"</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> addresses;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Main-启动类"><a href="#Main-启动类" class="headerlink" title="Main 启动类"></a>Main 启动类</h4><p>Main.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//获取所有参数</span></span><br><span class="line">        <span class="keyword">final</span> ParameterTool parameterTool = ExecutionEnvUtil.createParameterTool(args);</span><br><span class="line">        <span class="comment">//准备好环境</span></span><br><span class="line">        StreamExecutionEnvironment env = ExecutionEnvUtil.prepare(parameterTool);</span><br><span class="line">        <span class="comment">//从kafka读取数据</span></span><br><span class="line">        DataStreamSource&lt;Metrics&gt; data = KafkaConfigUtil.buildSource(env);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//从配置文件中读取 es 的地址</span></span><br><span class="line">        List&lt;HttpHost&gt; esAddresses = ElasticSearchSinkUtil.getEsAddresses(parameterTool.get(ELASTICSEARCH_HOSTS));</span><br><span class="line">        <span class="comment">//从配置文件中读取 bulk flush size，代表一次批处理的数量，这个可是性能调优参数，特别提醒</span></span><br><span class="line">        <span class="keyword">int</span> bulkSize = parameterTool.getInt(ELASTICSEARCH_BULK_FLUSH_MAX_ACTIONS, <span class="number">40</span>);</span><br><span class="line">        <span class="comment">//从配置文件中读取并行 sink 数，这个也是性能调优参数，特别提醒，这样才能够更快的消费，防止 kafka 数据堆积</span></span><br><span class="line">        <span class="keyword">int</span> sinkParallelism = parameterTool.getInt(STREAM_SINK_PARALLELISM, <span class="number">5</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//自己再自带的 es sink 上一层封装了下</span></span><br><span class="line">        ElasticSearchSinkUtil.addSink(esAddresses, bulkSize, sinkParallelism, data,</span><br><span class="line">                (Metrics metric, RuntimeContext runtimeContext, RequestIndexer requestIndexer) -&gt; &#123;</span><br><span class="line">                    requestIndexer.add(Requests.indexRequest()</span><br><span class="line">                            .index(ZHISHENG + <span class="string">"_"</span> + metric.getName())  <span class="comment">//es 索引名</span></span><br><span class="line">                            .type(ZHISHENG) <span class="comment">//es type</span></span><br><span class="line">                            .source(GsonUtil.toJSONBytes(metric), XContentType.JSON)); </span><br><span class="line">                &#125;);</span><br><span class="line">        env.execute(<span class="string">"flink learning connectors es6"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><p>配置都支持集群模式填写，注意用 , 分隔！</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">kafka.brokers=localhost:9092</span><br><span class="line">kafka.group.id=zhisheng-metrics-group-test</span><br><span class="line">kafka.zookeeper.connect=localhost:2181</span><br><span class="line">metrics.topic=zhisheng-metrics</span><br><span class="line">stream.parallelism=5</span><br><span class="line">stream.checkpoint.interval=1000</span><br><span class="line">stream.checkpoint.enable=false</span><br><span class="line">elasticsearch.hosts=localhost:9200</span><br><span class="line">elasticsearch.bulk.flush.max.actions=40</span><br><span class="line">stream.sink.parallelism=5</span><br></pre></td></tr></table></figure><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><p>执行 Main 类的 main 方法，我们的程序是只打印 flink 的日志，没有打印存入的日志（因为我们这里没有打日志）：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Df25KG.jpg" alt=""></p><p>所以看起来不知道我们的 sink 是否有用，数据是否从 kafka 读取出来后存入到 es 了。</p><p>你可以查看下本地起的 es 终端或者服务器的 es 日志就可以看到效果了。</p><p>es 日志如下：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/F62ZpP.jpg" alt=""></p><p>上图是我本地 Mac 电脑终端的 es 日志，可以看到我们的索引了。</p><p>如果还不放心，你也可以在你的电脑装个 kibana，然后更加的直观查看下 es 的索引情况（或者直接敲 es 的命令）</p><p>我们用 kibana 查看存入 es 的索引如下：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/nynNxR.jpg" alt=""></p><p>程序执行了一会，存入 es 的数据量就很大了。</p><h3 id="扩展配置"><a href="#扩展配置" class="headerlink" title="扩展配置"></a>扩展配置</h3><p>上面代码已经可以实现你的大部分场景了，但是如果你的业务场景需要保证数据的完整性（不能出现丢数据的情况），那么就需要添加一些重试策略，因为在我们的生产环境中，很有可能会因为某些组件不稳定性导致各种问题，所以这里我们就要在数据存入失败的时候做重试操作，这里 flink 自带的 es sink 就支持了，常用的失败重试配置有:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1、bulk.flush.backoff.enable 用来表示是否开启重试机制</span><br><span class="line"></span><br><span class="line">2、bulk.flush.backoff.type 重试策略，有两种：EXPONENTIAL 指数型（表示多次重试之间的时间间隔按照指数方式进行增长）、CONSTANT 常数型（表示多次重试之间的时间间隔为固定常数）</span><br><span class="line"></span><br><span class="line">3、bulk.flush.backoff.delay 进行重试的时间间隔</span><br><span class="line"></span><br><span class="line">4、bulk.flush.backoff.retries 失败重试的次数</span><br><span class="line"></span><br><span class="line">5、bulk.flush.max.actions: 批量写入时的最大写入条数</span><br><span class="line"></span><br><span class="line">6、bulk.flush.max.size.mb: 批量写入时的最大数据量</span><br><span class="line"></span><br><span class="line">7、bulk.flush.interval.ms: 批量写入的时间间隔，配置后则会按照该时间间隔严格执行，无视上面的两个批量写入配置</span><br></pre></td></tr></table></figure><p>看下啦，就是如下这些配置了，如果你需要的话，可以在这个地方配置扩充了。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/uLW2oW.jpg" alt=""></p><h3 id="FailureHandler-失败处理器"><a href="#FailureHandler-失败处理器" class="headerlink" title="FailureHandler 失败处理器"></a>FailureHandler 失败处理器</h3><p>写入 ES 的时候会有这些情况会导致写入 ES 失败：</p><p>1、ES 集群队列满了，报如下错误</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">12:08:07.326 [I/O dispatcher 13] ERROR o.a.f.s.c.e.ElasticsearchSinkBase - Failed Elasticsearch item request: ElasticsearchException[Elasticsearch exception [type=es_rejected_execution_exception, reason=rejected execution of org.elasticsearch.transport.TransportService$7@566c9379 on EsThreadPoolExecutor[name = node-1/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@f00b373[Running, pool size = 4, active threads = 4, queued tasks = 200, completed tasks = 6277]]]]</span><br></pre></td></tr></table></figure><p>是这样的，我电脑安装的 es 队列容量默认应该是 200，我没有修改过。我这里如果配置的 bulk flush size * 并发 sink 数量 这个值如果大于这个 queue capacity ，那么就很容易导致出现这种因为 es 队列满了而写入失败。</p><p>当然这里你也可以通过调大点 es 的队列。参考：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-threadpool.html</a></p><p>2、ES 集群某个节点挂了</p><p>这个就不用说了，肯定写入失败的。跟过源码可以发现 RestClient 类里的 performRequestAsync 方法一开始会随机的从集群中的某个节点进行写入数据，如果这台机器掉线，会进行重试在其他的机器上写入，那么当时写入的这台机器的请求就需要进行失败重试，否则就会把数据丢失！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/fqyrmm.jpg" alt=""></p><p>3、ES 集群某个节点的磁盘满了</p><p>这里说的磁盘满了，并不是磁盘真的就没有一点剩余空间的，是 es 会在写入的时候检查磁盘的使用情况，在 85% 的时候会打印日志警告。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/QOGm4g.jpg" alt=""></p><p>这里我看了下源码如下图：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/l1zFzb.jpg" alt=""></p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ehiQdO.jpg" alt=""></p><p>如果你想继续让 es 写入的话就需要去重新配一下 es 让它继续写入，或者你也可以清空些不必要的数据腾出磁盘空间来。</p><h4 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">DataStream&lt;String&gt; input = ...;</span><br><span class="line"></span><br><span class="line">input.addSink(<span class="keyword">new</span> ElasticsearchSink&lt;&gt;(</span><br><span class="line">    config, transportAddresses,</span><br><span class="line">    <span class="keyword">new</span> ElasticsearchSinkFunction&lt;String&gt;() &#123;...&#125;,</span><br><span class="line">    <span class="keyword">new</span> ActionRequestFailureHandler() &#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">void</span> <span class="title">onFailure</span><span class="params">(ActionRequest action,</span></span></span><br><span class="line"><span class="function"><span class="params">                Throwable failure,</span></span></span><br><span class="line"><span class="function"><span class="params">                <span class="keyword">int</span> restStatusCode,</span></span></span><br><span class="line"><span class="function"><span class="params">                RequestIndexer indexer)</span> throw Throwable </span>&#123;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (ExceptionUtils.containsThrowable(failure, EsRejectedExecutionException.class)) &#123;</span><br><span class="line">                <span class="comment">// full queue; re-add document for indexing</span></span><br><span class="line">                indexer.add(action);</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (ExceptionUtils.containsThrowable(failure, ElasticsearchParseException.class)) &#123;</span><br><span class="line">                <span class="comment">// malformed document; simply drop request without failing sink</span></span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// for all other failures, fail the sink</span></span><br><span class="line">                <span class="comment">// here the failure is simply rethrown, but users can also choose to throw custom exceptions</span></span><br><span class="line">                <span class="keyword">throw</span> failure;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;));</span><br></pre></td></tr></table></figure><p>如果仅仅只是想做失败重试，也可以直接使用官方提供的默认的 RetryRejectedExecutionFailureHandler ，该处理器会对 EsRejectedExecutionException 导致到失败写入做重试处理。如果你没有设置失败处理器(failure handler)，那么就会使用默认的 NoOpFailureHandler 来简单处理所有的异常。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>本文写了 Flink connector es，将 Kafka 中的数据读取并存储到 ElasticSearch 中，文中讲了如何封装自带的 sink，然后一些扩展配置以及 FailureHandler 情况下要怎么处理。（这个问题可是线上很容易遇到的）</p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/</a></p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/eqp24L.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Nq4VN3.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/9qJpA4.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="ElasticSearch" scheme="http://www.54tianzhisheng.cn/tags/ElasticSearch/"/>
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— Flink 中几种 Time 详解</title>
    <link href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/"/>
    <id>http://www.54tianzhisheng.cn/2018/12/11/Flink-time/</id>
    <published>2018-12-10T16:00:00.000Z</published>
    <updated>2019-04-24T17:09:44.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/BuSGR2.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>Flink 在流程序中支持不同的 <strong>Time</strong> 概念，就比如有 Processing Time、Event Time 和 Ingestion Time。</p><p>下面我们一起来看看这几个 Time：</p><h3 id="Processing-Time"><a href="#Processing-Time" class="headerlink" title="Processing Time"></a>Processing Time</h3><p>Processing Time 是指事件被处理时机器的系统时间。</p><p>当流程序在 Processing Time 上运行时，所有基于时间的操作(如时间窗口)将使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。</p><p>例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。</p><p>Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。</p><h3 id="Event-Time"><a href="#Event-Time" class="headerlink" title="Event Time"></a>Event Time</h3><p>Event Time 是事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time  水印，这是表示 Event Time  进度的机制。 </p><p>完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（按照事件的时间）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。</p><p>假设所有数据都已到达， Event Time  操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，无论它们到达的顺序如何。 </p><p>请注意，有时当 Event Time 程序实时处理实时数据时，它们将使用一些 Processing Time 操作，以确保它们及时进行。</p><h3 id="Ingestion-Time"><a href="#Ingestion-Time" class="headerlink" title="Ingestion Time"></a>Ingestion Time</h3><p>Ingestion Time 是事件进入 Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。</p><p>Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，它稍微贵一些，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（在源处分配一次），所以对事件的不同窗口操作将引用相同的时间戳，而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。</p><p>与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序不必指定如何生成水印。</p><p>在 Flink 中，，Ingestion Time 与 Event Time 非常相似，但 Ingestion Time 具有自动分配时间戳和自动生成水印功能。</p><p>说了这么多概念比较干涩，下面直接看图：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ZjX712.jpg" alt=""></p><h3 id="设定时间特性"><a href="#设定时间特性" class="headerlink" title="设定时间特性"></a>设定时间特性</h3><p>Flink DataStream 程序的第一部分通常是设置基本时间特性。 该设置定义了数据流源的行为方式（例如：它们是否将分配时间戳），以及像 <code>KeyedStream.timeWindow(Time.seconds(30))</code> 这样的窗口操作应该使用上面哪种时间概念。</p><p>以下示例显示了一个 Flink 程序，该程序在每小时时间窗口中聚合事件。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">final</span> StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();</span><br><span class="line"></span><br><span class="line">env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime);</span><br><span class="line"></span><br><span class="line"><span class="comment">// 其他</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime);</span></span><br><span class="line"><span class="comment">// env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);</span></span><br><span class="line"></span><br><span class="line">DataStream&lt;MyEvent&gt; stream = env.addSource(<span class="keyword">new</span> FlinkKafkaConsumer09&lt;MyEvent&gt;(topic, schema, props));</span><br><span class="line"></span><br><span class="line">stream</span><br><span class="line">    .keyBy( (event) -&gt; event.getUser() )</span><br><span class="line">    .timeWindow(Time.hours(<span class="number">1</span>))</span><br><span class="line">    .reduce( (a, b) -&gt; a.add(b) )</span><br><span class="line">    .addSink(...);</span><br></pre></td></tr></table></figure><h3 id="Event-Time-和-Watermarks"><a href="#Event-Time-和-Watermarks" class="headerlink" title="Event Time 和 Watermarks"></a>Event Time 和 Watermarks</h3><p>注意：Flink 实现了数据流模型中的许多技术。有关 Event Time 和 Watermarks 的详细介绍，请查看以下文章：</p><ul><li><a href="">https://www.oreilly.com/ideas/the-world-beyond-batch-streaming-101</a></li><li><a href="">https://research.google.com/pubs/archive/43864.pdf</a></li></ul><p>支持 Event Time 的流处理器需要一种方法来衡量 Event Time 的进度。 例如，当 Event Time 超过一小时结束时，需要通知构建每小时窗口的窗口操作符，以便操作员可以关闭正在进行的窗口。</p><p>Event Time 可以独立于 Processing Time 进行。 例如，在一个程序中，操作员的当前 Event Time 可能略微落后于 Processing Time （考虑到接收事件的延迟），而两者都以相同的速度进行。另一方面，另一个流程序可能只需要几秒钟的时间就可以处理完 Kafka Topic 中数周的 Event Time 数据。</p><p>Flink 中用于衡量 Event Time 进度的机制是 Watermarks。 Watermarks 作为数据流的一部分流动并带有时间戳 t。 Watermark（t）声明 Event Time 已到达该流中的时间 t，这意味着流中不应再有具有时间戳 t’&lt;= t 的元素（即时间戳大于或等于水印的事件）</p><p>下图显示了带有(逻辑)时间戳和内联水印的事件流。在本例中，事件是按顺序排列的(相对于它们的时间戳)，这意味着水印只是流中的周期性标记。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/oVNmJS.jpg" alt=""></p><p>Watermark 对于无序流是至关重要的，如下所示，其中事件不按时间戳排序。通常，Watermark 是一种声明，通过流中的该点，到达某个时间戳的所有事件都应该到达。一旦水印到达操作员，操作员就可以将其内部事件时间提前到水印的值。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/T83QKo.jpg" alt=""></p><h3 id="平行流中的水印"><a href="#平行流中的水印" class="headerlink" title="平行流中的水印"></a>平行流中的水印</h3><p>水印是在源函数处生成的，或直接在源函数之后生成的。源函数的每个并行子任务通常独立生成其水印。这些水印定义了特定并行源处的事件时间。</p><p>当水印通过流程序时，它们会提前到达操作人员处的事件时间。当一个操作符提前它的事件时间时，它为它的后续操作符在下游生成一个新的水印。</p><p>一些操作员消耗多个输入流; 例如，一个 union，或者跟随 keyBy（…）或 partition（…）函数的运算符。 这样的操作员当前事件时间是其输入流的事件时间的最小值。 由于其输入流更新其事件时间，因此操作员也是如此。</p><p>下图显示了流经并行流的事件和水印的示例，以及跟踪事件时间的运算符。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/WJv5rH.jpg" alt=""></p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://github.com/zhisheng17/flink/blob/feature%2Fzhisheng_release_1.6/docs/dev/event_time.md">https://github.com/zhisheng17/flink/blob/feature%2Fzhisheng_release_1.6/docs/dev/event_time.md</a></p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">http://www.54tianzhisheng.cn/2018/12/11/Flink-time/</a></p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/k0IblV.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/ERtZlo.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/BuSGR2.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>《从0到1学习Flink》—— 介绍Flink中的Stream Windows</title>
    <link href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/"/>
    <id>http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/</id>
    <published>2018-12-07T16:00:00.000Z</published>
    <updated>2019-04-24T17:07:24.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/SdRTWm.jpg" alt=""></p><a id="more"></a><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>目前有许多数据分析的场景从批处理到流处理的演变， 虽然可以将批处理作为流处理的特殊情况来处理，但是分析无穷集的流数据通常需要思维方式的转变并且具有其自己的术语（例如，“windowing（窗口化）”、“at-least-once（至少一次）”、“exactly-once（只有一次）” ）。</p><p>对于刚刚接触流处理的人来说，这种转变和新术语可能会非常混乱。 Apache Flink 是一个为生产环境而生的流处理器，具有易于使用的 API，可以用于定义高级流分析程序。</p><p>Flink 的 API 在数据流上具有非常灵活的窗口定义，使其在其他开源流处理框架中脱颖而出。</p><p>在这篇文章中，我们将讨论用于流处理的窗口的概念，介绍 Flink 的内置窗口，并解释它对自定义窗口语义的支持。</p><h3 id="什么是-Windows？"><a href="#什么是-Windows？" class="headerlink" title="什么是 Windows？"></a>什么是 Windows？</h3><p>下面我们结合一个现实的例子来说明。</p><p>就拿交通传感器的示例：统计经过某红绿灯的汽车数量之和？ </p><p>假设在一个红绿灯处，我们每隔 15 秒统计一次通过此红绿灯的汽车数量，如下图：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/Rh5KNm.jpg" alt=""></p><p>可以把汽车的经过看成一个流，无穷的流，不断有汽车经过此红绿灯，因此无法统计总共的汽车数量。但是，我们可以换一种思路，每隔 15 秒，我们都将与上一次的结果进行 sum 操作（滑动聚合），如下：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/qZuFCt.jpg" alt=""></p><p>这个结果似乎还是无法回答我们的问题，根本原因在于流是无界的，我们不能限制流，但可以在有一个有界的范围内处理无界的流数据。</p><p>因此，我们需要换一个问题的提法：每分钟经过某红绿灯的汽车数量之和？<br>这个问题，就相当于一个定义了一个 Window（窗口），window 的界限是1分钟，且每分钟内的数据互不干扰，因此也可以称为翻滚（不重合）窗口，如下图： </p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/boZyUF.jpg" alt=""></p><p>第一分钟的数量为8，第二分钟是22，第三分钟是27。。。这样，1个小时内会有60个window。</p><p>再考虑一种情况，每30秒统计一次过去1分钟的汽车数量之和： </p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/QZ92SU.jpg" alt=""></p><p>此时，window 出现了重合。这样，1个小时内会有120个 window。</p><p>扩展一下，我们可以在某个地区，收集每一个红绿灯处汽车经过的数量，然后每个红绿灯处都做一次基于1分钟的window统计，即并行处理： </p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/TIAzbx.jpg" alt=""></p><h3 id="它有什么作用？"><a href="#它有什么作用？" class="headerlink" title="它有什么作用？"></a>它有什么作用？</h3><p>通常来讲，Window 就是用来对一个无限的流设置一个有限的集合，在有界的数据集上进行操作的一种机制。window 又可以分为基于时间（Time-based）的 window 以及基于数量（Count-based）的 window。</p><h3 id="Flink-自带的-window"><a href="#Flink-自带的-window" class="headerlink" title="Flink 自带的 window"></a>Flink 自带的 window</h3><p>Flink DataStream API 提供了 Time 和 Count 的 window，同时增加了基于 Session 的 window。同时，由于某些特殊的需要，DataStream API 也提供了定制化的 window 操作，供用户自定义 window。</p><p>下面，主要介绍 Time-Based window 以及 Count-Based window，以及自定义的 window 操作，Session-Based Window 操作将会在后续的文章中讲到。</p><h4 id="Time-Windows"><a href="#Time-Windows" class="headerlink" title="Time Windows"></a>Time Windows</h4><p>正如命名那样，Time Windows 根据时间来聚合流数据。例如：一分钟的 tumbling time window 收集一分钟的元素，并在一分钟过后对窗口中的所有元素应用于一个函数。</p><p>在 Flink 中定义 tumbling time windows(翻滚时间窗口) 和 sliding time windows(滑动时间窗口) 非常简单：</p><p><strong>tumbling time windows(翻滚时间窗口)</strong></p><p>输入一个时间参数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="number">1</span>)</span><br><span class="line">.timeWindow(Time.minutes(<span class="number">1</span>)) <span class="comment">//tumbling time window 每分钟统计一次数量和</span></span><br><span class="line">.sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p><strong>sliding time windows(滑动时间窗口)</strong></p><p>输入两个时间参数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="number">1</span>)</span><br><span class="line">.timeWindow(Time.minutes(<span class="number">1</span>), Time.seconds(<span class="number">30</span>)) <span class="comment">//sliding time window 每隔 30s 统计过去一分钟的数量和</span></span><br><span class="line">.sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>有一点我们还没有讨论，即“收集一分钟的元素”的确切含义，它可以归结为一个问题，“流处理器如何解释时间?”</p><p>Apache Flink 具有三个不同的时间概念，即 processing time, event time 和 ingestion time。</p><p>这里可以参考我下一篇文章：</p><p><a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Event Time、Processing Time和Ingestion Time</a></p><h4 id="Count-Windows"><a href="#Count-Windows" class="headerlink" title="Count Windows"></a>Count Windows</h4><p>Apache Flink 还提供计数窗口功能。如果计数窗口设置的为 100 ，那么将会在窗口中收集 100 个事件，并在添加第 100 个元素时计算窗口的值。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/rRGAcK.jpg" alt=""></p><p>在 Flink 的 DataStream API 中，tumbling count window 和 sliding count window 的定义如下:</p><p><strong>tumbling count window</strong></p><p>输入一个时间参数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="number">1</span>)</span><br><span class="line">.countWindow(<span class="number">100</span>) <span class="comment">//统计每 100 个元素的数量之和</span></span><br><span class="line">.sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><p><strong>sliding count window</strong></p><p>输入两个时间参数</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data.keyBy(<span class="number">1</span>) </span><br><span class="line">.countWindow(<span class="number">100</span>, <span class="number">10</span>) <span class="comment">//每 10 个元素统计过去 100 个元素的数量之和</span></span><br><span class="line">.sum(<span class="number">1</span>);</span><br></pre></td></tr></table></figure><h4 id="解剖-Flink-的窗口机制"><a href="#解剖-Flink-的窗口机制" class="headerlink" title="解剖 Flink 的窗口机制"></a>解剖 Flink 的窗口机制</h4><p>Flink 的内置 time window 和 count window 已经覆盖了大多数应用场景，但是有时候也需要定制窗口逻辑，此时 Flink 的内置的 window 无法解决这些问题。为了还支持自定义 window 实现不同的逻辑，DataStream API 为其窗口机制提供了接口。</p><p>下图描述了 Flink 的窗口机制，并介绍了所涉及的组件：</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/xrhwC8.jpg" alt=""></p><p>到达窗口操作符的元素被传递给 WindowAssigner。WindowAssigner 将元素分配给一个或多个窗口，可能会创建新的窗口。<br>窗口本身只是元素列表的标识符，它可能提供一些可选的元信息，例如 TimeWindow 中的开始和结束时间。注意，元素可以被添加到多个窗口，这也意味着一个元素可以同时在多个窗口存在。</p><p>每个窗口都拥有一个 Trigger(触发器)，该 Trigger(触发器) 决定何时计算和清除窗口。当先前注册的计时器超时时，将为插入窗口的每个元素调用触发器。在每个事件上，触发器都可以决定触发(即、清除(删除窗口并丢弃其内容)，或者启动并清除窗口。一个窗口可以被求值多次，并且在被清除之前一直存在。注意，在清除窗口之前，窗口将一直消耗内存。</p><p>当 Trigger(触发器) 触发时，可以将窗口元素列表提供给可选的 Evictor，Evictor 可以遍历窗口元素列表，并可以决定从列表的开头删除首先进入窗口的一些元素。然后其余的元素被赋给一个计算函数，如果没有定义 Evictor，触发器直接将所有窗口元素交给计算函数。</p><p>计算函数接收 Evictor 过滤后的窗口元素，并计算窗口的一个或多个元素的结果。 DataStream API 接受不同类型的计算函数，包括预定义的聚合函数，如 sum（），min（），max（），以及 ReduceFunction，FoldFunction 或 WindowFunction。</p><p>这些是构成 Flink 窗口机制的组件。 接下来我们逐步演示如何使用 DataStream API 实现自定义窗口逻辑。 我们从 DataStream [IN] 类型的流开始，并使用 key 选择器函数对其分组，该函数将 key 相同类型的数据分组在一块。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">SingleOutputStreamOperator&lt;xxx&gt; data = env.addSource(...);</span><br><span class="line">data.keyBy()</span><br></pre></td></tr></table></figure><h3 id="如何自定义-Window？"><a href="#如何自定义-Window？" class="headerlink" title="如何自定义 Window？"></a>如何自定义 Window？</h3><p>1、Window Assigner</p><p>负责将元素分配到不同的 window。</p><p>Window API 提供了自定义的 WindowAssigner 接口，我们可以实现 WindowAssigner 的 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> Collection&lt;W&gt; <span class="title">assignWindows</span><span class="params">(T element, <span class="keyword">long</span> timestamp)</span></span></span><br></pre></td></tr></table></figure><p>方法。同时，对于基于 Count 的 window 而言，默认采用了 GlobalWindow 的 window assigner，例如：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keyBy.window(GlobalWindows.create())</span><br></pre></td></tr></table></figure><p>2、Trigger </p><p>Trigger 即触发器，定义何时或什么情况下移除 window</p><p>我们可以指定触发器来覆盖 WindowAssigner 提供的默认触发器。 请注意，指定的触发器不会添加其他触发条件，但会替换当前触发器。</p><p>3、Evictor（可选）</p><p>驱逐者，即保留上一 window 留下的某些元素</p><p>4、通过 apply WindowFunction 来返回 DataStream 类型数据。</p><p>利用 Flink 的内部窗口机制和 DataStream API 可以实现自定义的窗口逻辑，例如 session window。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>对于现代流处理器来说，支持连续数据流上的各种类型的窗口是必不可少的。 Apache Flink 是一个具有强大功能集的流处理器，包括一个非常灵活的机制，可以在连续数据流上构建窗口。 Flink 为常见场景提供内置的窗口运算符，以及允许用户自定义窗口逻辑。 </p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p>1、<a href="">https://flink.apache.org/news/2015/12/04/Introducing-windows.html</a></p><p>2、<a href="">https://blog.csdn.net/lmalds/article/details/51604501</a></p><h3 id="关注我"><a href="#关注我" class="headerlink" title="关注我"></a>关注我</h3><p>转载请务必注明原创地址为：<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/</a></p><p>微信公众号：<strong>zhisheng</strong></p><p>另外我自己整理了些 Flink 的学习资料，目前已经全部放到微信公众号了。你可以加我的微信：<strong>zhisheng_tian</strong>，然后回复关键字：<strong>Flink</strong> 即可无条件获取到。</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/IPb9iV.jpg" alt=""></p><p>更多私密资料请加入知识星球！</p><p><img src="https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/2oPk7U.jpg" alt=""></p><h3 id="Github-代码仓库"><a href="#Github-代码仓库" class="headerlink" title="Github 代码仓库"></a>Github 代码仓库</h3><p><a href="https://github.com/zhisheng17/flink-learning/">https://github.com/zhisheng17/flink-learning/</a></p><p>以后这个项目的所有代码都将放在这个仓库里，包含了自己学习 flink 的一些 demo 和博客</p><h3 id="相关文章"><a href="#相关文章" class="headerlink" title="相关文章"></a>相关文章</h3><p>1、<a href="http://www.54tianzhisheng.cn/2018/10/13/flink-introduction/">《从0到1学习Flink》—— Apache Flink 介绍</a></p><p>2、<a href="http://www.54tianzhisheng.cn/2018/09/18/flink-install">《从0到1学习Flink》—— Mac 上搭建 Flink 1.6.0 环境并构建运行简单程序入门</a></p><p>3、<a href="http://www.54tianzhisheng.cn/2018/10/27/flink-config/">《从0到1学习Flink》—— Flink 配置文件详解</a></p><p>4、<a href="http://www.54tianzhisheng.cn/2018/10/28/flink-sources/">《从0到1学习Flink》—— Data Source 介绍</a></p><p>5、<a href="http://www.54tianzhisheng.cn/2018/10/30/flink-create-source/">《从0到1学习Flink》—— 如何自定义 Data Source ？</a></p><p>6、<a href="http://www.54tianzhisheng.cn/2018/10/29/flink-sink/">《从0到1学习Flink》—— Data Sink 介绍</a></p><p>7、<a href="http://www.54tianzhisheng.cn/2018/10/31/flink-create-sink/">《从0到1学习Flink》—— 如何自定义 Data Sink ？</a></p><p>8、<a href="http://www.54tianzhisheng.cn/2018/11/04/Flink-Data-transformation/">《从0到1学习Flink》—— Flink Data transformation(转换)</a></p><p>9、<a href="http://www.54tianzhisheng.cn/2018/12/08/Flink-Stream-Windows/">《从0到1学习Flink》—— 介绍Flink中的Stream Windows</a></p><p>10、<a href="http://www.54tianzhisheng.cn/2018/12/11/Flink-time/">《从0到1学习Flink》—— Flink 中的几种 Time 详解</a></p><p>11、<a href="http://www.54tianzhisheng.cn/2018/12/30/Flink-ElasticSearch-Sink/">《从0到1学习Flink》—— Flink 写入数据到 ElasticSearch</a></p><p>12、<a href="http://www.54tianzhisheng.cn/2019/01/05/Flink-run/">《从0到1学习Flink》—— Flink 项目如何运行？</a></p><p>13、<a href="http://www.54tianzhisheng.cn/2019/01/06/Flink-Kafka-sink/">《从0到1学习Flink》—— Flink 写入数据到 Kafka</a></p><p>14、<a href="http://www.54tianzhisheng.cn/2019/01/13/Flink-JobManager-High-availability/">《从0到1学习Flink》—— Flink JobManager 高可用性配置</a></p><p>15、<a href="http://www.54tianzhisheng.cn/2019/01/14/Flink-parallelism-slot/">《从0到1学习Flink》—— Flink parallelism 和 Slot 介绍</a></p><p>16、<a href="http://www.54tianzhisheng.cn/2019/01/15/Flink-MySQL-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据批量写入到 MySQL</a></p><p>17、<a href="http://www.54tianzhisheng.cn/2019/01/20/Flink-RabbitMQ-sink/">《从0到1学习Flink》—— Flink 读取 Kafka 数据写入到 RabbitMQ</a></p><p>18、<a href="http://www.54tianzhisheng.cn/2019/03/13/flink-job-jars/">《从0到1学习Flink》—— 你上传的 jar 包藏到哪里去了?</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;img src=&quot;https://zhisheng-blog.oss-cn-hangzhou.aliyuncs.com/images/SdRTWm.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Flink" scheme="http://www.54tianzhisheng.cn/tags/Flink/"/>
    
      <category term="大数据" scheme="http://www.54tianzhisheng.cn/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
      <category term="流式计算" scheme="http://www.54tianzhisheng.cn/tags/%E6%B5%81%E5%BC%8F%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
</feed>
